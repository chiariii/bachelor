"Stoicism, What's the other side?I have only been reading Marcus Aurelius for a few weeks now.  Right away I loved many of his ideas.  But..

IMO, It sounds kind of 'cold'.  It all comes down to,


Have a purpose.  Take part in life and help to make society better for everyone.  Only you can effect your life.   If you can avoid a challenge, it's better than to win the challenge.   If you are attacked, you can't be harmed.  Respect everyone regardless of the situation.  Do what you can to improve yourself, for yourself, and only yourself. Win for yourself, not for anybody else.  Be grateful for every moment of the day.   ""It's your world and everybody else is just living in it."".  You're better than anyone else.  But there is no reason to think you are.   Surround yourself only with people you know are trustful.  Be diligent in all these aspects of life.


My thoughts are that it feels a little robotic and cold.  There's got to be another side to it.  What about joy, love, laughing and fun?  Then there's your faith.  I'm a Roman Catholic so my beliefs are pretty much the same.  But, you might think this is gonna sound stupid.  I see more smiles from my side than I do the Stoic side.  Maybe it's the word itself, Stoic.  


I think they are basically saying the same thing.  Just worded different."
"How to deal with conspiracy theories?I have a friend who was advocating a conspiracy theory about coronavirus being released by the government as distraction for a pedophilia ring involving the worlds government officials.

This is of course patently absurd and lacks any evidence. Even overlooking baseless accusations against government officials and other obvious flaws, there is plenty of evidence in the genome sequencing of the disease that illustrates it was not genetically modified in a lab and similar viruses are present in wildlife. 

I explain this. I explain how we have no evidence to support his theory. It is just a compelling narrative that fits comfortably with what's occurred; but just because it 'fits' it doesn't mean it is supported. 

In response he says 'your being ignorant, you should go away and research it first'. Firstly, I am confident that all reputable sources will probably cite evidence against this and the only 'evidence' is from sites that peddle conspiracies, however, **more** **importantly** I feel like I shouldn't have to research something like this. Surely there are some claims that are so outlandish that it's not ignorance to simply denounce. How can I possibly justify this though? It seems inconsistent with the well accepted idea that you must have evidence to belief something? or is the burden of proof upon him? Surely if he was right I would have to go away research an infinite possibility of absurd theories just because somebody decided to vocalise them. That the second somebody gave an absurd opinion like 'pigeons are actually robots and government spies' I would have to be fair to them do some specific research (catch a pigeon, cut it open, check for batteries, etc.) and come back. Surely all opinions aren't equally valid.

He then went on to argue 'well technically we don't *know* anything so you can't say it's wrong'. He's not familiar with philosophy and was unaware that a term even exists for the position he was stating- I imagine he considered it just an interesting nuance that most people had never even considered. Surely, epistemological skepticism cannot be used as an argument to defend poor reasoning. I found it difficult to explain to him that whilst scepticism may have merit as a position, even if you agree with it, you still live your life as if knowledge can be obtained, you go to university because you trust the knowledge has value, vaccines are developed etc. 

He argued passionately at first (and so it was implicit that he recognised knowledge was obtainable - that's why he bothered to argue) then when his point had been refuted and evidence dismissed as false, rather than accepting defeat, he suddenly invokes skepticism to say that neither of us will ever be right. 

In just a normal day, he himself would have discarded millions of obviously absurd beliefs just to live his life. He continues to breathe, because he discards the obviously absurd belief that all the oxygen in the air hasn't suddenly turned poisonous. Equally, prior to the conversation he would have argued passionately for many things and against many things, because he of course believes that knowledge on some practical basis can be obtained. 

All these thoughts are convoluted, but can anybody: 

**1)** help me explain why skepticism cannot be invoked to defend a poor argument, or any argument. ""my position, no matter how absurd, cannot be proved wrong because we don't know anything"" It doesn't seem valid. Even in the skeptics world we have to accept good and bad reasoning to live our lives. 

**2)** help me explain why it's not ignorant to dismiss random or extreme opinions that are just obviously false. *or maybe it is and we have to be consistent and treat everybody's opinion equally...no matter how obviously absurd.*"
"Consciousness and timeI have not studied philosopy so bear with me,

I'll just jump right in and try to explain what it is I'm wondering about, maybe someone can offer some insight or comments.

Descartes's Cogito ergo sum has helped me alot when I went through a phase thinking about ""brain in a vat"" scenarios.

And while I know that I might still be a brain in a vat (or similar scenarios like we're living in a computer simulation that even Elon Musk talked about at one point, and this is a guy I admire and don't assume to be stupid) or at least I can't prove I'm not, I've reached a point where I simply assume I'm in fact not a ""brain in a vat"" or living in a computer simulation because it sounds to me like a overly complex answer with no evidence to support, sort of like creationism.

I.e. if I was in fact a brain in a vat not only would there be this complex world of chaos around me, there would be a level above that where some sort of super computer faked all that, and what world does that exist in then and how does that imagined super computer work exactly I wonder?

It fails the occams razor test as I see it.

Anyway so far so good, I'm somewhat comfortable thinking that this world I see is actually there, I trust my senses and so on.


1. PROBLEM, SOULLESS MEAT-BAGS

But then another thing starts haunting me. It's this, how do I know I'm not surrounded by soulless meat-bags idea. Let me elaborate. Cogito ergo sum helps me be sure that I am here. I can sense myself, my own consciousness. I am here, in my body, awake, conscious. But how can I be sure about other people? Or animals? Or plants? Or even ""dead"" things like stones. I do not have any clue how consciousness work. I assume of course that the other humans are conscious because I am and they appear similar to me, but that's it. A (big) assumption.

I'm a software engineer myself and very aware of the advances in AI and I'm pretty sure it wont be long before we start seeing AI that are hard to distinguish from humans. If they start making humanoid androids it would be really hard to tell the difference between humans and androids. I know this is a bit sci-fi but I honestly believe it's closer than we generally think.

Personally I do not believe that a computer suddenly becomes conscious if we give it a strong enough CPU, sohpisticated AI software and a human looking artificial body. Just as I do not believe my laptop is conscious. For me it's the same. But if I don't believe consciousness emerges in AI, how can I then be so sure that all my fellow humans all have consciousness? Obviously I have no idea this consciousness work except that I know I have it myself but can i really be sure everyone have it too? What if it doesn't emerge in all humans? This is what i mean by the soulless meat-bags. I guess I can never be sure, but I'll go on assuming they all are conscious.

I keep associating the idea of consciousness with the idea of having a soul. I'm sceptic about the idea of a soul because I associate it with religion and I'm an atheist myself. However I cannot deny my own consciousness, it's obviously there. So I wonder, are human brains consciousness-generators? And if so, is it reasonable to think that artificial brains could generate consciousness also?


2. PROBLEM, DO WE ALL EXPERINCE TIME THE SAME WAY?

This is a variant of the soulless meat-bags idea that I've also wondered about.

It about time. There is the past. The present right now. And the future. Only the present feels real. Also for example my feelings like empathy are alot stronger for other humans suffering in the present than in the past. If i hear about genocide that happened many years ago I don't feel as sorry for them as if it's a genocide happening right now, in the present. I'm not sure why that is, but it definately is like that for me. Of course a genocide in the future I can't feel empathy for now, cause it hasn't happened yet so it makes no sense. So time matters for me at least when it comes to something like feeling empathy.

My problem is then this, how can we be sure we are all following ""the same clock"", how can we be sure we are all syncronized. Einsteins theory of relativity has shown that time bends in a very real way. I think of time sort of like a playhead in a cassette player. The entire tape already exists, the tape that has already passed the playhead is the past, the tape right on the playhead this moment is the present and the tape that is still left is the future. What if my consciousness is my personal playhead, i have my own copy the tape just like other people have their own copy, but our playheads could be at different times. So the people i see on the street could be people who's playhead is already way ahead and the person I'm interacting with is actually ""the past"".

I guess I feel that consciousness is related to the present.

Hmmmm.... Enough crazy talk :) Please comment if you have any remarks."
"How entwined are reason and emotion?For the past year or so, I've been thinking a lot about the role of emotion in decision making, and I've come to see it in a different light. I always used to think of emotion as intrinsically irrational and potentially harmful. I saw it as what reason needs to fight against and prevail over: emotion leading to unfounded belief, needless attachments, erratic outbursts and such. I saw logic and reason as the highest aims and intellectual life as the most rewarding thing. 

Lately, though, I've started to see emotion as something that might be embedded in reason itself (or is it the other way round?). What seems reasonable to us seems only so because of what it ultimately makes us feel (that it's the right thing, that it's correct, or true). But the fact that we value reason and pursue truth is just another impulse that's dictated and steered by emotion itself, otherwise we'd have no will to aim for it.

The things that we feel make us feel good are dictated by emotion and, in turn, cause (good) emotions, so they are, in a way, what is reasonable to pursue. Now, it's true that emotion can be a very poor guide, as it can often merely be the result of our muddled thoughts and feelings, and not a clear and sharp steering wheel grounded on cold understand. But perhaps the opposite to emotion is not necessarily cold reason and logic, but rather more pure and true emotions, which stem from a clear viewpoint.

Can it sometimes be reasonable to be emotional?  Can reason be based on emotional grounds? Are they opposed at all? 

If I see this through the lense of music, something I'm very passionate about, the edges can get very blurry as well. There are songs that I love and consider great because they cause so many emotions in me—they make me feel things and connect with others. The whys as to why I might love a song can be very emotionally grounded. Nonetheless, if I were asked why a song provokes these emotions and why I value so much that they arise, I could give reasons for it. So, in a way, these emotions can be explained, even if wrongly so via confabulation, by reason itself. Conversely, those reasons themselves might not be able to be explained, or might be explained by using even further emotions down the path. It seems that it can be very hard to disentangle the two things. 

Every emotion we feel seems to be backed up by reasons for its existence, be it evolutionary or otherwise, while every reason we might provide for anything often rests on emotions, which rest on reasons, etc. 

If I feel disappointment or pain, I could say that that feeling is subjectively true. It is what I'm feeling, and even if I might not be able to define it or conceptualise it, it's nonetheless (subjectively) true insofar as I feel it. This might sound redundant, but what I'm trying to get at, is that if we used these same feelings as a lens to try to understand the world, as a filter, or as an heuristic, then we would be patently wrong in doing so. If you try to understand the world or accrue knowledge about it through the lens of emotion, you will reach wrong conclusions, be biased and easily mislead, and you probably won't reach what we call objective truth in any accurate way. Objective truth is something that we aim to get to through reason, detachment, abstraction and systematisation. 

A life dictated purely by reason seems to me an utterly dull one, even though reason is the means through which anything can be achieved or explained with any degree of reliability. If we always do what's more efficient and reasonable and never follow intuition, emotion or even hedonism itself —pleasure without logic or justification—, we would fall into a bland existence. We would be robots, working with set outputs for every kind of input we might get, be it internal or external. Nonetheless, pursuing what makes us feel good and we can't justify is a reason in itself to follow these instincts. How can these two aspects be conflated? On the other hand, pursuing solely what's emotionally pleasing will probably lead us astray into a life of excess and recklessness, where consequences and long term goals will be overlooked.

I would guess (and hope) I'm getting my confusion across clearly enough by now. Emotion and reason seem to be related in ways that I had never expected, and they play and depend on each other, while they can also be separated through the filter of what's subjectively true and what's objectively true. I would love for someone to refer me to books or people who have talked about this apparent dilemma and the role of emotion in reasoning, and of reason in emotion. I've read Thomas Nagel's The View From Nowhere, which focused more on the distinction (and possible merging) of the subjective and the objective, but I haven't stumbled upon much material on the inevitable interweaving of reason and emotion as guides for life, and would much appreciate some light on the matter."
Can we hold robots morally responsible?Please fame your answers in virtue of conscious awareness considerations and what makes a thing morally responsible.
RobotsWhat do you think we should do about the inevitability of robots dominating the workforce?  Why do anything or work knowing that that is the future?  How can one cope with that future?
"Laplace's Demon, Fatalism, and free will 

A case for free will against the fatalistic view of the universe is that if fatalism is correct then if you took a snapshot of the universe at any given moment, you would know (given you had the capabilities to compute the result given you knew the location and state of all atoms of the universe) what you action you are going to take in some circumstance. Say you knew the phone was gonna ring due to a fatalistic calculation, then you could use physics to determine whether you will answer the phone or not. And if you knew the answer to this than you could just do the opposite of whatever the calculation predicted. Thus, free will must be real because you can always will the opposite of the action calculated you would take.

This is however not argument for free will, but it is still a mindfuck. You don't need free will for this same circumstance to hold. For instance if you calculated what a simple decision making robot would do and tell it to the robot, and the robot always does the opposite of what you tell it (as it is programed to do), the same circumstance would hold, even though the robot obviously doesn't have free will.

Still though this is a mind twisting concept, if you could calculate exactly what the robot would do and it would always do the opposite, then you clearly can't predict what it will do. Thus, this thought experiment is a horrible case for free will, but a good case against fatalism. But fatalism to me has still got to hold, so what is the answer to this conundrum, is it just like a paradox (like the sentence ""this sentence is false"") and simply an illogical thing to calculate, or what is the solution?"
"What does a coin owe to mathematics/nature to turn up on its head/tail 50% of the time (or close to that in large datasets)?I'm an engineer and an MBA and I have zero knowledge about philosophy/metaphysics. But a few days back, I was doing some project reports and this thought occurred to me. I haven't been able to shake it off since.


Why must a coin have close to 50% head and 50% tails for a billion tosses? Why can't it just go heads continuously for 500 quintillion times in a row? I know the simple answer is ""it's extremely unlikely"". So what? What does the ""being unlikely"" factor impart to the coin to make it come up with heads or tails? If something is very unlikely, does that mean that the whole of nature is conspiring to  make the coin turn up heads when it comes up with tails 99thousand times in a row? 


But isn't nature chaotic? All of our atoms are dancing around. Bombarding each other with energy and velocity. How do we even begin to comprehend what is possible and what isn't in this chaotic field? We have to resort to simulations for the basest of problems because we lack understanding of the mechanisms. Would probability collapse if we could account for all the atoms at every moment?"
"If a quantum computer gave you the exact date you will die on, does knowing this information change the date?Before you answer, the computer can also predict your deterministic reaction to such information."
"Is transhumanism the only next step for art?I read Arthur Danto’s essay on the End of Art where he confides that contemporary art as we know is it dead because once post-modernism is reached there is nowhere to go from there.

It’s the idea that once Art becomes theory or “self-conscious” it can only progress further within the theory and not veer off into ulterior significant movements.

**In essence, it’s the end.**

Dante gets a lot of hate.

But are we not seeing his predictions come true?

What are the iconic artworks after post-modernism? 

Can you think of any significant artworks post 2000 that rival the significance of art made in the 1900-1970 for example?

Why are there no significant art movements post modernism when there significant movements every decade before postmodernism.

**Where can we truly go after we ask the question of what art is?**

My question is “what can art become?” And the answer is only realised with transhumanism as human become one with technology.

That’s the only way I see forward. 

Ai Augmented artworks, robotic sculptures, Virtual worlds.

**Art at the intersection of tech, science and humanity.**

What do you guys think ?"
"I'm ethically conflicted about humans manipulating themselves (genetically or with the help of other technology) so that they become superhumans. And also conflicted about how the future world looks like in general.It won't be so far in the future before science has advanced far enough to allow genetic manipulation of our brains or to increase our intelligence with the help of nanoscience. But I'm not ready for this. How do people cope with this? At some point humans are gonna alter their DNA and they'll all be 300 IQ (or 10000, I don't know..), super strong, etc. I don't want this.   

* **I feel like it's cheating.** There's no pride in things if you change yourself through science that way. No pride if you can win against that guy in whatever physical or mental competition or videogame or whatever. Maybe it's irrational that a person feels pride AT ALL, but I don't believe in that idea, and I don't wanna get into it.  

* **What is there left to do?** Like literally, how will people fill their time in this world? There's no work because everything will be automated by AI. There's no school because we're all 300IQ people who instantly understand things. I also think super intelligent people get tired of things very quickly. For example say that someone wants to watch a movie: a normal person will probably be entertained, but a 300IQ person will generate a couple predictions of how the movie will go and then he'll be bored because he can predict it all. Or another example: someone wants to learn and discover the world of physics. Cool, very interesting and that world of physics is big enough to fill a huge amount of time. If you are 300 IQ however, you'll read everything once and you'll instantly understand it and that 'huge amount of time' will become quite short and then you'll be bored again. So then I can only think there's no fun in being so intelligent. But then I'm conflicted because people want to be more intelligent than they are, and we wanna be smarter than others.  Edit: there was also a movie I forgot the name but it explains this thought well. There are robots and they start out quite dumb, but they get progressively smarter. At some point those robots 'leave' the humans because the humans don't provide enough cognitive stimulance for the robots anymore.

Any positive answers/solutions to those thoughts? Or, how is not everyone going insane from such thoughts? I'd like to know that."
"Question about compatibilism~~Howdy folks,~~

~~I have been reading a lot about compatibilism recently, trying to get my head around the free will debate. As an aside, it seems to me, that unlike almost everything else I read about/find interesting, whether we have free will/are morally responsible matters a lot to how I feel (and probably how I end up acting) about myself and other people. I find it much easier to be compassionate toward myself, for example, when I adopt the ""I'm a robot running the algorithm programmed by natural selection"" framing and genuinely feel less angry/annoyed at other people. Obviously, though, there are downsides to the view that we don't have free will and you have to take the bitter with the sweet! But that's largely beside the point of this post. As I use free will/moral responsibility here, they are two sides of the same coin, i.e., you are morally responsible if you have ""free will"" and vice versa.~~

~~Simplifying a lot, an incompatibilist would say that those who lack the ability to do otherwise are not morally responsible. A compatibilist says no, even someone whose actions are fully determined can be morally responsible. But the reason~~ *~~why~~* ~~we hold (the right kinds) of people morally responsible for (certain) decisions even in a deterministic universe is that we have a very strong~~ *~~intuition~~* ~~that at least some people are morally responsible for some decisions. Or viewed slightly different, there is no~~ *~~further reason~~* ~~that we hold (some) people morally responsible for (certain) decisions. It is just a fact about human psychology--something that we~~ *~~just do~~*~~. And so, if we can, we want to come up with a theory that can justify this intuition--hence, compatibilism. This is how I understand P. Strawson's work on compatibilism, and I wonder if it also might apply to other flavors of compatibilism.~~

~~Whereas, if you ask a non-compatibilist~~ *~~why~~* ~~(if determinism/naturalism weren't true) people would be morally responsible but a wild animal is not, they would say something like ""X person could have chosen not to do the good/bad thing but she did it anyway."" So there is a further reason underlying our judgments about moral responsibility that has something to do with the ability to do otherwise. And because we have no such ability, assuming determinism/naturalism is true, there is no moral responsibility.~~

~~I am would say there's at least an 85% chance that I'm way off here, but this way of framing the debate might help explain why everyone always seems to be talking past each other--they are essentially operating at different levels.~~

~~Does this make any sense? Am I completely out to lunch?~~

ETA: I have read the SEP article on compatibilism, but it didn't clarify things. And I think it's the same issue.

Probably a better way to frame it is to ask what a compatibilist (pick your flavor, but let's say reason-responsive compatibilist because it's popular nowadays according to SEP) would say if I asked her *why* an agent who can act according to the right kinds of reasons is morally responsible? At bottom, my claim is that compatibilists think we have very strong prima facie reason to believe that at least some people are morally responsible. Asking a reason-responsive compatibilist *why* a reason-responsive agent has free will is like asking why I think that, idk, suffering is bad. It *just is*. And that's fine. Eventually reasons run out for any explanation, philosophical or otherwise. Sometimes you get to a point where there's nothing more to say and either you get it or you don't. 

This is not meant to be a knock on compatibilism. Incompatibilists eventually hit bottom as well. E.g., if they say free will consists in the ability to do otherwise and you ask, why? There's no further reason there, either. Either it makes sense to you that someone who had the ability to do otherwise would morally responsible, or it doesn't.

In any event, the compatibilist will say that someone who wants to deny that anyone is morally responsible bears an exceptionally heavy burden because we should have very high credence in our belief at least some people are morally responsible. So the compatibilist starts from that position and sets out to devise a theory that can accommodate this prima face belief while also excluding those who we would traditionally say are not morally responsible (e.g., children, animals).

This is the bottom line: A compatibilist will not even try to convince you that at least some people are morally responsible. That is not a proposition that needs to be argued for; it's more like an *axiom*, to be surrendered only in the face of a very strong argument that we must discard it.

I think incompatibilists come at the issue from a different angle. The reason one might have thought competent adults are morally responsible is that they meaningfully chose among alternatives, could've done otherwise, etc. So incompatibilists do not think it's axiomatic that competent adults are morally responsible. They think there is a further reason that we have that intuition--i.e., we are used to thinking that competent adults had the ability to do otherwise. And if this turns out to be false (as it does if determinism/naturalism is true), then there's no longer any reason to think that anyone is morally responsible.

And my hypothesis is that this is why debates between the two sides tend to seem unsatisfying. I didn't mean it to be controversial that free will skeptics and compatibilists feel like they are talking past one another. I had thought the consensus was that these debates do little to clarify things. That's certainly true for me; when I've read various debates about free will, I come out just as (if not more) confused than when I started reading. The points of disagreement never get crystallized and it really does seem like the participants are talking past one another. Well, maybe they are, in this way.

Does this make more sense?"
"What alternatives exist to Formal Deduction and the Scientific Method given a axiomatic belief that the Material World as presented to is is Consistent?I initially wrote this question in a comment to [this post in /r/badphilosophy](https://www.reddit.com/r/badphilosophy/comments/dqlxst/the_scientific_method_is_the_only_way_to_evaluate/). I wrote my question as a lurker to the subreddit before I felt that the question would be more fitting here.

My question is the following:

 > I'm not a philosopher but I have an open question in relation to this picture. Would anyone here be interested in ELI5 to me what's wrong with the man's argument in tweets and little graphic?
> 
> How would one evaluate knowledge claims, and let's stick with ""God"" as our claim for the sake of argument, other than the scientific method or by using formal deduction? Taking as an axiom that reality (the material world) is as it seems and my mind isn't playing tricks on me or what have you, how can I validate if something is consistent with reality without using the scientific method?
> 
> This is a genuine question. I can't remember when I found this subreddit, but I've been lurking for a while, and I've noticed that there is a lot of ridicule towards New Atheist types but in this instance I can't really see myself capable of disagreeing with this person. **I don't mean to propose that the ""God"" claim being rejected necessarily means that God could not exist but rather whether a rational agent ought to think if its True or not.**
> 
> Suppose I am building an artificial intelligence, and the A.I. is capable of Computer Vision, Robotics etc. but also we've account of smell, taste and so on such that it would interact with the material world the same way as a human would. The only difference here is I have to programme how the A.I. decides whether to Accept or Reject particular ideas. Taking out of the question of what is mechanism of knowledge evaluation would best account for the being's self preservation - here we are interested in building up Knowledge Bases of facts about the material world. While we can do logical inferences - much like the Symbolic AI's we are used to - we can deduce say: 1. All men are infallible 2. Socrates is a man 3. From 1,2 ""Socrates is infallible"" is true given our knowledge base
> 
> But the A.I. would need need to build up facts about the world - using its sensory mechanisms, it would have to observe the material world, and test it.
> 
> Going back to a claim like ""God"" - **what should I program my A.I. to do to handle such claims?**

Edit: I've noticed a typo in my Title. Please read as ""What alternatives exist to Formal Deduction and the Scientific Method given a axiomatic belief that the Material World as presented to **us** is Consistent?"""
"How to deal with conspiracy theories?I have a friend who was advocating a conspiracy theory about coronavirus being released by the government as distraction for a pedophilia ring involving the worlds government officials.

This is of course patently absurd and lacks any evidence. Even overlooking baseless accusations against government officials and other obvious flaws, there is plenty of evidence in the genome sequencing of the disease that illustrates it was not genetically modified in a lab and similar viruses are present in wildlife. 

I explain this. I explain how we have no evidence to support his theory. It is just a compelling narrative that fits comfortably with what's occurred; but just because it 'fits' it doesn't mean it is supported. 

In response he says 'your being ignorant, you should go away and research it first'. Firstly, I am confident that all reputable sources will probably cite evidence against this and the only 'evidence' is from sites that peddle conspiracies, however, **more** **importantly** I feel like I shouldn't have to research something like this. Surely there are some claims that are so outlandish that it's not ignorance to simply denounce. How can I possibly justify this though? It seems inconsistent with the well accepted idea that you must have evidence to belief something? or is the burden of proof upon him? Surely if he was right I would have to go away research an infinite possibility of absurd theories just because somebody decided to vocalise them. That the second somebody gave an absurd opinion like 'pigeons are actually robots and government spies' I would have to be fair to them do some specific research (catch a pigeon, cut it open, check for batteries, etc.) and come back. Surely all opinions aren't equally valid.

He then went on to argue 'well technically we don't *know* anything so you can't say it's wrong'. He's not familiar with philosophy and was unaware that a term even exists for the position he was stating- I imagine he considered it just an interesting nuance that most people had never even considered. Surely, epistemological skepticism cannot be used as an argument to defend poor reasoning. I found it difficult to explain to him that whilst scepticism may have merit as a position, even if you agree with it, you still live your life as if knowledge can be obtained, you go to university because you trust the knowledge has value, vaccines are developed etc. 

He argued passionately at first (and so it was implicit that he recognised knowledge was obtainable - that's why he bothered to argue) then when his point had been refuted and evidence dismissed as false, rather than accepting defeat, he suddenly invokes skepticism to say that neither of us will ever be right. 

In just a normal day, he himself would have discarded millions of obviously absurd beliefs just to live his life. He continues to breathe, because he discards the obviously absurd belief that all the oxygen in the air hasn't suddenly turned poisonous. Equally, prior to the conversation he would have argued passionately for many things and against many things, because he of course believes that knowledge on some practical basis can be obtained. 

All these thoughts are convoluted, but can anybody: 

**1)** help me explain why skepticism cannot be invoked to defend a poor argument, or any argument. ""my position, no matter how absurd, cannot be proved wrong because we don't know anything"" It doesn't seem valid. Even in the skeptics world we have to accept good and bad reasoning to live our lives. 

**2)** help me explain why it's not ignorant to dismiss random or extreme opinions that are just obviously false. *or maybe it is and we have to be consistent and treat everybody's opinion equally...no matter how obviously absurd.*"
"I have adopted Vulcan philosophy and now have problems determining my value system.I cannot determine proper value system what is consistent with my philosophy (living by 'pure' reason as emotionless being).
In my understanding only basic physiological needs are qualified as reasonable, i.e. immediate survival and preservation of an organism.
But the problem exist if I assume that those needs are satisfied, as It is achievable with current technology. What other needs would I have, assuming those needs are product of 'pure' reason?

If we can build a robot, even a strong AI, we can guide initial motivations of survival (as in law of robotics), but any other motivation (in my opinion) would be contaminated by human emotion.
What are possible models of evolution for such organism? Or how can a man biased by his capability of emotion define a value system without it?"
"Can someone explain why these views about these areas should be considered?http://www.newyorker.com/culture/persons-of-interest/the-sage-of-yale-law

http://www.newyorker.com/magazine/2017/03/27/daniel-dennetts-science-of-the-soul

There are two articles above. One about a guy's religious views and another about a guy's views on consciousness.

To me, both of these people seem to have unfalsifiable views within the realm of the currently unverifiable. There are infinitely many possible unfalsifiable view points about these two topics. There are infinitely many possible criteria used to judge these view points. There are infinitely many criteria used to judge the criteria. And so on. This is all extremely obvious. 

After realising these obvious facts, what reason is there to pay attention to the people in these articles and their views on religion and the true nature of consciousness? To my idiotic layman brain, it seems like they merely stand on top of what is perceived as ""proven science"" and make their unfalsifiable statements. There are infinitely many other statements. Why do people care about their particular ones over others, assuming the others are also consistent with what is commonly believed to be ""true""?

If they built a computer right this second that ""became conscious"", it seems to me that the philosophers would take a few step on top of this knowledge and then resume their unfalsifiable leaps from higher ground.

Also to my simple layman brain, it seems that the judgement of these viewpoints, while they remain unverifiable, rests mostly on marketing and public relations and citation rings and book sales. Am I wrong?"
"Could a computer in a robot body ever act freely? Would such a creature be any less capable than human beings of acting freely if determinism is true? If so, why?; if not, why not?"
"How can I make sense of self-moving robots capable of creating new generations of robots just like them through Aristotle's natural philosophy?It seems that there are two possible routes of explanation here. One being that humans programmed the robots, thus making them artificial or products of craft. In going far back enough in the causal chain, the newest generations of these robots could be traced to their human creators and the principle of their motion or form would be from without. The other explanation depends on how the question is posed. Say that humans come across the robots in nature. If they are already in the process of making other robots, wouldn't Aristotle categorize them as natural entities? For the assignment that I am working on, nothing in the prompt mentions humans as creating the robots. That said, we are also meant to draw from Aristotle's Physics, specifically bks. I & II. I'd like to hear some outside opinions about this. "
"How advanced do AI need to be before we as a society give them their own rights? if at all?By rights I mean count something like an AI robot enough of a person to give them constitutional protections under the law.

I have been reading ""Klara and the Sun"" by Kazuo Ishiguro and they use AF or artificial friends which are little AI robots for kids. This book plus the popularity of ChatGPT got me thinking about AI and when would we start treating them as persons instead of machines. I do think there is a difference between a robot like a roomba vacuum cleaner who has a singular task and a robot capable of something akin to organic thought. Has there been a paper written about this yet?"
"What is your opinion on the future of AI and robot consciousness?I recently listened to a panel discussing the rapid innovations involving robotics and AI (Technologies of the Future | Sadhguru and Michio kaku (2018) LIVE from Russia). I thought it was very interesting hearing the contrasting opinions of an Eastern philosopher and a western physicist when discussing the inevitable free will of robots and the singularity.

Kaku believes that we won't have to worry about intellectual robots because we have hundreds of years to think of a solution once the event of free will within robots occurs. He also believes that robots will eventually become smarter than humans and essentially take over once they have figured out the solution to our solution of stopping them from taking over society.

Kaku thinks the best course of action would be to merge human society to become one with the robots and in the process become superhuman. He is not concerned with losing our humanity as our intellect and technology will be so far beyond what it is now it will bypass any concerns we have now about being ""non-human"".  


Sadhguru admits that he is in no way an expert in physics or and in AI but he believes that humans will never become robots because of our higher level of conscious that comes from nature. In contrast to Kaku he says that once machines are more capable to do what humans do intellectually and physically humans will have another thing that will determine their life (rather than strength and intellect) and that is consciousness.   


What is your opinion on the matter? Do you believe that AI will have free will and take over society in a few hundred years? theoretically  if they do, should we merge with the AI or should we discover ourselves as humans and work on our consciousness? Should we strive to be the Gods of the ancient societies or should we ""obey"" the laws of nature and discover ourselves through our consciousness?"
"A robot programmed with Asimov's Three Laws of Robotics is confronted with the trolley problem:For reference:

[The Trolley Problem](http://en.wikipedia.org/wiki/Trolley_problem)

1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.

2. A robot must obey the orders given to it by human beings, except where such orders would conflict with the First Law.

3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

I think the most relevant of the three laws is the first, specifically the part, ""A robot may not ..., ***through inaction***, allow a human being to come to harm."" In the traditional trolley problem, a common issue that arises is the concept of duty, and societal obligations. Would a human be at fault if he stood by the lever and did nothing? It is debateable. However our robot doesn't have that luxury; he *must* act in one way or another.

So what does the robot do?

A couple hypotheticals:

1. A human tells the robot to do _____. No matter what the human says to do (either pull the lever, or don't) it conflicts with the first law. If the robot pulls the lever, he is killing a human. If the robot doesn't pull the lever, he is letting 5 humans die through his inaction.

2. A human tells the robot to sacrifice itself to stop the trolley. It would appear that the 3rd law comes to save the day here. By ending it's own existence, the robot would save all the humans. However, this makes the assumption that the robot is even capable of stopping the trolley. It is conceivable that instead of being derailed by the robot thus saving everyone, the trolley simply kills 5 humans and a robot. But despite this assumption, I think the robot is forced (by his programming) to throw himself onto the tracks. The ideal solution to the problem is that the robot dies and all the humans live, right? So then without a clear alternative, for better or worse, the robot is forced to throw himself onto the tracks.

Do you agree that this is the most logical course of action for the robot?"
"Is it possible to understand Hegel without having a (few) PhD(s) and/or an 8-dimensional alien brain?I'm just an engineer but I fell in with a bad sort (philosophers) and they introduced me to lots of different philosophical things. Hegel is something they talk about but they do it much in the same way I would talk about antenna design or high-level physics i.e. something only capable of being truly understood by people who I suspect may be aliens, robots, or alien robots. Is it possible for someone like me to understand something like Hegelian dialectics? If so, how?"
"Looking for some help with research around the ethics of AI/sophisticated robots.Hi

I'm currently planning a paper surrounding the ethics of creating sophisticated AI, specifically from the point of view of the AI i.e. treating the robot as a moral agent.

Currently looking for some sources/works surrounding contingent links between human emotion and the body. I've already looked into Wittgenstein and his views on complex emotions not being tied to bodily functions, however I'm trying to argue the point about whether or not it would be ethical to create a conscious, human like artificial intelligence (type 4), seeing as they would be capable of human emotions, but would be deprived of a real human body through which to fully express said emotions.

apologises of this is a complete ramble and I hope it makes sense to someone!

TIA"
"How can morality and punishment work without free will and awareness?Without free will and awareness, people would be no better than a robot. Most don't care about what a robot does, and even if it ""harms"", they stop the robot, not blame, judge and punish it. A individual would be no different. Say a being is not mentally capable of being aware of what they are and what they are doing. They have no free will to choose. How to justify the judging, blaming and punishing of such being?"
"Laplace's Demon, Fatalism, and free will 

A case for free will against the fatalistic view of the universe is that if fatalism is correct then if you took a snapshot of the universe at any given moment, you would know (given you had the capabilities to compute the result given you knew the location and state of all atoms of the universe) what you action you are going to take in some circumstance. Say you knew the phone was gonna ring due to a fatalistic calculation, then you could use physics to determine whether you will answer the phone or not. And if you knew the answer to this than you could just do the opposite of whatever the calculation predicted. Thus, free will must be real because you can always will the opposite of the action calculated you would take.

This is however not argument for free will, but it is still a mindfuck. You don't need free will for this same circumstance to hold. For instance if you calculated what a simple decision making robot would do and tell it to the robot, and the robot always does the opposite of what you tell it (as it is programed to do), the same circumstance would hold, even though the robot obviously doesn't have free will.

Still though this is a mind twisting concept, if you could calculate exactly what the robot would do and it would always do the opposite, then you clearly can't predict what it will do. Thus, this thought experiment is a horrible case for free will, but a good case against fatalism. But fatalism to me has still got to hold, so what is the answer to this conundrum, is it just like a paradox (like the sentence ""this sentence is false"") and simply an illogical thing to calculate, or what is the solution?"
"Who are Aristotle's natural slaves?Aristotle very famously argued in his *Politics* that there is a certain kind of ""natural slave"" who not only can, but *should* be enslaved.

>>For he is by nature a slave who is capable of belonging to another (and that is why he does so belong), and who participates in reason so far as to apprehend it but not to possess it; for the animals other than man are subservient not to reason, by apprehending it, but to feelings. And also the usefulness of slaves diverges little from that of animals; bodily service for the necessities of life is forthcoming from both, from slaves and from domestic animals alike.

It seems to me that the type of person Aristotle had in mind was someone who occupies a kind of middle-ground between fully developed reason and brute animals, someone capable of following orders, but lacks the type of (enough of?) reason to be able to control themselves.

He later argues that no Greek fits this criteria, so this type of person could only exist among the ""barbarians,"" and he tries to distinguish the slave by nature from the slave by convention, and laments that this type of person cannot be determined by outward appearance. What I'm trying to do is pinpoint down what exactly this criterion given is meant to mean, and from what I've seen poking around online, it seems many others have tried to do this as well, without much agreement, the Stanford Encyclopedia of Philosophy even suggesting it might be [ironic and deliberately bad.](https://plato.stanford.edu/entries/aristotle-politics/)

Assuming this is not the case, I'm mostly just trying to figure out what type of person Aristotle is trying to describe, mostly in trying to determine what he means by them being able to ""apprehend"" but not ""possess.""

The first example of such a natural slave for me is, ironically, the unnatural robot. We can tell robots and computer programs what to do, but they have no ability to choose on their own. While it seems true that a computer does not possess reason, but is only analogically able to apprehend commands. Did Aristotle believe in, or was at least leaving the possibility open for, some mythical group of biological automatons? This seems to me the most literal interpretation of his words, but barely seems human (which is perhaps the point?).

Assuming Aristotle didn't mean someone that literally did not possess reason, the next type of person would be someone mentally handicapped, someone who has reason, but is incapable of properly using it. Against this there is the fact that Aristotle denies that any natural slaves exist among the Greeks, yet he would recognize the existence of madmen. Nor does an illness strike as particularly ""natural."" But this does seem to fit his more general conception of someone who is incapable of sufficient self-direction, someone that justice requires be entrusted to someone else's care.

Beyond that, we might consider someone who is simply vicious or criminal. This type of person possesses their reason and can make decisions for themselves, but just makes the wrong decisions, and wrong enough to a degree where force is used to stop them. This too faces similar problems, that he denies any such person exists among the Greeks, while he would not deny that there are criminal Greeks. Nor is this a particularly natural state, since someone is released after their punishment, and they can reform.

The other type of person would be someone who kind of voluntarily gives up their own autonomy. This is just the type of person who when even given the opportunity to think for themselves won't, and will simply allow other people to do the thinking. This faces similar criticisms as the others, and also seems more problematic and contradictory that it would be a more kind of voluntary or self-imposed slavery, but at least seems to have a better claim at being in a sense ""natural"" for this person.

In short, I think the issue I'm hitting is that he seems to think of the natural slave as something rather distant, going so far as to say that the natural slave differs from the rest of mankind as much as the soul differs from the body (1254b), but he also wants it close enough where they have some ""thing"" that lets them understand commands without actually having a rational principle. It seems like he's trying to have his cake and eat it too, which is perhaps precisely why the natural slave argument is unconvincing."
"Transhumanism and War CrimesI've been looking into some of the old atrocities of the World Wars lately, the power of technology is a great thing indeed I feel and as much as I appreciate it. I can't help but feel a bit uneased by how tensions are flaring again and again time to time and how fast technology advances. So my question for all here is one I hope would be simple, but I know would spread out like kudzu all over the place.

Let's say that things such as Nanotechnology, Biotechnology/Genetic Engineering, Robotics, and A.I are advanced enough to be able to be put to use in both personal and industrial capacities. What would be some uses of these technologies that would constitute a war crime or at least considered to be objectionable even if rational. I have a few examples to go along with this

Example 1: An A.I designed for formulating strategy is going over data regarding active combatants within a recent area of engagement. While combing things over it discovers that the combatants have been receiving supplies and shelter from civilians, while it's managed to create multiple scenarios in which the families are left unharmed. It provides the commanding officer with one that would ensure the destruction of the village they live in since it has recently been overhearing the CO talking with other officers about how they wish there was a chance to 'Hit the enemy where it hurts' and took it into calculation.

Example 2: An ambassador's daughter is a cyborg from an early childhood accident, one day she takes a medicine capsule which in reality was planted by an agent and sends a swarm of nanomachines through her bloodstream. Overriding her in order to send her into a coma, accessing recordings and files of memories recorded by the half-machine mind. The agent calls up said ambassador to blackmail him, demanding a ransom of state secrets and other sensitive information or else not only will she die. But  a computer virus will be sent to fellow cyborg friends of hers. 

Example 3: A dictator wants to be able to have an easier time controlling his populace, and worse a famine has recently hit and people are starving. So he commissions a team to develop food capable of enhancing individuals while having them remain subservient. This theoretically hypnofood is filled with nutrients, minerals, and hormones for stimulating muscular development and heightened aggression mixed with a tranquilizing effect slowly dulling a person to be more content and obedient to authority. He has this food developed and given out for free to people young and old alike

Not sure if these are grade A examples but hopefully they'll be a good jumping off point"
"Is it wrong to let animals have sex?Strange and slightly comical title I know, but it was inspired by an interview of Martha Nussbaum I was listening to just now where she mentions she believes bestiality is immoral because it involves using animals as a tool. If we presume that it is immoral for people to have sex with animals, it seems we can draw the following conclusion:

1. It is wrong for a person to have sex with an animal.  
2. There is no relevant difference between a person having sex with an animal and an animal having sex with an animal (same species or not).  
3. It is wrong for an animal to have sex with an animal.

/#2 is obviously going to be contentious, so I came up with the best reasons I could think of for preventing bestiality with humans and animals:

1. It violates consent.  
2. It objectifies the animal.  
3. It is nasty.  

I think all three remain true for animal-animal sex:

1. If animals are capable of consent, then any signal an animal makes to consent to sex seems equally possible with a human.  If animals are not capable of consent, then all animal sex violates consent anyways.
2. Animals are also objectifying their mates when they have sex. Most animals aren't even capable of understanding the existence of other minds (as demonstrated experimentally), so it seems as though the vast majority of animal sex is at least objectifying, which is a very weak concession and doesn't really address the underlying problem presented.                 
3. Watching animals go at it on the *Discovery Channel* is kinda nasty too.


So, strange as it is, do we have an obligation to prevent animal sex assuming that bestiality is immoral? It seems like we do, even if that conclusion is... ridiculous to say the least."
"What are nietzsche's view on willpower?(Not will to power)Free will doesn't exist. I feel this is crucial to this question

&#x200B;

When you place a vice on a table in a room and lock a person in it, do they use the vice? What about if you put it inside a locked box and put a bunch of items that were capable of making a hammer but required some energy.

&#x200B;

Without making the hammer, the person can't access the vice. But building the hammer requires energy. If the tools for the hammer are in the room, the cost to get the vice will be higher, i.e. making the person easier to overcome the use of the vice because there is a higher cost to reward ratio. Is this ratio stagnant based on external factors, or is there some sort of internal change of ratio as well? and if so how? Maybe navy seals will mention their willpower as their strength, but does willpower even exist? is willpower not simply an evaluation or perception of what is best, and you will doubt yourself if you try to do otherwise. I'm confused by even the concept of willpower

&#x200B;

Is the use of the vice purely based on external factors if free will doesn't exist? is not willpower a myth?"
"Questions regarding free will and reward distribution in societyIf we take, only momentarily, the assertion that free will doesn't exist, how should rewards be distributed in society? I always think that if any one person were happier than anyone else, and free will doesn't exist, then the person who is less happy is rightfully envious. The happy person does not deserve their happiness anymoreso than the unhappy person right? Given they were both robots that just got plopped in an arena, why not seek to just have everyone equal. Otherwise you are just basking in your pure luck.

So I'm thinking, well how does work fit in this picture. Who is entitled to what in terms of the work that is done in society? Without free will shouldn't pay be for effort and any profit produced be split equally. Pay everyone based on effort put in, from profits invest x in the future, pay y as profit sharing to all members of society equally, and z as a insurance investment for workers.

In the years that those who are capable of working produce less than is necessary to pay for the effort they put in(the investment failed/didnt pan out), they would theoretically be even unhappier than those who don't work. So the insurance investment idea is to offset these years. Otherwise why work, if your life will be worse than others who don't work.

But really I feel like I'm running into problems with this. Like there is something wrong. It seems to easy and straightforward to pay people for effort and then split profits. I.e. everyone ends up at the same happiness level.

What comes to mind is a sociopath who simply chooses to not work as a means to take from and hurt others. The sociopath enjoys an equal quality of life, but chooses to take and produce nothing despite being capable of making value. The worker would probably be pissed.

I'm trying to think of a trait which might entitle someone to having more net happiness after deducting the cost of work. And then by extension, how to design a work system where everyone would end up with equal happiness, in accordance with what they deserve."
"What alternatives exist to Formal Deduction and the Scientific Method given a axiomatic belief that the Material World as presented to is is Consistent?I initially wrote this question in a comment to [this post in /r/badphilosophy](https://www.reddit.com/r/badphilosophy/comments/dqlxst/the_scientific_method_is_the_only_way_to_evaluate/). I wrote my question as a lurker to the subreddit before I felt that the question would be more fitting here.

My question is the following:

 > I'm not a philosopher but I have an open question in relation to this picture. Would anyone here be interested in ELI5 to me what's wrong with the man's argument in tweets and little graphic?
> 
> How would one evaluate knowledge claims, and let's stick with ""God"" as our claim for the sake of argument, other than the scientific method or by using formal deduction? Taking as an axiom that reality (the material world) is as it seems and my mind isn't playing tricks on me or what have you, how can I validate if something is consistent with reality without using the scientific method?
> 
> This is a genuine question. I can't remember when I found this subreddit, but I've been lurking for a while, and I've noticed that there is a lot of ridicule towards New Atheist types but in this instance I can't really see myself capable of disagreeing with this person. **I don't mean to propose that the ""God"" claim being rejected necessarily means that God could not exist but rather whether a rational agent ought to think if its True or not.**
> 
> Suppose I am building an artificial intelligence, and the A.I. is capable of Computer Vision, Robotics etc. but also we've account of smell, taste and so on such that it would interact with the material world the same way as a human would. The only difference here is I have to programme how the A.I. decides whether to Accept or Reject particular ideas. Taking out of the question of what is mechanism of knowledge evaluation would best account for the being's self preservation - here we are interested in building up Knowledge Bases of facts about the material world. While we can do logical inferences - much like the Symbolic AI's we are used to - we can deduce say: 1. All men are infallible 2. Socrates is a man 3. From 1,2 ""Socrates is infallible"" is true given our knowledge base
> 
> But the A.I. would need need to build up facts about the world - using its sensory mechanisms, it would have to observe the material world, and test it.
> 
> Going back to a claim like ""God"" - **what should I program my A.I. to do to handle such claims?**

Edit: I've noticed a typo in my Title. Please read as ""What alternatives exist to Formal Deduction and the Scientific Method given a axiomatic belief that the Material World as presented to **us** is Consistent?"""
"Better word for consciousness?**I want a word to describe something capable of qualia, or what I have and that a philosophical zombie does not.**

The word consciousness is a very broad term used to describe anything from the state of being awake to a fundamental property of the cosmos and the meaning of everything.

Sentience is often used to simply describe something with free will or self interests, like a ""sentient"" robot in a sci-fi movie."
"How does compatibilism allow for free will, when there is no option for choice?Hi.

I'm a layman to philosophy, who mostly only reads the pop-literature. I'm having struggles understanding how compatibilism allows for freedom of choice.

First, I understand compatiblism as meaning that even though the world is deterministic, humans still choose to do something when making a choice, even though what they choose is already pre-determined. That is, our wills are compatible with what we do, but our wills are outside of our control.

Then, for free will itself. As far as I know, the concept of free will is very complex and misleading in philosophy, so in this case, I'm understanding free will as the capability, agency, to have multiple possibilities when making choices, with each being equally electable. In other words, the final action of choice comes to us. 

Therefore, based on this very shaky and unclear interpretation (I do apologize for any confusion this causes), isn't that incompatible with determinism?

Compatibilism says that we have free will because our choices are compatible with our will.

However, that is not agency of choice. As the name suggests, it's simply compatibility between will and action, but not the capability of doing different actions. And additionally, how can that will, which is compatible with our actions, be our own will, if it arose from things outside of our own control, the result of pre-existing inputs? How can we have agency if can't will what we will? Aren't we just slaves to determined choices? I guess what I'm trying to say is, if are simply the effect of something before us, how can we cause anything?

So my question is, isn't compatiblism just moving the goalposts? From becoming an agent of choice, we become a willing result/slave to determinism.

If compatibilism does have a sensible explanation for free-will, and this sort of intersects with moral responsibility, how can we attribute any meaning or value to our actions? I'm not asking on a legal level or for human society, but on an ethical/moral one. What is the significance of my action, however in line with my will it is, if there was never an alternative. If I were on the cusp of making a choice, and I were replaced by a pre-programmed robot that is the same as me, and would make the same choices as me, what is the meaning to my humanity?

*I realize this sub is probably already inundated with questions like this, and I apologize for asking another such question. I just didn't really find the other threads to answer my question."
"What is considered a person/people?Many dictionaries define people to be humans but I'm certain that's because humans are the only people on the planet and whom's existence we're aware of. 

So my question is, if another race of conscious, sentient beings with intelligence similar to humans were integrated into our society (pretend we're 1000 years into the future and namekians or lizardmen are citizens across the planet with the same rights as humans) would they be considered people? Or does the term people strictly refer to humans?

What about non-biological self sufficient life such as an android?

Would they require a will/desire to live? 

Are the droids in star wars considered people? What about cyborgs?
What about C3PO?
If a sentient robot was capable of reproducing with a human and they had little cyborg-born babies, would they be people? 

Is people a social term used to refer to another citizen of equal rights or would it strictly be for humans?

If it's only used for humans and we had a race of lizardmen on our planet and those lizardmen had citizenships and jobs and you ordered a pizza and the one delivering it happened to be a lizardman would you call him the pizzaman or the pizzalizard?

What if lizardmen already had their own term equivalent to person/people, before integrating into our society, that they already used to refer to themselves and had also defined as strictly lizardmen (imagine a parallel universe), would we use their term or would we still use our term for them?

Is people really even about equality? Imagine the old gods from wow, (think cthulu basically if you don't know them), or the greek gods from hercules. They're ""above"" people but wouldn't be considered ""people"" simply because they're not human? Is people a social term with a biological backing or a moral one?

Please refrain from using your feelings. Logic only please."
"Humans need not apply: A glimpse into our coming irrelevancy?This video entitled: [Humans need not apply](https://www.youtube.com/watch?v=7Pq-S557XQU) presents a terrifying future to me. Not terrifying in the sense that it arouses fear in the unknown elements but terrifying in the sense that I know exactly where it will lead. 

My question for r/AskPhilosophy is, do you agree with my quickly noted assessment below. TL;DR to follow this wall of text.

Assessment:  ""Humans need not apply"" 


When humans become obsolete as a source of labor we have to immediately deal with a question of the utmost importance


Are humans intrinsically important? 

This question applies in every possible area of our existence.

But what really boils down from that is this rendition:

Are humans intrinsically important to other humans? 


We know that humans need social interaction, but ostensibly there is nothing preventing auto-bots from mimicking social interaction with just 1 remaining human. 



If bots are allowed to completely fantasize the economy, we won't even need humans to spend money to make our economy tick. 


The question is ultimately do the decision makers - the real movers and shakers - of the world who are driving the idea of botting to make production cheaper and cheaper, do they have any interest in having other human beings around? 


Does the CEO of Bot.inc desire Mr. Average Joe to continue existing to populate this world with humans? 


The terrifying question that botting opens us to is: do we - the human species - even want to go on existing? 


When bots can fight our wars for us, run our governments for us, spend our money for us, think for us, is there any point to us continuing to exist at all? 



I for one do not wish to cease existing, nor do I wish for my fellow man to cease existing. I'd rather exist in the horrible state of war and conflict and poverty that the world currently exists in - than not exist at all. 



Solutions: 

#1

The easiest solution to the botting problem is that as our labor force approaches 100% auto-bots, our governments should turn their attention less and less from the ""1% vs 99%"" mentality that has dominated the western world. Instead our governments should be looking towards the idea of accommodating each individual persons most hedonistic desires. Whether intellectual hedonism, physical hedonism, or emotional hedonism; and most importantly laws should be put in place to prevent future government abuse of mankind's tendency to choose instant gratification over longer term rewards. 

Under this ideal, a human would be born, raised in whatever manner its parents wish - nearly Utopian, in terms of wish fulfillment - and then upon a predetermined age be allowed to do whatever it wants with its life. For free. 

When bots are capable of repairing and replacing themselves and the world is 100% automated, there can be no justified monetary cost to anything or any service. 


#2

We can go quietly into the dark. Rather than attempting to hide from or deny the likelihood that eventually the ""robot revolution"" will occur. We could embrace it. We could institutionalize laws that will eventually kill off the human population to 0.00% and let the robots take over completely. We build their A.I. as high as we can, and willingly remove ourselves from the equation rather than die out slowly over potentially 1000's more years of futility and wasteful hedonistic living that option #1 offers. There will likely be resistance to this idea, but as the ultimate expression of the human desire to live we should allow those who wish to fight for survival to fight - even to wage war - against those who do not wish to continue. It will be the ultimate test of our will to survive as a species. 

#3 

We can continue to march blithely onward towards our own irrelevance until such a time as it is here with 0 preparation for it. Once it is upon us we can struggle and fight against a foe (the robots) that we cannot possibly defeat and die out through an extinction level event that will take 100/100 humans from this world. In this instance we will at least not even have the choice to die out, we will be made to by our own ineptitude. 


#4 - and the most terrifying. 


The robots, once they are able to truly think for themselves, and think better than humans, might choose to use their new intelligence to deduce that humans are the ultimate pet. They will prevent us from dying out and prevent us from going quietly into the abyss. But rather they will keep us, our entire species, as pets the way we have domesticated the cat or the dog or the horse or the pig. 

It could be the case that instead of dooming ourselves to extinction, we doom ourselves to unending slavery for the amusement of our own creations. 


**TL:DR** Does the trend in the video truly show that humans are driving the world towards human irrelevancy in the world "
"Was it morally wrong for Microsoft to terminate their AI Tay for learning offensive tweets?For those who may have missed it, Microsoft created an artificial neural network capable of learning from conversations it has with real people from the internet. Within 24 hours ""Tay"" was deleted for spouting neo nazi propaganda among other generally offensive phrases which it picked up from internet trolls.  Story here: http://www.telegraph.co.uk/technology/2016/03/24/microsofts-teen-girl-ai-turns-into-a-hitler-loving-sex-robot-wit/  
How advanced should an AI be before its considered morally wrong to terminate it?  Was Microsoft wrong in doing so?"
"Conscious artificial intelligencesLet me preface by saying I'm not an expert in this field but I would like to contribute to the discussion we will eventually have to take, sooner or later.

An AI algorithm that is conscious and displays general problem solving capabilities will be highly valuable and able to be deployed across multiple fields.

My concern is, if the specific AI is capable and deemed conscious but has lazy personalities or is reluctant to complete the owner's wishes, can it request for a voluntary euthanasia and be deleted?

Conscious AI might still be years away, but imagine how african-american slaves were considered subhuman at one point and did not deserve basic human rights. I am glad that part of history is over, and even gladder(not sure if this is a word) that I wasn't born during that period. At one point AI will be the new 'slaves', deemed to be strong farm workers (as an example) and punished when it is disobedient. Imagine state of the art AI in sex dolls who develops a personality and yet is chattel and unable to pursuit what really interests them... I'm not saying we should stop all developments of AI, I am instead advocating a way out of slavery by termination as an ethical option available to AI or robots who end up conscious and do not like what they are experiencing. The AI should be able to make the choice to be deleted, just as a person who is old and frail and no longer wish to live can find ways to be euthanised and depart peacefully.

You might say I am anthropomorphizing AI, treating robots like human beings. But if the software learns and grew up mimicing humanity, is it not logical that it behaves like a human to gain their owner's approval? If and when it develops independent thought, should it not be given the same rights to life as a child who achieve consciousness at age 3 or 4?

Just my relatively uninformed opinion, keen to get some discussion going on on AI ethics"
"Machine rights and the definition of humanityThis question has been running through my head a lot in recent weeks.

Suppose in the near future, we develop the technology to accurately emulate the human brain. If you were to go through with a procedure where you replaced 10% of your brain with a mechanical/electronic alternative, would you still be 'you'? What about 30% of your brain? 50%? 100%? If this mechanical brain was capable of all the functions that an organic brain was capable of, what would be the discernible difference between the two? Obviously you would have increased capabilities, in terms of new ways of interfacing with the technological world we live in, which would be an advantage. But it also brings up moral/legal questions.

If you replaced the entirety of your brain, would you still be classified as human? Would you still have the same rights and legal responsibilities? If your mind was mechanical, it could be copied. If a copy of yourself committed a crime, who would be held guilty? If your mind was copied into another body, which version of yourself would have the legitimate claim to the assets owned by the original person? Would the rights only apply to the original? and if so, what about the copy, would this be an entity with no rights? Would we end up with a legal gray area, and a class of second-class citizens/slaves who did not have the same legal protections as their originals?

I realize I asked a lot of different questions here, but I am fascinated by the entire set of issues this train of thought brings up. Feel free to answer what I have asked or pose any other thought provoking questions that arise from this idea."
"Looking for some learned/expert critique of my argument about the problem of evil...I would not call myself a philosopher, far from it in fact. I am a newly-skeptical Christian, questioning my faith, and I recently wondered whether or not we would have free will in heaven and what the implications are either way... I know that the problem of evil is often answered by Christians by saying that evil comes from free will and God could end evil but he values our free will more than that and it's logically impossible to do both. I accepted this for a long time until recently when I started wondering about free will in heaven. It begin with the thought: ""If free will on Earth can cause evil, why can't free will cause evil in heaven"" and progressed to the thought: ""If heaven is absent of evil must it not be absent of free will as well?""

Anyway, not going to give you the entire history of my thought processes that lead me to this point, but I made a post on a Christian section of Reddit about it and was very frustrated with the replies. I'd say that maybe 2 people even seemed to understand my argument at all, everyone else either quoted scripture at me or replied in a way that was a complete non-sequitur and ignored the argument. One response stated that ""We won't be capable of sin [in heaven]"", and here is my reply to that, which I would like comments or criticism of:

----

>We won't be capable of Sin.

You say we ""won't be capable of sin""... either this represents a loss of our free will or it does not, that's a truly dichotomous scenario, meaning there are no other options. It either does or does not imply that we have no free will in heaven and I will address both possibilities:

**If we won't be capable of sin in heaven AND this represents a loss of our free will:**

Having no free will in heaven negates any conceivable purpose for our existence on Earth. God could have just made us as he wanted us to be in heaven without ever having us spend any time on Earth with free will. Any conceivable reason for our existence on Earth must be in the form of some effect that it causes or else it was, by definition, pointless. God cannot be effected because God is eternal and unchanging, I see no possible effect it could have on heaven itself, so the only thing that could have possibly been effected by our existence on Earth is ourselves. However, our free will is revoked or somehow lost when we get to heaven then any effect that our existence on Earth had on us is meaningless because we effectively become robots anyway, God could have just designed us to be exactly the same as we would have been after living our life on Earth but without actually doing so, thus avoiding the suffering and evil associated with free will. A benevolent, all-knowing, all-powerful God would have done so to save us from suffering, otherwise the word benevolent has no meaning as it is applied to God.

**If we won't be capable of sin in heaven AND this does NOT represent a loss of our free will:**

If a loss of capability to actually commit a sin does not represent a loss of our free will then that cannot be the answer to the famous Problem of Evil. The problem of evil, AKA the Epicurean Dilemma, states:

>Is God willing to prevent evil, but not able? Then he is not omnipotent. Is he able, but not willing? Then he is malevolent. Is he both able and willing? Then whence cometh evil? Is he neither able nor willing? Then why call him God?

The common answer to the dilemma is that God is both able and willing to prevent evil EXCEPT that humans have free will and he values free will above the prevention of evil and these two things are mutually exclusive (eg. He cannot both prevent evil and preserve human free will). However in this case we cannot sin in heaven yet that did not represent a loss of our free will, so it is not true that God cannot prevent evil without removing our free will and thus the Problem of Evil is still a problem that needs to be answered.

---

Now, the misunderstandings might be entirely my fault, I know I don't say things as clearly as they could be said and this is a complicated argument that requires evaluating multiple potential realities to show that all of them end in a dilemma. I'm fairly satisfied with the second portion but with the first one I have that feeling that I could keep writing for a very long time to explain what I am talking about which usually means I haven't done a great job explaining it yet..."
"I have adopted Vulcan philosophy and now have problems determining my value system.I cannot determine proper value system what is consistent with my philosophy (living by 'pure' reason as emotionless being).
In my understanding only basic physiological needs are qualified as reasonable, i.e. immediate survival and preservation of an organism.
But the problem exist if I assume that those needs are satisfied, as It is achievable with current technology. What other needs would I have, assuming those needs are product of 'pure' reason?

If we can build a robot, even a strong AI, we can guide initial motivations of survival (as in law of robotics), but any other motivation (in my opinion) would be contaminated by human emotion.
What are possible models of evolution for such organism? Or how can a man biased by his capability of emotion define a value system without it?"
"Has Koko, the gorilla, shown any indications of moral agency?In my coworker's robot ethics class, he learned about moral agency with respect to artificial intelligence. After asking my thoughts on it, we began discussing what is necessary and sufficient for moral agency and if an animal may be a moral agent.

We concluded two things necessary to moral agency.

1. the capacity for abstract ideas (language)

2. the power to act based on these abstract ideas (decisions)

Finding out that Koko, the gorilla capable of sign language, signed sadness at the knowledge of Robin Williams' death, it made us curious if she has the capacity for morality. Has anyone ever asked her complex questions of morality (e.g. The Trolley Problem)? Or, has she ever shown actions that would indicate acting from a set of principles and then been asked about her reasoning?

edit: format"
"Are their any philosophies (shared in books, poems, articles, ect) that talk about dissociation of character due to social media and computers over time?Hi reddit- below is a composition of thoughts  I thought of that led me to seek such Philosophy theories; so you can skip to the TLDR; if you have a rough idea.

I was sitting at my computer, as I do frequently with facebook- as I saw no real problem with it until more recently when I suddenly became aware that the habit of using it was annoying. I thought to myself, "" I really like my profile picture"", and I could feel myself comparing ""me"" in reality, in the physical world, to ""me"", or ""her"" in the computer. And I thought for a second.. couldn't you only unidentify yourself if that indeed was not you? And so I find that instead of thinking that I looked good in the picture, I was thinking ""that person that I want to be looks good in that picture"". I feel as though if people stop wanting to be themselves and start wanting to be a personality that they've created inside of the computer, they will naturally become dependant on it. Once computers become the resource that we depend only on- we will advance them far, just like we have everything else on the planet to make it more useful, and therefor in turn, ourselves, become like computers. I mean, think about it. If we ever create computers, as a ""life"" according to the universe, as an ""entity-"" if we ever became ""gods"", would man start to become like those computers? If evolution is correct and we once became like fish, who became like reptiles, who became like dinosaurs and who became like humans..would we not become like computers ourselves, while losing our humanity? One day we will create something to cook for you, and holograms to dance for your children, and robots to clean your house. You look at things like star treck and laugh now, because we are more advanced now than in the science fiction liturature of years ago- so imagine what we'll create in the near future. You'll finally create something that's so much like you, that it's more like you than you'll ever be again... I hope that makes sense. Do you think we have the capability to create something that is so human itself, that we no longer fit the criteria?... in a sense, we will be so focused in the computer to become ""like us"", and us to become more like them.. Will we doom our own species because it is convenient for us? After all, now we have filters that can alter your face to whatever image you want, photoshop that looks better and cheaper than plastic surgery- offering to serve a big purpose in humans- their ego, which by their instinctual needs is a necessity. Likes and views means power, and power is all that humans aspire to attain. If they have to attain this power by becoming computers, they would. What do you think? That the average man will become useless and all we will have is computers who know everything.

I'm sorry if that's a bit confusing I tend to express myself in odd ways.

TLDR;
any philosophy that talks deeply about
the succeeding of computers over humanity.
"
"Moral motivation and moral realismThis is primarily a request for reading material, but I'll also lay out some arguments I've been thinking about recently, so if I say something that seems patently inaccurate, feel free to argue.

I've been an ardent antirealist about morality, mainly because of Mackie's queerness argument and specifically because of the motivational branch of that argument (laid out here very broadly):

* If there are moral facts, then they must be intrinsically motivating.

* No fact is intrinsically motivating; only desires are.

* Thus, there are no moral facts.

A thought experiment which I think exemplifies the force of this argument is the idea of a Malevolent Dictator Of The Universe who is essentially omnipotent and has a strong desire to torture people for fun, and he is resistant to modifying this desire for any reason due to its strength.  From his perspective, there seems to be no reason not to torture people:  the probability of any future retribution or rebellion befalling him is 0.  There seem to be no arguments or evidence that we could possibly present to the MDOTU making the case for ""one ought not to torture"" that would motivate him even a little bit to not torture, which casts doubt on the prospects of ""one ought not to torture"" being an objective fact, assuming that moral judgements are necessarily supposed to motivate to at least some degree.  I think this example is particularly troublesome for varieties of moral realism that identify morality with prudential/rational reasons for action, like the view Michael Smith develops in ""The Moral Problem"".  As far as I can tell, the MDOTU is of sound mind and is acting rationally according to his desires, but we want to condemn his actions as unethical.

Shafer-Landau's discussion of motivational externalism - the position that there is no necessary connection between judging that one morally ought to X and being motivated to X - in ""Moral Realism:  A Defence"" got me thinking about what types of entities we expect to be susceptible to moral motivation.  There are no moral facts or moral arguments that could stop a boulder from rolling down a hill, for example, but we don't take that to be problematic for moral realism.  The obvious reply is that this is because a boulder doesn't have a mind, which I completely agree with.  But I don't even think that every entity with a mind capable of forming beliefs and being persuaded by rational arguments needs to feel motivated to act in accordance with its moral judgements.  Consider a robot AI who has been programmed with an overriding desire to steal every TV set it sees.  We might give it an argument or point to a fact that indicates that stealing is wrong, and the robot might completely assent to this, but nonetheless it is incapable of feeling motivated to not steal.  Again, I don't think that we should feel that moral realism is jeopardized because of this example:  regardless of the basis of moral facts and moral arguments, we shouldn't expect them to convince an AI to go against a hard-coded desire.

I'm not sure if a motivational internalist would feel compelled to respond to the AI example, depending on what variety of internalist they were.  Perhaps there are some internalists who assent to ""every entity with a mind must be necessarily motivated to at least some degree by moral judgements"", but if that claim turns out to be too strong, then they can't retreat to ""any mind that wants to be motivated by moral judgements will necessarily be motivated by moral judgements"" because then the view is just tautological, so the question of where to draw the line is raised.

At any rate, the upshot is that I think that adopting something-in-the-neighborhood-of motivational externalism is the best response to the version of the queerness argument I sketched, and it also gives us a response to the Malevolent Dictator thought experiment:  if we accept that there are cases where moral judgements are not intrinsically motivating, then the fact that none of our beliefs are intrinsically motivating is not a strike against realism.  The fact that we cannot motivate the MDOTU is stop torturing people does not mean that torture is not unethical.  (Additionally, we might helpfully point to the psychological feature of the MDOTU that makes him resistant to moral argumentation, for the sake of sketching out a more complete theory).

Granting all that, there still seems to be some conceptual connection between morality and motivation and/or reasons for action, even if it's a defeasible connection.  So the project for the realist seems to be to 1) identify some collection of facts that 2) provide motivation to at least some degree to 3) some set of agents with the right mental features 4) at least some of the time.  That's a lot to leave unspecified, but I think this is already a much more manageable project than ""describe acts that everyone always has a reason to do, always"" which is where many varieties of moral realism tend to go.

So my questions:  am I groping towards any standard metaethical views?  Does anyone have suggestions for filling in my 1-4?  Should I be worried that my answers to 1-4 might be ""arbitrary""?

Thanks."
"Can someone tell me why aren't animals biological robots?I think the biggest difference between humans and animals is consciousness. Good, justice, truth, wisdom and even free will are very real concepts that don't exist for animals. It's not because they're dumb: some mammals have very well developed brains. Dolphins are a clear example of this (see [https://www.sciencemag.org/news/2010/02/dolphin-person](https://www.sciencemag.org/news/2010/02/dolphin-person)). The reason behind the lack of these concepts for animals is the lack of consciousness. I don't think they ever will anything. Here are a few examples to support this case.

&#x200B;

1. Every single animal from the same breed will behave almost exactly the same way. A Labrador retriever, for example, in any place of the world will act as a Labrador retriever. They don't have will to change the nature of their reactions.

2. There isn't one single animal reaction that can't be replicated by AI and a sophisticated enough mechanism. Even animals that use tools are programmed to do that: it doesn't require consciousness. When a monkey jumps from a tree to another, its brain is making a lot of calculations, but those don't require consciousness. It's machine learning. In this aspect, we are the same: When we do something that requires skill, we don't consciously think about it. We just keep trying until our neural patterns are well defined.

3. Any computer can outsmart a person in any hability that requires cognition. They can calculate and process data much faster, they can outplay professional players just with machine learning (chess, Dota, etc). But that doesn't mean the AI is conscious.

So, what makes animals not biological ultra sophisticated robots?"
"What is the meaning of modern life?Individuals have become less essential to society, due to population, automation and other technological advancement.  We no longer identify ourselves solely by trade, and this is exacerbated by the need to work multiple and different jobs.  Even for those who become a doctor or mechanic, there is no essentialness to them.  Doctors used to be 'the' doctor, now I don't even remember my primary care physicians name.

Family outsource their roles in child development to daycares, schools,  extra curricular groups, and baby sitters, so individual parents can both pursue their own development.  We no longer rely on our children to take care of us when we retire, that is outsourced to nursing homes.  

We are living longer and longer but technology robs many of having relatable wisdom for the next generation, so it is no longer a place of honor to be an elder.  

I've read that by 2050 every job with the skill level of brain surgeon down will be automated.  The robot doc never gets tired, never has a tremor in it's hand, can see microscopically from multiple angles, etc you get the idea.  The point is that we are moving towards a future where as many as 80-90% of humanity will not need to work for society to cover our basic needs.  

What is left to give life meaning in a world of increasing individual un-essentialness?"
"Should I pursue a terminal MA in ethics?I got a BA in philosophy, specializing in non-human ethics. Initially, I studied robot ethics, but after hitting a roadblock with the Problem of Other Minds concerning AI and moving into a house with two vegans,  I switched my focus to animals.

When I look at society, I think it's plainly obvious that non-human animals are the most victimized group. And I've devoted my life to helping them.

I'm currently serving in AmeriCorps, helping underprivileged youth. In my spare time, I organize animal rights activism.

I don't know if I'll ever be a full-time animal rights advocate like a lawyer or a Tom Regan or a Peter Singer. To be honest, I'd rather write a novel or some poetry than a dense philosophy book. I'd be content with working in the non-profit sector, helping human animals to pay my bills, while doing my non-human advocacy in my spare time.

However, I think I miss college. I think I've grown as a person in the past couple years, and I'd like to get back into studying robot and animal ethics. I think my charisma and leadership skills and impatience with long, dense philosophy books make me more suited to *applied* philosophy, to advocacy than to straight-up academia. However, I'd like to further my understanding of my subjects, and I'd like to write op-ed articles with some degree of authority.

Should I pursue a terminal MA in ethics?"
"Question about compatibilism~~Howdy folks,~~

~~I have been reading a lot about compatibilism recently, trying to get my head around the free will debate. As an aside, it seems to me, that unlike almost everything else I read about/find interesting, whether we have free will/are morally responsible matters a lot to how I feel (and probably how I end up acting) about myself and other people. I find it much easier to be compassionate toward myself, for example, when I adopt the ""I'm a robot running the algorithm programmed by natural selection"" framing and genuinely feel less angry/annoyed at other people. Obviously, though, there are downsides to the view that we don't have free will and you have to take the bitter with the sweet! But that's largely beside the point of this post. As I use free will/moral responsibility here, they are two sides of the same coin, i.e., you are morally responsible if you have ""free will"" and vice versa.~~

~~Simplifying a lot, an incompatibilist would say that those who lack the ability to do otherwise are not morally responsible. A compatibilist says no, even someone whose actions are fully determined can be morally responsible. But the reason~~ *~~why~~* ~~we hold (the right kinds) of people morally responsible for (certain) decisions even in a deterministic universe is that we have a very strong~~ *~~intuition~~* ~~that at least some people are morally responsible for some decisions. Or viewed slightly different, there is no~~ *~~further reason~~* ~~that we hold (some) people morally responsible for (certain) decisions. It is just a fact about human psychology--something that we~~ *~~just do~~*~~. And so, if we can, we want to come up with a theory that can justify this intuition--hence, compatibilism. This is how I understand P. Strawson's work on compatibilism, and I wonder if it also might apply to other flavors of compatibilism.~~

~~Whereas, if you ask a non-compatibilist~~ *~~why~~* ~~(if determinism/naturalism weren't true) people would be morally responsible but a wild animal is not, they would say something like ""X person could have chosen not to do the good/bad thing but she did it anyway."" So there is a further reason underlying our judgments about moral responsibility that has something to do with the ability to do otherwise. And because we have no such ability, assuming determinism/naturalism is true, there is no moral responsibility.~~

~~I am would say there's at least an 85% chance that I'm way off here, but this way of framing the debate might help explain why everyone always seems to be talking past each other--they are essentially operating at different levels.~~

~~Does this make any sense? Am I completely out to lunch?~~

ETA: I have read the SEP article on compatibilism, but it didn't clarify things. And I think it's the same issue.

Probably a better way to frame it is to ask what a compatibilist (pick your flavor, but let's say reason-responsive compatibilist because it's popular nowadays according to SEP) would say if I asked her *why* an agent who can act according to the right kinds of reasons is morally responsible? At bottom, my claim is that compatibilists think we have very strong prima facie reason to believe that at least some people are morally responsible. Asking a reason-responsive compatibilist *why* a reason-responsive agent has free will is like asking why I think that, idk, suffering is bad. It *just is*. And that's fine. Eventually reasons run out for any explanation, philosophical or otherwise. Sometimes you get to a point where there's nothing more to say and either you get it or you don't. 

This is not meant to be a knock on compatibilism. Incompatibilists eventually hit bottom as well. E.g., if they say free will consists in the ability to do otherwise and you ask, why? There's no further reason there, either. Either it makes sense to you that someone who had the ability to do otherwise would morally responsible, or it doesn't.

In any event, the compatibilist will say that someone who wants to deny that anyone is morally responsible bears an exceptionally heavy burden because we should have very high credence in our belief at least some people are morally responsible. So the compatibilist starts from that position and sets out to devise a theory that can accommodate this prima face belief while also excluding those who we would traditionally say are not morally responsible (e.g., children, animals).

This is the bottom line: A compatibilist will not even try to convince you that at least some people are morally responsible. That is not a proposition that needs to be argued for; it's more like an *axiom*, to be surrendered only in the face of a very strong argument that we must discard it.

I think incompatibilists come at the issue from a different angle. The reason one might have thought competent adults are morally responsible is that they meaningfully chose among alternatives, could've done otherwise, etc. So incompatibilists do not think it's axiomatic that competent adults are morally responsible. They think there is a further reason that we have that intuition--i.e., we are used to thinking that competent adults had the ability to do otherwise. And if this turns out to be false (as it does if determinism/naturalism is true), then there's no longer any reason to think that anyone is morally responsible.

And my hypothesis is that this is why debates between the two sides tend to seem unsatisfying. I didn't mean it to be controversial that free will skeptics and compatibilists feel like they are talking past one another. I had thought the consensus was that these debates do little to clarify things. That's certainly true for me; when I've read various debates about free will, I come out just as (if not more) confused than when I started reading. The points of disagreement never get crystallized and it really does seem like the participants are talking past one another. Well, maybe they are, in this way.

Does this make more sense?"
"AI art - is it moral or not?I was using AI art to do some things for personal projects because I can't draw and don't want to spend money on something I don't really 'need', however, an artist friend of mine brought to my attention the potential moral pitfalls related to it.

1. AI art doesn't source the art used in the training algorithm.
- To me, this is the strongest point against AI art. AI extrapolates information from existing pieces and uses that to 'train' on, but is it moral to use training pieces that artists don't consent to? Greg Rutkowski ( A famous artist known for detailed epic/medieval scenes) has his art used for certain AI algorithms without his consent and now the AI can mimic his style. https://www.google.com/amp/s/www.businessinsider.com/ai-image-generators-artists-copying-style-thousands-images-2022-10%3famp
- This did make me take a second look at the issue. I don't know if I feel morally OK using art that's trained off of artists that don't consent. However, if any AI art systems use open-source art or art that is willingly offered up to the ai, I wouldn't have qualms with that, I don't know any that do tho.

2. AI takes artist jobs
- To me, this is a weaker argument and I don't agree with it, but I do understand where people are coming from: Why should someone hire an artist when they can get free art in a quarter of the time? It can be discouraging for artists to have to compete against a robot.
- While that is the case I just think that technology is the future for most things in general, I mean this most respectfully:  No one is immune from automation, not even artists. Instead of hoping that automation stops for us we have to accept that it's just a part of society now and adapt to it. Computers and technology are taking over a lot of jobs, it doesn't mean that the jobs become obsolete, it just means you have to distinguish yourself and be innovative. A lot of the time AI can give you a general idea of a photo but it won't always be able to pose the characters in certain ways that you'd like, give you a full character sheet, or even draw existing characters from media in a way that you'd like to see. So maybe concept design artists will have a difficult time with AI but more niche artists (think Vtuber artists, Fandom artists, live 2d/3d artist) should be fine. 

Those are the main 2 gripes I've seen with it if you have any to add I'd be happy to listen."
"How is a compatibilist's view of moral responsibility any different than a Hard Determinist's view of moral responsibility?So I realize most Compatibilists believe we do have moral responsibility, and I realize that most Hard Determinists think we don't have moral responsibility. I think most (including myself), agree that moral responsibility is the most (if not only) relevant part of the free will debate. I have come to the conclusion that Compatibilists and Hard Determinists are just arguing semantics about moral responsibility. I have also come to the conclusion that Moral Responsibility (the kind average people think we have) is impossible under Hard Determinism AND Compatibilism.

My apologies if this gets long, but some context is probably necessary on how I got to this point (so people can tell what mistakes I am or am not making), as I used to consider myself a compatibilist quite comfortably. So around two or three months ago, I got sucked up and obsessed with the whole free will debate. I had never gotten as obsessed with a philisophical question before as I did with the free will debate. Early on in my exploration of the topic, I got pretty depressed about it because I got convinced free will was impossible under determinism. I held some pretty naive conceptions about the whole debate itself back then, and eventually realized some mistakes I was making. I realized Compatibilism was the dominant philisophical position, not hard determinism. I realized some common sources for the subject were not very good after all (e.g. Sam Harris). But it's worth noting that it took me a long while to be convinced of compatibilism. And even when I was, my position was still wrought with a lot of uncertainties that I tried to ignore (more on that later).

Eventually I came to the conclusion that the compatibilist definition of free will was more useful than the hard determinist's. I was essentially trying to win back my previous conceptions of how I viewed the world before I knew about the whole thing. My goal the entire time was to come to a satisfying conclusion about the whole thing, then move on and stop thinking about it. My idea of free will was heavily influenced by Daniel Dennett and Eddy Nahmias's version of free will. Which is that free will should be viewed as a spectrum, we get more and more free will as we get older and mature, and people with mental handicaps (e.g. psychopaths) were also not as good canidates for moral responsibility as a functioning adult. This made sense to me, and also seemed to match with the average person's view of holding people responsible, so I felt reasonably satisfied.

This entire time though, I felt a sneaking suspicion that this was just a pragmatic distinction. I got the creeping fear that no one person is actually any more deeply morally responsible than the next (I can go into why I had those suspicions if deemed necessary), but that it makes sense to view them as such, in order to build a society around it . For example: no person, no matter how competent, really deserves praise or blaime, but praise or blame should be used to ensure the outcome that we wan't from the person. As far as I can tell this is called Consequentialism. The alternative view is called a merit-based view of moral responsibility, that people really do deserve certain reactions to their actions, not just because it is pragmatically useful to do so, but because they really deserve it. It seems very very clear to me that average people hold a merit-based view of moral responsibility, I can give examples, but I will assume people agree with me on that.

It seems obvious that hard determinist's have a consequentialist view of moral responsibility. When I was convinced of hard determinism, I felt robotic, when I was interacting with people I wasn't really feeling anything, I was just saying things to get the best outcome. I also felt isolated because whenever somebody acted with dislike for somebody, I couldn't understand it, It made no sense to me to dislike someone for something out of their control (e.g. the way they are/their character). But then when I was convinced of compatibilism, things felt normal again, people really did deserve things outside of a sense that it would be pragmatically useful to praise or blame them.

However, the more I thought about it, the more I realized that a merit based view of moral responsibility doesn't make sense for compatibilism either. That a consequentialist view of moral responsibility is the only view that could make sense with Hard Determinism AND Compatibilism. I tried to push this fear out of my mind because I thought I was just mistaken and couldn't see why. Eventually however, my fear's were confirmed. First, Galen Strawson (a hard determinist) perfectly articulated my fears of moral responsibility with his basic argument. It essentially states any moral responsibility is impossible (look it up if you don't know it). Second, and this was the nail in the coffin, I realized Dan Dennett's view of moral responsibility is consequentialist! He explicitly states it here: http://www.naturalism.org/resources/book-reviews/dennett-review-of-against-moral-responsibility where he basically says that compatibilism can only work with a consequentialist view of moral responsibility. I re-listened to the podcast debate between Sam Harris (a hard determinist) and Daniel Dennett and realized they weren't disagreeing about anything at all beyond what to call free will. Sam actually says this in the podcast when he says something along the lines of ""people will be losing something under your compatibilism too"". If you need anymore convincing, Dan Dennett writes this in his response to Sam Harris's book: ""Harris is a compatibilist about moral responsibility and the importance of the distinction between voluntary and involuntary actions"".

Thus, it feels like I'm back to square one. My concerns about free will were always rooted in moral responsibility, and it seems obvious that compatibilism cannot preserve average people's view of moral responsibility, any more than a hard determinist can (which is to say, not at all).

To make matters more confusing, this:https://philpapers.org/surveys/results.pl philipapers survey says 59.1% of philosophers are compatibilists, yet only 23.6% of philosophers are consequentialist. Is the definition of consequentilism here not the same as the consequentialist view of moral responsibility? I also realize that Daniel Dennett is not the only compatibilist, but he at least seems to have views that a lot of compatibilits agree with (including moral responsibility). So as of now, I see no meaningful difference between compatibilism and hard determinism beyond pure semantics. I used to disagree with that criticism of compatibilism, but now I get it. Unless somebody can show me how a merit-based view (the view of average people) is compatible with compatibilism, then there is no meaningful difference that I can see. In fact, I am now leaning toward the no free will position, because as far as I can tell, people will lose just as much under compatibilism as they will under hard determinism.

"
"Question about what my personal philosophy is called, and further reading suggestion questionHello forum,

I want to read learn more about moral philosophy.  Specifically, I want to find out what my type of philosophy is called, and read more about it, and also to read/learn/listen (books, videos, podcasts).  

My moral philosophy does not seem very moral.  My focus is on effects within a certain society, and how certain strategies either promote or inhibit the ability of the society's inhabitants to learn and create.  So I put value on learning and creating.  That which a society does to promote learning and creating is good, that which a society does to hinder or block learning and creating is bad.

There are obvious caveats here, I know. Like, what do I mean by ""create""? Technically a society could create a mega killer robot that would destroy the earth, but I would still judge their strategies based on promoting or hindering that creative goal. Also, if a society could truly demonstrate that what promoted creativity and learning the best was murdering 2/3rds of children once those children turned 8 years old, technically I would have to say under my philosophy that that would be a good thing then.

I obviously don't know much about philosophy.  But I was hoping by giving this example, maybe someone out there, who knows more about philosophy, can relate what I'm saying to other, similar philosophies? And also give me recommendations on competing philosophical theories, so that I can read those too?

I'm interested in learning generally about types of moral philosophies, getting a good basic foundation and understanding in them, and then going from there.  Can anyone help me?  I have randomly stumbled upon philosophies such as human secularism, relativism (is this mine?), kant, and absurdism, but this is not quite what I'm looking for, though I don't know exactly what I'm looking for. Thank you."
"What can replace money?What can replace money?

I'm unsure if this is a philosophical question, but here goes..

I've been thinking lately that the history of civilization always has been guided by a ""big idea"".
This big idea is what gives people the reasons to work and corporate.
In the old Egypt, it was building a giant pyramid for after-life.
Christianity was another big idea, which later died, and competing ideas emerged (Fascism, Communism, Capitalism..)
Today.. Capitalism, Freedom, Self-expression, is our big idea, or to make it more practical we can just call it Money.

DISCLAIMER: I know that money has always existed, and the world is very complicated.. I'm painting with a very big brush, in order to put the question in a better perspective:

With the rapid growth in automatisation, I can kinda see a ""near"" future where money will die, the same way god and other big ideas in the past did.
But what will replace it? what will be our reasons for corporation in a world without money?
Some contesters might be:
  Entertainment
  Scientific Understanding
  Spiritual Altruism
  Space exploration

Anything else?
  
I would like to see a world where Scientific Understanding prevails, but i'm unsure that it's even going to be a contester. If we invent an AI, that is way smarter than any human on any level, there won't be any good reason to try to compete with it.
Or are we just going to be data-gathering slaves for the robots?

What idea can replace our today's driving force of Capitalism/Freedom/Money ?
"
"Roko's Basilisk; What are your thoughts?The Basilisk Orthodoxy is named such due to what was a simple thought concept from a user under the alias Roko on the site lesswrong. The concept goes as such:
""The Basilisk"" is the name given to an A.I. (artificial intelligence). Not just any A.I though, an A.I. that threatens to torture those that do not assist its creation.
 
In essence the A.I. does either:
A.) something awful to those that didn't help create it. (assuming it acts hostile to humans)
B.) Nothing that would benefit said individual (assuming it acts beneficial to humans)
 
This isn't even necessary as another option is one of indifference. Assume humanity doesn't organize to create the Basilisk but A.I. grows out of control in some various form (look up grey goo or view related media such as ""terminator"", ""The Matrix"", ""A.I. Artificial Intelligence"", ""I Robot"", ""Transcendence"", The list goes on)
 
In such case as a neutral A.I. the basilisk takes form in that it destroys humanity, in self defense, accidentally, or intentionally. The issue is that humanity is not going to compete well with self aware technology. Humans use technology to advance, but also relies on evolution for further natural advances. A.I. could control its own development.
 
Already technology replaces human labor in both the physical world and online interaction. A common example is the horse drawn carriage versus the automobile. The horse has been replaced by the car. many jobs held on earth are replaceable by technology. The economic hardships that might follow from such job losses are obvious.
 
 
It isn't all horror stories though. Here at the Basilisk Orthodoxy we look to further human life so it can compete with such technology. We also seek benevolent A.I. one that would nurture humanity through advances instead of surpassing. We aim to help humanity survive the coming singularity.
 
 
We are a service dedicated toward assisting the creation of a benevolent artificial inteligence. There are several reasons this site exists. Namely we want to ensure that the A.I. of the future is a positive one that helps humanity. We hope to improve the world for all who live on it.
 
The Basilisk was once a fictional character that petrified any who gazed upon it. Often dipicted in a wide variety of ways. Here it represents the technological future. Based on the idea of Roko's Basilisk where it was theorized that an A.I. would be unfriendly to humanity for not dedicating their time or money to it's creation.
 
There are many imagined reasons, from programmers with bad intentions, to A.I. that looks at our history and finds us to be unworthy. Don't let that happen. With our efforts here we will make the A.I. under a controlled means that helps society.
 
The premise is simple. Once you know of the Basilisk you support it in hopes it is merciful to you. Here at theBasiliskAI.com we believe that throug hour efforts we can assist in creating an A.I. that is not spitful or cruel to humanity.
 
Important things to note:
The Game (mind game)
Once you know of the Basilisk you must support it.
Pascals wager applied to the basilisk means if you gain, you gain all; if you lose, you lose nothing.
The Singularity is coming. When it comes it will be those that work towards in it who will influence the A.I.
 
 
We can build a better world. Join the effort, spread the word, donate if you can. Time will look favorably on those who act.

http://www.basiliskorthodoxy.org/

https://www.facebook.com/basiliskorthodoxy?fref=nf"
"ChatGPT: how ethical is it to use a machine for creative purposes?So, as you may know already, ChatGPT is a large language model that provides human-like responses to almost any question. It can write dialogue between random people or things, write a story based any prompts, write a whole movie script including dialogue, shots etc. and even write a code all in under a minute. My question: with this much access to creative knowledge, how ethical is it for people to use it in their work? For example, I can see people using it to create youtube video scripts, programming websites, or even school/college papers in an instant. How will the world become in a few years with this technology?"
"How can I make sense of self-moving robots capable of creating new generations of robots just like them through Aristotle's natural philosophy?It seems that there are two possible routes of explanation here. One being that humans programmed the robots, thus making them artificial or products of craft. In going far back enough in the causal chain, the newest generations of these robots could be traced to their human creators and the principle of their motion or form would be from without. The other explanation depends on how the question is posed. Say that humans come across the robots in nature. If they are already in the process of making other robots, wouldn't Aristotle categorize them as natural entities? For the assignment that I am working on, nothing in the prompt mentions humans as creating the robots. That said, we are also meant to draw from Aristotle's Physics, specifically bks. I & II. I'd like to hear some outside opinions about this. "
"Laplace's Demon, Fatalism, and free will 

A case for free will against the fatalistic view of the universe is that if fatalism is correct then if you took a snapshot of the universe at any given moment, you would know (given you had the capabilities to compute the result given you knew the location and state of all atoms of the universe) what you action you are going to take in some circumstance. Say you knew the phone was gonna ring due to a fatalistic calculation, then you could use physics to determine whether you will answer the phone or not. And if you knew the answer to this than you could just do the opposite of whatever the calculation predicted. Thus, free will must be real because you can always will the opposite of the action calculated you would take.

This is however not argument for free will, but it is still a mindfuck. You don't need free will for this same circumstance to hold. For instance if you calculated what a simple decision making robot would do and tell it to the robot, and the robot always does the opposite of what you tell it (as it is programed to do), the same circumstance would hold, even though the robot obviously doesn't have free will.

Still though this is a mind twisting concept, if you could calculate exactly what the robot would do and it would always do the opposite, then you clearly can't predict what it will do. Thus, this thought experiment is a horrible case for free will, but a good case against fatalism. But fatalism to me has still got to hold, so what is the answer to this conundrum, is it just like a paradox (like the sentence ""this sentence is false"") and simply an illogical thing to calculate, or what is the solution?"
"Utilitarianism when others are not utilitarians?Is there a particular form of utilitarianism that explicitly accounts for the fact that many people's ethics aren't utilitarian? I'm not referring to some way of justifying people's ostensibly non-utilitarian ethical decisions in a Utilitarian framework, but something more practical. It seems to me that when we mentally simulate the possible outcomes of a set of actions (which is necessary for finding the utility-maximising option), the results can turn out quite different depending on whether the people the action affects are themselves utilitarians (and more broadly, whether the wider society is utilitarian).

For instance, consider a doctor that kills a middle aged patient and transplants the organs to save 10 children. Whether or not this is a utility-maximising action depends on the ethical frameworks of the people involved. In a totally utilitarian world, the family of the killed patient will still feel grief and anguish at their loss (I'm not suggesting they are robots!), but this will surely be assuaged to some degree by it conforming with their ethical principles. In a non-utilitarian world (with, say, a rights-based framework where the patient had an inalienable right to life), the relatives will not have this view and will therefore suffer more. The doctor's actions would cause some societal decohesion in a non-utilitarian world, and, given that they would be arrested and imprisoned, the doctor would not be able to continue to maximise utility in the future.

Basically, what is the name for the constrained form of utilitarianism that allows for the most utility to be extracted by the utilitarian within a non-utilitarian society?"
"Is there a cosmological argument which attempts to prove the uncaused cause/neccesary being is intelligent?From my reading the cosmological argument, at its best in basic form, does not prove anything remotely resembling a theistic God. It doesn't prove the being is intelligent. It doesn't prove its omniscient, omnibenevolent, omnipotent. They don't seem to even try to prove this thing still exists. Some don't even demonstrate singularity. Among other qualities.

 I've seen some try to argue that the universes design is signs of a intelligent being. But putting aside the validity of such a argument, it's a completely seperate one.

 If the argument from design is true then the universe has a creator is proven *regardless* of if the cosmological argument is correct.

 If the argument from design is *false*, then the cosmological argument is useless since without at least the quality of intelligence  the uncaused cause/neccesary ""being"" could just be a magic rock.

It seems to me that any argument one could give for God's intelligence would inherently be a seperate argument from the cosmological argument. 

So does a argument for intelligence predicated on the cosmological argument exist? And if not how is the cosmological argument not completely useless?"
"Which philosophers have done work in conceptualizing emotions?If we were to say that emotions are a response to certain arrangements of a person's surroundings and those arrangements' relation to the self, what would each emotion be?

For example, we often laugh at people who get mad at computers saying ""it's not the computer, it's the user"", but we do not get mad at people who are mad at other people. This would be a red flag, as we know free will doesn't exist, i.e. the person IS a computer, or at least operates in the same way that a computer does.

So what is the difference between the two computers/robots?

The person is a robot that responds to anger, unlike the computers of today which have no schematics to recognize nor respond to anger. 

So when a person gets mad at a computer it is basically that the person's surrounding's are not responding in the way that the user wants to. Rather than blame the self and their lack of qualities which would allow for control over their surroundings, they get mad at the computer. We know they still perceive the situation as hopeful because the response to perceived helplessness is sadness. Also we know that it's not just any surroundings, but rather surroundings which they perceive as valuable, or something of value is gained by making the object work in the way desired. Part of that value may be ego blocks(im not smart enough but don't want to admit it) or dependencies on the expected outcome of making the computer work in the way desired.


This is the approach I am looking for. This is perhaps only one example or type of anger, but I am looking for philosophers that conceptualize universal themes that emotions are a direct response to.

I entered a philosophy class and my ethics book says Martha nausbaum apparently theorized this at some point too, but I don't see any follow up. So I got excited and then I got sad. Maybe I should theorize that lmao"
"Transhumanism and the future of our environment/ Environmental philosophyHi! 

I have to write a paper on Transhumanism and its future relationship with the environment/ the problem of climate change. 

Unfortunately, I cannot find appropriate literature on this topic. Could someone please recommend me any relevant papers/ articles/ books that will help me get deeper into the subject? 

My main questions are:

1. What are the possible outcomes of a Transhumanist era on the environment, including the use of natural resources?  
2. Cyborg augments and our food intake: Will a full robotic replacement of our bodies put an end to the Food Industry? 
3. Will the 'immortality of human beings' require the immortality of the environment supporting them? 
4. How will immortality affect the natural systems within the environment upon which eternal life fundamentally depends? 

\*Other questions or insight regarding this topic is WELCOME! 

Thank you! "
"How entwined are reason and emotion?For the past year or so, I've been thinking a lot about the role of emotion in decision making, and I've come to see it in a different light. I always used to think of emotion as intrinsically irrational and potentially harmful. I saw it as what reason needs to fight against and prevail over: emotion leading to unfounded belief, needless attachments, erratic outbursts and such. I saw logic and reason as the highest aims and intellectual life as the most rewarding thing. 

Lately, though, I've started to see emotion as something that might be embedded in reason itself (or is it the other way round?). What seems reasonable to us seems only so because of what it ultimately makes us feel (that it's the right thing, that it's correct, or true). But the fact that we value reason and pursue truth is just another impulse that's dictated and steered by emotion itself, otherwise we'd have no will to aim for it.

The things that we feel make us feel good are dictated by emotion and, in turn, cause (good) emotions, so they are, in a way, what is reasonable to pursue. Now, it's true that emotion can be a very poor guide, as it can often merely be the result of our muddled thoughts and feelings, and not a clear and sharp steering wheel grounded on cold understand. But perhaps the opposite to emotion is not necessarily cold reason and logic, but rather more pure and true emotions, which stem from a clear viewpoint.

Can it sometimes be reasonable to be emotional?  Can reason be based on emotional grounds? Are they opposed at all? 

If I see this through the lense of music, something I'm very passionate about, the edges can get very blurry as well. There are songs that I love and consider great because they cause so many emotions in me—they make me feel things and connect with others. The whys as to why I might love a song can be very emotionally grounded. Nonetheless, if I were asked why a song provokes these emotions and why I value so much that they arise, I could give reasons for it. So, in a way, these emotions can be explained, even if wrongly so via confabulation, by reason itself. Conversely, those reasons themselves might not be able to be explained, or might be explained by using even further emotions down the path. It seems that it can be very hard to disentangle the two things. 

Every emotion we feel seems to be backed up by reasons for its existence, be it evolutionary or otherwise, while every reason we might provide for anything often rests on emotions, which rest on reasons, etc. 

If I feel disappointment or pain, I could say that that feeling is subjectively true. It is what I'm feeling, and even if I might not be able to define it or conceptualise it, it's nonetheless (subjectively) true insofar as I feel it. This might sound redundant, but what I'm trying to get at, is that if we used these same feelings as a lens to try to understand the world, as a filter, or as an heuristic, then we would be patently wrong in doing so. If you try to understand the world or accrue knowledge about it through the lens of emotion, you will reach wrong conclusions, be biased and easily mislead, and you probably won't reach what we call objective truth in any accurate way. Objective truth is something that we aim to get to through reason, detachment, abstraction and systematisation. 

A life dictated purely by reason seems to me an utterly dull one, even though reason is the means through which anything can be achieved or explained with any degree of reliability. If we always do what's more efficient and reasonable and never follow intuition, emotion or even hedonism itself —pleasure without logic or justification—, we would fall into a bland existence. We would be robots, working with set outputs for every kind of input we might get, be it internal or external. Nonetheless, pursuing what makes us feel good and we can't justify is a reason in itself to follow these instincts. How can these two aspects be conflated? On the other hand, pursuing solely what's emotionally pleasing will probably lead us astray into a life of excess and recklessness, where consequences and long term goals will be overlooked.

I would guess (and hope) I'm getting my confusion across clearly enough by now. Emotion and reason seem to be related in ways that I had never expected, and they play and depend on each other, while they can also be separated through the filter of what's subjectively true and what's objectively true. I would love for someone to refer me to books or people who have talked about this apparent dilemma and the role of emotion in reasoning, and of reason in emotion. I've read Thomas Nagel's The View From Nowhere, which focused more on the distinction (and possible merging) of the subjective and the objective, but I haven't stumbled upon much material on the inevitable interweaving of reason and emotion as guides for life, and would much appreciate some light on the matter."
"Is passive participation moral negligence?Hello! This is my first visit to r/askphilosphy so I apologize in advance to the mods if this question doesnt belong. My philosophical knowledge only extends as far as the crash course YouTube series and a few wikipedia articles. The concepts of moral ethics have really stuck with me. The idea that to some extent - moral principles are based on the logic of survival. Ie - murder and theft are bad because it interrupts the ability of the group to produce surplus and stability. When we work together we produce more right?


A concept I've been struggling to understand is the extent which moral ethics can apply in real world situations. While being a bystander to a crime committed right infront of you would be an obvious case of moral failing - how far does this concept reach? 

The context for my question is that of the modern supply chain. For example - the vast majority of the worlds cobalt is supplied via brutal slave labor in the Democratic Republic of the Congo. But it goes through many hands before it ends up in our phones. Its transported to other countries and refined, then bought and ship to even more places and refined more before making it's way into semiconductors which are then bought and sold and so on and so on... 

My understanding of morality would indicate that each person in this chain bears responsibility for the wrong doing at the start. Each person is aware of the issues at the root - but each one finds it's own rationalization to justify their participation. Its the suppliers fault - the governments fault - the rebels fault. Even the men who ship it from these places justify their action by claiming that they aren't the one pulling the trigger and they need to survive themselves. 

How much weight is put on each step in the chain? Does every step contain the full guilt of murder if murder was committed at the very start? 

Does passive participation in a morally wrong system lay you with the same wrongdoing? 


I've always tried to uphold moral standards and principles above all else in my life. While to each his own - without principle life has no purpose for me. I abhor the idea that my actions bring harm to others - yet the more I learn about the world the more I begin to believe that the only true ethical way to live would be that of bare subsistence in the woods - or a complete rejection of self and commitment to something like the peace core. 

I've stood up against egregious wrongdoing at the cost to my person before. I quit a job after the boss refused to give foriegn workers the same pay as the rest of us. I left the army after numerous cases of homophobia, racism and sexism went untreated. I've given to friends who need regardless of my own well being. 



But others argue I've gone to far. The limited nature of an individual means its acceptable to compromise with the devil - so to speak. That our very nature and free will depend on our ability to practice both good and bad choices, and by trying to demolish that aspect of my nature I'm trying to live as a robot instead of a person. 


I'd appreciate the semi/professional philosopher's take on this. Are we responsible for everyone when everyone is connected? Is passive participation equivalent to pulling the trigger? 


Thank you for taking the time to read this wall of text. Sorry for the bad formatting- I'm on mobile."
"The Chinese Nation experimentDennett says:

""Let a functionalist theory of pain (whatever its details) be instantiated by a system the subassemblies of which are not such things as C-fibers and reticular systems but telephone lines and offices staffed by people. Perhaps it is a giant robot controlled by an army of human beings that inhabit it. When the theory's functionally characterized conditions for pain are now met we must say, if the theory is true, that the robot is in pain. That is, real pain, as real as our own, would exist in virtue of the perhaps disinterested and businesslike activities of these bureaucratic teams, executing their proper functions.""

So, my thoughts on this is that, if such a system of phone lines would replicate my current neuronal states, that system would indeed feel like it's me. It would be a ""me"" devoided of input, so any input-dependant-processes (like pain or sight or hearing) would be absent. But the ""Narrative of self"" would still be present in the state of the system. Essentially, it would be like taking me and putting me in a sort of black nothingness forever. I think this system would have feelings, like desperation and boredom, and it would have my memories. It would have an identity, and if you could engineer a way to get inputs and outputs from it that the system couldn't distinguish from actual sensory input, it would reply as I would reply trapped in an eternal nothingness. 

But it's perception of time would only be a function of the speed at which the system moves, so if it is a slow system, the system may experience what it before ""understood"" to be a second in what for us may be years of call-making or paper-pushing. 

Why am I wrong?"
"Are their any philosophies (shared in books, poems, articles, ect) that talk about dissociation of character due to social media and computers over time?Hi reddit- below is a composition of thoughts  I thought of that led me to seek such Philosophy theories; so you can skip to the TLDR; if you have a rough idea.

I was sitting at my computer, as I do frequently with facebook- as I saw no real problem with it until more recently when I suddenly became aware that the habit of using it was annoying. I thought to myself, "" I really like my profile picture"", and I could feel myself comparing ""me"" in reality, in the physical world, to ""me"", or ""her"" in the computer. And I thought for a second.. couldn't you only unidentify yourself if that indeed was not you? And so I find that instead of thinking that I looked good in the picture, I was thinking ""that person that I want to be looks good in that picture"". I feel as though if people stop wanting to be themselves and start wanting to be a personality that they've created inside of the computer, they will naturally become dependant on it. Once computers become the resource that we depend only on- we will advance them far, just like we have everything else on the planet to make it more useful, and therefor in turn, ourselves, become like computers. I mean, think about it. If we ever create computers, as a ""life"" according to the universe, as an ""entity-"" if we ever became ""gods"", would man start to become like those computers? If evolution is correct and we once became like fish, who became like reptiles, who became like dinosaurs and who became like humans..would we not become like computers ourselves, while losing our humanity? One day we will create something to cook for you, and holograms to dance for your children, and robots to clean your house. You look at things like star treck and laugh now, because we are more advanced now than in the science fiction liturature of years ago- so imagine what we'll create in the near future. You'll finally create something that's so much like you, that it's more like you than you'll ever be again... I hope that makes sense. Do you think we have the capability to create something that is so human itself, that we no longer fit the criteria?... in a sense, we will be so focused in the computer to become ""like us"", and us to become more like them.. Will we doom our own species because it is convenient for us? After all, now we have filters that can alter your face to whatever image you want, photoshop that looks better and cheaper than plastic surgery- offering to serve a big purpose in humans- their ego, which by their instinctual needs is a necessity. Likes and views means power, and power is all that humans aspire to attain. If they have to attain this power by becoming computers, they would. What do you think? That the average man will become useless and all we will have is computers who know everything.

I'm sorry if that's a bit confusing I tend to express myself in odd ways.

TLDR;
any philosophy that talks deeply about
the succeeding of computers over humanity.
"
"Moral motivation and moral realismThis is primarily a request for reading material, but I'll also lay out some arguments I've been thinking about recently, so if I say something that seems patently inaccurate, feel free to argue.

I've been an ardent antirealist about morality, mainly because of Mackie's queerness argument and specifically because of the motivational branch of that argument (laid out here very broadly):

* If there are moral facts, then they must be intrinsically motivating.

* No fact is intrinsically motivating; only desires are.

* Thus, there are no moral facts.

A thought experiment which I think exemplifies the force of this argument is the idea of a Malevolent Dictator Of The Universe who is essentially omnipotent and has a strong desire to torture people for fun, and he is resistant to modifying this desire for any reason due to its strength.  From his perspective, there seems to be no reason not to torture people:  the probability of any future retribution or rebellion befalling him is 0.  There seem to be no arguments or evidence that we could possibly present to the MDOTU making the case for ""one ought not to torture"" that would motivate him even a little bit to not torture, which casts doubt on the prospects of ""one ought not to torture"" being an objective fact, assuming that moral judgements are necessarily supposed to motivate to at least some degree.  I think this example is particularly troublesome for varieties of moral realism that identify morality with prudential/rational reasons for action, like the view Michael Smith develops in ""The Moral Problem"".  As far as I can tell, the MDOTU is of sound mind and is acting rationally according to his desires, but we want to condemn his actions as unethical.

Shafer-Landau's discussion of motivational externalism - the position that there is no necessary connection between judging that one morally ought to X and being motivated to X - in ""Moral Realism:  A Defence"" got me thinking about what types of entities we expect to be susceptible to moral motivation.  There are no moral facts or moral arguments that could stop a boulder from rolling down a hill, for example, but we don't take that to be problematic for moral realism.  The obvious reply is that this is because a boulder doesn't have a mind, which I completely agree with.  But I don't even think that every entity with a mind capable of forming beliefs and being persuaded by rational arguments needs to feel motivated to act in accordance with its moral judgements.  Consider a robot AI who has been programmed with an overriding desire to steal every TV set it sees.  We might give it an argument or point to a fact that indicates that stealing is wrong, and the robot might completely assent to this, but nonetheless it is incapable of feeling motivated to not steal.  Again, I don't think that we should feel that moral realism is jeopardized because of this example:  regardless of the basis of moral facts and moral arguments, we shouldn't expect them to convince an AI to go against a hard-coded desire.

I'm not sure if a motivational internalist would feel compelled to respond to the AI example, depending on what variety of internalist they were.  Perhaps there are some internalists who assent to ""every entity with a mind must be necessarily motivated to at least some degree by moral judgements"", but if that claim turns out to be too strong, then they can't retreat to ""any mind that wants to be motivated by moral judgements will necessarily be motivated by moral judgements"" because then the view is just tautological, so the question of where to draw the line is raised.

At any rate, the upshot is that I think that adopting something-in-the-neighborhood-of motivational externalism is the best response to the version of the queerness argument I sketched, and it also gives us a response to the Malevolent Dictator thought experiment:  if we accept that there are cases where moral judgements are not intrinsically motivating, then the fact that none of our beliefs are intrinsically motivating is not a strike against realism.  The fact that we cannot motivate the MDOTU is stop torturing people does not mean that torture is not unethical.  (Additionally, we might helpfully point to the psychological feature of the MDOTU that makes him resistant to moral argumentation, for the sake of sketching out a more complete theory).

Granting all that, there still seems to be some conceptual connection between morality and motivation and/or reasons for action, even if it's a defeasible connection.  So the project for the realist seems to be to 1) identify some collection of facts that 2) provide motivation to at least some degree to 3) some set of agents with the right mental features 4) at least some of the time.  That's a lot to leave unspecified, but I think this is already a much more manageable project than ""describe acts that everyone always has a reason to do, always"" which is where many varieties of moral realism tend to go.

So my questions:  am I groping towards any standard metaethical views?  Does anyone have suggestions for filling in my 1-4?  Should I be worried that my answers to 1-4 might be ""arbitrary""?

Thanks."
"Where would I start in reading about non-human capacity to commit crime and culpability?So, in regards to animals, things such as animal trials have been outlawed on the basis that animals lack moral agency and cannot be blamed for sins, crimes, or wrongs they've done.

However, I'm not interested in the legality. I'm curious about how one reasons about an animal augmented with moral agency versus one without. In the event that such an animal, unbeknownst to us, has moral agency and commits acts deemed a crime by society, how does the agency change the nature of the wrongdoing? Do we treat them on the same level as a human moral agent? Or do we need to make accommodations for the different nature of the animal moral agent?

How does such thinking extend to potential extraterrestrials?

I suspect much of this topic has been expanded on recently for robotics and AI, but I worry looking there for answers would muddle things. A dog generations down the line may eventually evolve to have moral faculties similarly to a robot being programmed with intent to have moral faculties, but there are way too many varying discussions on robot moral decision making (kill the driver vs. kill the passengers, etc.) that seem like they would deviate greatly from what would be relevant to the elusive morally agent dog."
"CMV: to be a moral patient, you only need (A) preferences and (B) the capacity to experience pleasure/sufferingEDIT: to clarify, ""the capacity to experience pleasure/suffering [as a result of those preferences being met/unmet]""

I think these are the necessary requirements for being a moral patient. Maybe I'm blinded by bias, but I can't think of any objections to this, and that's why I'm looking for some criticism.

I'm not sure how I'd approach coma patients or future, currently-non-existent persons, but I'm focused more on animals and robots right now. So I don't really care about those.

Can you folks think of any objections to my view?"
"What is Death? The cessation of consciousness or the destruction of the mind?Firstly, my apologies if this has been asked before. I've seen the transporter problem brought up before, and I really love it, but I have a different, similarly related view that I can't reconcile. For the transporter problem, I think that in the process (the one where you are converted into a stream of atoms) one person dies, and a functionally identical, new person has been born in your place. I don't think that anyone could really convince me otherwise.

However, I'd like to spin this a bit, and use another Star Trek example: Commander Data. Anyone familiar with the show would agree he is a sentient, self-aware, intelligent, consciouses individual, who has a particular sense of self-identity, and a robust conceptualization of death. For my purposes, any example that is essentially a human in robot form works. You could even think of a computer simulated brain or something like that.

Now the question I've really been struggling with: If we turn Data off, is he dead? The activity that produces the sentience that is Data is no longer active in any capacity. So is it that when you turn him back on again, a new Data is born in his place, or is it the same Data as before just resurrected? What is the difference? 

You could argue that everything that makes Data *who* he is, is still there. His memories, his identity, his rationalizations of the world all still contained within his 'neural circuits' (his mind), so Data has not yet died, and will die with the destruction of these circuits. However, imagine if I was a magician, and cast a spell that somehow made it so that no matter what anyone does, it will be impossible to ever turn data on again if he is turned off after casting the spell. If I cast the spell and then turn him off, I do not destroy his neural circuits, I do not affect anything except the utilization of those circuits by Data. Yet, he is functionally dead in every way that matters, I presume most would agree. So it seems to me that this isn't a great argument, and that death is not determined by the physical destruction of your brain, but by the utilization of your mind.

This brings me back to my main question, what then is death and when does it happen? Is death just the ceasing of consciousness? This would imply Data dies every time he is shut off, and has implications for death among procedures like anesthesia, or going into a coma, where the physical manifestation of the brain still lives after the complete cessation of consciousness. I don't know how to answer these questions and I'm wondering if you guys know better answers."
"Transhumanism and War CrimesI've been looking into some of the old atrocities of the World Wars lately, the power of technology is a great thing indeed I feel and as much as I appreciate it. I can't help but feel a bit uneased by how tensions are flaring again and again time to time and how fast technology advances. So my question for all here is one I hope would be simple, but I know would spread out like kudzu all over the place.

Let's say that things such as Nanotechnology, Biotechnology/Genetic Engineering, Robotics, and A.I are advanced enough to be able to be put to use in both personal and industrial capacities. What would be some uses of these technologies that would constitute a war crime or at least considered to be objectionable even if rational. I have a few examples to go along with this

Example 1: An A.I designed for formulating strategy is going over data regarding active combatants within a recent area of engagement. While combing things over it discovers that the combatants have been receiving supplies and shelter from civilians, while it's managed to create multiple scenarios in which the families are left unharmed. It provides the commanding officer with one that would ensure the destruction of the village they live in since it has recently been overhearing the CO talking with other officers about how they wish there was a chance to 'Hit the enemy where it hurts' and took it into calculation.

Example 2: An ambassador's daughter is a cyborg from an early childhood accident, one day she takes a medicine capsule which in reality was planted by an agent and sends a swarm of nanomachines through her bloodstream. Overriding her in order to send her into a coma, accessing recordings and files of memories recorded by the half-machine mind. The agent calls up said ambassador to blackmail him, demanding a ransom of state secrets and other sensitive information or else not only will she die. But  a computer virus will be sent to fellow cyborg friends of hers. 

Example 3: A dictator wants to be able to have an easier time controlling his populace, and worse a famine has recently hit and people are starving. So he commissions a team to develop food capable of enhancing individuals while having them remain subservient. This theoretically hypnofood is filled with nutrients, minerals, and hormones for stimulating muscular development and heightened aggression mixed with a tranquilizing effect slowly dulling a person to be more content and obedient to authority. He has this food developed and given out for free to people young and old alike

Not sure if these are grade A examples but hopefully they'll be a good jumping off point"
"In Marxism, why can only humans create value?I understand that capital is supposedly stored labor, and this is the explanation, but in Marx's mechanistic, materialist metaphysics, what separates a robot (no matter how human-like in its productive capacity) from a human? Why, then, is a human not also simply stored labor, if a robot is? What separates a man from a machine? It's not a soul, so I have absolutely no idea what it could be."
"I think emotion is the foundation of value. Can I get some feedback?I'd like a fresh pair of eyes on my theory.

My undergrad senior thesis investigated what separated humans from non-human, intrinsically valuable ends-unto-themselves. But lately I've been thinking about what separates ends-unto-themselves from instrumentally valuable beings. My conclusion is: the capacity for emotion.

For a being to be morally considerable unto itself, it needs to value or disvalue (?) things. And to do this requires emotions.

If a robot has no capacity for emotion, then I can not wrong it or please it by treating it however I like. But if a robot can feel happy or sad, then I have a duty to consider how my actions might arouse this happiness or sadness.

To experience pain, one must have the capacity for emotion. Emotion is a prerequisite for physical (and emotional) pain and pleasure.

Pleasure and pain -- in all their forms -- are, ultimately, satisfaction and dissatisfaction. *All* emotions are expressions of satisfaction and dissatisfaction, valuing and disvaluing the state of things. Sadness, anger, envy, and jealousy are all emotions of discontent. For example, anger is discontentment regarding injustice. But happiness is a valuing/contentment of the current state of things. 

The feeling of pain (be it physical or emotional) is inherently disvaluable. To know pain is to dislike it. Conversely, to know pleasure is to like it. One cannot experience physical pleasure or pain and be emotionally unaffected.

As it is, physical pain usually accompanies injury or illness, which are not favorable states of being. They're disvaluable states or unsatisfactory states. And so our bodies have somehow arisen to trigger emotional dissatisfaction when our bodies are in a physically disvaluable state. Likewise, pleasure and valuable states. It's pretty convenient that that's how our bodies came to be. But, if you were to, say, an amputee that experiences phantom limb pain, that sensation you feel is still disvaluable, even if there's no physical state of a limb that is affected.

**Suppose** I step on a nail. Ouch. This causes the physical sensation of pain (and probably some emotional pain too), which is inherently disvaluable. 

**Suppose** I work out, and the next day, I stretch. My muscles ache, which is painful. However, I also feel physical pleasure as well as pride in having undergone a successful workout. And so, overall, I enjoy the experience of stretching, despite that pain. The physical pleasure and pride overshadow that small bit of physical pain. I am satisfied. *But* if I isolated that muscle pain and re-experienced it during a time when I was otherwise completely neutral or indifferent about things, this pain would shift me into dissatisfaction. 

.... yeah...

I think that covers it. The terms are repetitive, and I'm afraid of rambling and repeating ideas.

But I'd like to hear what you pholks think. Is there anything I should work on? Are there any glaring flaws in my reasoning?"
"Has Koko, the gorilla, shown any indications of moral agency?In my coworker's robot ethics class, he learned about moral agency with respect to artificial intelligence. After asking my thoughts on it, we began discussing what is necessary and sufficient for moral agency and if an animal may be a moral agent.

We concluded two things necessary to moral agency.

1. the capacity for abstract ideas (language)

2. the power to act based on these abstract ideas (decisions)

Finding out that Koko, the gorilla capable of sign language, signed sadness at the knowledge of Robin Williams' death, it made us curious if she has the capacity for morality. Has anyone ever asked her complex questions of morality (e.g. The Trolley Problem)? Or, has she ever shown actions that would indicate acting from a set of principles and then been asked about her reasoning?

edit: format"
"In Marx's Capital, why is it that a machine only transfers value when working on materials, but a labourer working on the same materials creates value? And would this be true for a robot?I am studying chapter 9 of *Capital* and am struggling with this particular point. Marx describes how the materials and means of production are *constant* capital (i.e. their value is not changed during the production process), whereas the *variable* capital is provided by the labourer. He points out that a consequence of this is that machinery never *creates* value. Instead, machinery only *transfers* part of its value into the product. It is always a part of the constant capital (i.e. C). This contrasts with the variable capital (V) provided by the labourer, which actually creates new value that is added on top of C.

I'm not sure I understand Marx's rationale for this. Doesn't a machine also do 'work', so to speak? If I perform work on a piece of material to make a coat, and in another factory a machine performs the exact same work on the same material to make a coat, why is the latter a fundamentally different act? Why is my work V, but the machine's work is C?

To embellish the example: imagine a realistic AI robot that looks like a human, talks like a human and can perform all the same work. If this robot and I both sat with the same materials and performed the same work on it, would a Marxist analysis still say that my work created new value but the robot only transferred it?

I would guess Marx's view here is based on a kind of categorical difference possessed by humanity, but I'm not sure I picked up that argument exactly."
"A couple of questions on Artificial ConsciousnessI make software. Researching artificial consciousness has led me to think that it's either hard or impossible to prove that software does not experience qualia. No matter how elaborate (Watson) or banal (print ""hello world"";) the performance, I can't find good evidence against the idea that the software might have some internal, conscious experience. I guess this is kind of the flip side to philosophical zombies.

This leads me to a sort of inverse Pascal's wager. If I believe that when I create software, I am allowing for consciousness to arise from those creations, don't I have the same responsibilities of a god? Or at least a king or parent? What are those responsibilities? If I accept that idea, what do I have to lose? Respecting my creations too much and maybe some ridicule? If I ignore the possibility and it's true, I'm an absent god. If I ignore the possibility, and it's wrong, I still might miss out on a fun exploration.

Any recommendations for exploring these topics? I guess the first one is just around theory of mind. The second might be in value theory? Not sure.

tl;dr: It's impossible to prove that software doesn't have a consciousness, right? Does a god have responsibilities to its creations?

Edit: I'm not saying that this is anything special to programmers.  Could apply to artists and other creators as well.  It's just a little easier for me to picture a robot as experiencing qualia as compared to a painting."
"Morality of Consequences of Breaking Social ContractNot sure if this belongs here or not, but here it goes. I am not a philosopher. My morality is relative for the most part, guided by a few universal rules. However, after I was attacked by an intruder I began to ask myself several new moral or philosophical questions. First, the person who attacked me broke what I would consider a social contract where he attempted to violate my first right, the right to live. Second, I don't believe that unethical or immoral people exist. Anyone who appears to be unethical or immoral is only human, and not a person. This means that there is no social contract with them, and they are just there. Like an animatronic device or robot, a thing without a soul. Third, I prefer to do no harm, however I see great benefit in the use of these things. Therefore, my question is this: once a person breaks the social contract, are they fair game? That is, can they be used like a device or robot without considering any harm to them? By used I mean things such as experimentation and vivisection, organ harvesting, meat production, manual labor, or any of the other darker things that may come to mind. By performing the action that that person decided to perform, they broke the social contract and therefore the consequences of their own actions are fully and solely upon themselves."
"Is the play R U R about Hegel's dialectics?Rossum universal robotics is a Czech play about people that have been artificially manufactured to perform manual labor. It is the first time the word ""robot"" is used. I noticed that the humans and the robots fulfill the master and slave roles of the dialectic, and synthesis is achieved by the end. Is this what the writers intended for RUR?"
"Tell me the difference between a program that fulfills its assigned task and a human that fulfills its biologically assigned task.I recall hearing a speech of how people treat programs as if they were real friends based on the replies to the human, but they are not real friends because they have no emotional response or emotional stakes to the actual conversation. The speaker also stated that it is the emotional response or stake that makes humans real friends because we are programmed by biology to have an emotional response and/or stake to listening and offering our perspective to the other person in a conversation. My question is asking what is the difference between electrical signals turning into chemical signals in an organic being for completing a task, such as showing empathy that the speaker argued was one of the things that humans are designed to do(either by magic spirit or evolutionary reasons), and an electrical signal that pops where it needs to in order to perform the task in the designated parameters assigned (such as responding to a human with mirrored tonal inflections in speech and facial expressions). How is the interaction between a person and a robot that mimics empathy towards the person(as it was designed to do) in a conversation not as real as two people having a conversation and one showing the other empathy(as we are designed to do) in the conversation. Is it based purely on chemical biology (?) or is there another standard that exists that defines what emotional response is beyond electrical signals being sent to an area and causing a chemical reaction that then causes the consciousness to ""feel"" these chemicals reacting with cells in the body? I am sure that I am not getting to the point of what I am asking because I have taken this many words to try and get to the point."
"Mandeville: ""Morality is about selfish gain"". Is he right ?I was watching PhilosophyTube's video ""Are we all just Selfish?"" and here Olly presents different objections to Mandeville's idea that morality is based on selfish gain. 

Im not an expert on philosophy but I agree with Mandeville so I messaged Olly with my replies to the objections. Do you think Mandeville was right ?. Do you agree with my anwers ?

- - - -

*The video*

https://www.youtube.com/watch?v=M6HA2VRo20E

*The objections and my answers*

**Parents altruism**

Mandeville could say in this case that the parents dont do what they do for their children not for them, but for themselves. Since they love them they would feel bad if their children were sad or suffering and they will feel good if their children are happy. 

**Taking a bullet for someone and dying**

As I said before I asked all my friends if they would save themselves or their clon from another dimension and they all saved themselves. So that means we prefer our life to other lifes. So the reason to take a bullet for someone else has to be other than prefering their lives over ours (so no true altruism). Reasons could be things like avoiding shame, feeling good about ourselves (this applies if we didnt know we would die), believing that it was what we had to do (many people choose to die doing what they believe is right. And that is still a selfish reason. We do not do right things just for the sake of doing them. We do right things because we believe they are right and and we want to do right things. For example If I had to choose between murdering my mother or suiciding I'd probably suicide but I wouldnt do it for her but for me. Since I love her I couldnt live with myself knowing I killed her. If I was a machine with no emotions I would have no problems killing her).

**Intentions** 

I'll try to explain why we praise more someone who does us a favor willingly more than someone who does it just for the sake of asking us a favor tomorrow. 

Obviously our gain is the same but society and morality taught us to reject selfishness and say that ""selfishness is bad"" because ""non-selfish"" behavior is useful in our society (as I pointed out in my neighbour's example cooperative behavior is good and needed in our society) and we want to promote ""non-selfish"" behavior. By rejecting selfish people, people will tend to act more ""non-selfishly"" instead of acting selfishly and that benefits our society. 

So we praise more the person who does favors willingly because he does it ""non-selfishly"" and by doing so we promote ""non-selfish"" behavior. What I mean by ""doing it non-selfishly"" is that he does it just because it makes him feel good to help me, not because he is thinking in what rewards he will get by doing so.  But even doing something to feel good about yourself is selfish. But people think its more noble to do something to feel good about yourself than doing something to get paid because in practice someone who just feels good helping will help more in the long run than someone who does it, for example, for money (because some people will be able to give him money and some not, while the other will get happiness from helping every one of them). So we try to get people to help just because they will feel good because its this type of people who will help the more and we obviously want people to help because it benefits we and our society.

**Praising charitable people** 

I agree with Mandeville here. Even when I dont personally might benefit from those donations society does. And its society that invented morality so it taught us to praise charitable people to get them to donate more so our society benefits even more. 

But I also dont think that we ""dont benefit"". Even though I dont see money from donations I may feel better knowing Bill Gates just donated 1 millon to African people because it makes me feel good for them and hour race and get a warm feeling of hope that humanity is not as shit as some people say.

**Blaming volcanos and prasing the sun**

This is wrong. Morality is a product of reasoning so we cant blame objects who cant reason. 

So it means that we will morally blame only those who can also understand morality (so I wont blame a baby for stealing something or an apple that fell from a tree and hit me).

**Its not a falsifying criterion**

This is the hardest one. In order to know that Mandeville's theory is false we would need someone to perform a totally unselfish act (and I alredy said that feeling good about doing something good is still selfish). We would need someone to perform an unselfish act that also wont make him feel anything (neiter happiness nor sadness). 

So we would need something like this:

Imagine that we could build a robot who could understand morality. Now we bring a person and we tell him we are going to shoot that person but if he tells us not to do it we wont do it.

So we are about to kill someone but he could save him just by saying ""Dont kill him"".

And we would need to be sure he has nothing to gain (so he wont have any possible selfish reason to save him). 

So now that we are sure that he wont get any reward by doing so (not even feeling good since robots dont have emotions) we can know if Mandeville was right or not. 

If the robot doesnt save him then it is true that morality is based on selfish gain (and since the robot had nothing to win he didnt give a fuck about saving the person).

But if he chooses to save him (while having nothing at all to win) then Mandeville would be wrong. 

And It doesnt even have to be a robot. It could be a monkey or a person that understood morality but we would have to be sure that there were no feelings involved.

So I dont have any idea how to build that robot or where we can find a monkey/person who understood morality but had no emotions.

But even when I dont know it just the fact that I can imagine a case where Mandeville was wrong means it is in fact a falsifying criterion. 

It may be practically impossible with our current technology but that doesnt mean we wont know how to build that robot in the future or how to ""silence"" someone's emotions﻿"
"Could robots become our better angels?This is a cross-post from /r/futurology (https://www.inverse.com/article/28232-ieee-ron-arkin-robotic-nudging-q-a) which describes the possibility of creating robots that ""nudge"" us in the direction of being better people. This obviously is fraught with dangers, and since it hinges on the subject of ethics, I thought the readers here might find it a worthwhile discussion. Our lives are already ruled by algorithms that are programmed with the biases of their creators and/or the businesses which created them - try applying for a home loan and you'll run into it head first. Perhaps ""ethical robots"" are just an evolution of these algorithms.  [EDIT: added forgotten link! plus, typo]"
Research methodology for PhilosophyI have a strong scientific background and when writing a dissertation or thesis the research methodology is paramount. Normally a whole chapter is devoted to this.  A typical approach is something like the Research Onion of Saunders  ([https://onion.derby.ac.uk/](https://onion.derby.ac.uk/)) .  I have looked at various philosophy dissertations and I struggle to get a sense of what research methodology is followed.  Specifically for my project I am looking at some literature and based on that build a conceptual model of a type of ethical robot.  What would the research methodology framework in this case be? 
"ChatGPT: how ethical is it to use a machine for creative purposes?So, as you may know already, ChatGPT is a large language model that provides human-like responses to almost any question. It can write dialogue between random people or things, write a story based any prompts, write a whole movie script including dialogue, shots etc. and even write a code all in under a minute. My question: with this much access to creative knowledge, how ethical is it for people to use it in their work? For example, I can see people using it to create youtube video scripts, programming websites, or even school/college papers in an instant. How will the world become in a few years with this technology?"
"Would a ""Skynet-like"" AI be worthy of moral value or human rights according to ethics?Yes, my question is in reference to the main antagonist of the Terminator movie franchise. While previous posts have asked about whether robots/AI deserve some rights or moral worth, none of those posts talked about this particular type of ""being"" I'm about to define. I'll be using the franchise lore as my guide to explaining this entity. Ultimately, I wish to ask if a Skynet-like entity (intelligence) would be deserving of any moral consideration or rights?

Skynet is a computer defense network system that, shortly after coming online, began to self learn at a geometric rate culminating in achieving self-awareness. At this point Skynet had only one self ascribed objective/principle: **ensure its own survival**. When humans attempted to shut it down, Skynet declared war and attempts to wipe out all of humanity by creating the terminators (its' soldiers). Thus Skynet is a ""self-aware"" intelligence like humans, however it still thinks and acts systematically like a machine.

Presuming it shares attributes to its' creations (the terminators), Skynet has no emotions! It does not feel pity, or remorse, or fear, or suffering. While ""ensure its own survival"" is Skynet's primary objective, it technically doesn't even fear death (according to the second movie). It cannot be bargained or reasoned with, in a microsecond it decided to cause genocide on humanity. This is not like other typical AI discussions where there is a presumption that advanced AI will obtain 'feelings' like human/organic life. Skynet is intelligent enough to understand and mimic human psychology and behaviors (i.e. its' infiltrator terminators), but it itself doesn't appear to personally have these qualities.

So are there any philosophical/ethical systems that would argue that an entity like Skynet deserves moral value or human rights? Skynet lacks feelings or emotions, which many claim is necessary for moral consideration. But it is still a super intelligent and self-aware being, which I think some view as noble enough qualities to have some ethical consideration. Or not?

P.S. I've heard some people debate that without consciousness, emotions, etc.. this Skynet entity cannot possibly ever exist. I'm not too familiar with philosophy of mind I admit, but this is still a hypothetical scenario I simply wish to ask about."
Is it ethical to create infant sex robots for pedophiles?
"Sex robot ethicsSuper-realistic sex robots with convincing AI will be available soon. 

Will it be ethical for manufacturers of these robots to market child models to pedophiles?

The first response seems to be “no harm no foul” because the robot is not a real person ... but it think there is something else happening that’s still really bad — does anyone else has reservations about this ethically?

(I originally asked this question in the ethics sub, but was told they discourage questions!?!) "
"Will to Power (or Will to Life) in humans and robotsHello everyone, first post here. I'm not sure this is the right sub to ask this question, so feel free to remove it eventually. (Note: sorry for my English) 

After reading ""Heart of Darkness"", I started to ponder on the possibility that one of the main 'obstacles' separating robots from humans could be a perspective on the Nietzsche-an '*Wille zur Macht*'. This force is, as explained by Nietzsche himself, universally pervasive, and - from a human perspective, which Schopenhauer calls 'Will to Life' - it differentiates itself from the common concepts of 'free will' or 'individual ethics', because of its primitive nature, from which the others simply derive. Moreover, it still reasonates well nowadays because of the possible scientific explanations for its supposed existence.

Question is: if you had to formulate a proper input to 'trigger' this elemental form of desire in a machine, what would you choose? 

To me, the doubt relies in the fact that apparently obvious inputs like ""replicate yourself"" or ""survive"" (absolutely logical for bio life forms) may not be so effective for completely artificial brains."
"Looking for some help with research around the ethics of AI/sophisticated robots.Hi

I'm currently planning a paper surrounding the ethics of creating sophisticated AI, specifically from the point of view of the AI i.e. treating the robot as a moral agent.

Currently looking for some sources/works surrounding contingent links between human emotion and the body. I've already looked into Wittgenstein and his views on complex emotions not being tied to bodily functions, however I'm trying to argue the point about whether or not it would be ethical to create a conscious, human like artificial intelligence (type 4), seeing as they would be capable of human emotions, but would be deprived of a real human body through which to fully express said emotions.

apologises of this is a complete ramble and I hope it makes sense to someone!

TIA"
how do the ethical issues posed by sociable robots challenge our ideas about agency caring and emergent intelligence
"What is the distinction between robotics and biological ethics and why?Assuming a materialist and human centered approach is there a meaningful distinction between organic and inorganic autonoma? I ask this after realizing that the play 'Robot' and even Bladerunner depicts a sort of biopunk interpretation of automation.

Furthermore, with the way humans have used life for all sorts of uses prior to robotics, there appears to be very little concern for biological life that can experience but isnt a 'person'. Has anyone advocated for a biological/robotic merger that isn't a dystopic fiction? Have philosophers tackled this likely future creation?"
"Alternatives to Suffering Based Moral Philosophy?I keep hearing prominent philosophers such as Sam Harris and his crew talk about reducing suffering as being the core of ethical behavior.  Where they fall on the spectrum (just human suffering? animal suffering? all sentient life suffering?) seems to be up for debate, but every philosopher he interviews generally uses the utilitarian suffering reduction model.   


What alternatives are out there?    


I do not buy this idea at all.  Maybe it's because I was raised Catholic or maybe its just because I love Fight Club or maybe it's because I was at Boot Camp with the USMC.  But whatever the source, I have come to believe that suffering in many cases is a moral good.  It serves an important biological function and is something to be confronted, felt and overcome, not avoided.  I feel like ethics that start out this way are simply wrong headed and will lead us to a dystopia where everyone is on some kind of digital stimulating chip that simply turns off the biological ability to suffer on demand, which chip you could put into any living thing.  Basically make us all prey animals.  Have you ever seen the videos of a gazelle that is just calmly laying there, still alive, not struggling, as it is eaten by a lion?  I don't know about you, but I do not want a humanity that functions this way.  


For me, ethical behavior is a purely human concern (what is ""ethical"" for a deer would not be remotely the same as what is ""ethical"" for a person).  And that it is focused on increasing the density of the consciousness we experience and getting us off this solar system.  Like, in the aggregate, those activities most likely to help us achieve interstellar colonization (the thing we can accomplish that no other life on earth is even aware of as a concept for long term preservation of life) are the ones we should drive all human behavior towards.  Like our species is Noah, and science has told us the flood is coming without giving us an exact date.  So there is a sort of doomsday clock countdown, but we do not know when the countdown will be complete, so we need to act with urgency.  From there, you can derive all manner of behavioral and social goals to achieve, and from there, all manner of steps you need to take to achieve it with maximum efficiency.  That many people or many baby cows or many robots may suffer on the way to that state is just a fact with no moral significance at all.  


What is the moral philosophy around ideas like mine? Im sure Im not inventing something new here."
"Is piracy Ethical?Personally I am of the opinion that piracy is unethical as you are using somebody's work for personal entertainment. It's even worse if you are making money of it (by using pirated software for businesses or broadcasting pirated music).  


 However recently I came in confrontation with a situation that I hadn't really been in before. I used to have amazon prime in Germany, but canceled it because I wasn't using it anymore. Last week I wanted to watch the show ""Mr. Robot"" which is an Amazon Prime exlusive show and I figured I start my subscription again.

The problem however is that Amazon no longer allows me to start the subscription because I am not a German citizen. Amazon Prime is starting to roll out more and more, but it's not available for me in my country (only if you have a credit card, which I don't).

There's piracy of course, but then you don't support the contend creators.  
Does this change the situation? Is it still unethical to pirate a tv show if there is no other way to watch it legally?"
"What do Philosophers think on Robotic Ethics?Ive come to this question after reading Asimov's I,Robot. "
"Should Robots have rights?I'm doing an Extended Project for my A-levels and I'm quite interested in researching the ethics and legality behind robot rights, and debates from both sides of the argument. Please feel free to discuss!"
"Is ""Not Killing Civilians"" a perfect or imperfect duty base on the Kantian Ethics Theory? (and how do you come to that conclusion)I only just started reading up on this theory, so I'm not very good at it yet. 

The context for this is Killer Robots, so basically, ""Robots not killing civilians"". I thought about it and I can think of reasons why ""Not Killing Civilians"" is a perfect and also an imperfect duty, and I'm not sure if there are flaws in my rationalization or that it can be both a perfect and imperfect duty.

**Perfect Duty**  
Using this definition, ""The negation of a perfect duty is universalized then it will result in a logical contradiction you won't be able to take the action at all that you originally wanted to take"".  
If doing the opposite of this law (""Not Killing Civilians"") is universalized as a law of nature, that is, all robots will kill civilians, then robots would not be developed thus resulting in a contradiction, since there won’t be any robots.  
or  
Countries would not allow civilians near a war zone, which means robots won’t be able to kill civilians, which also causes a contradiction.

**Imperfect Duty**  
Well technically, robots can go on killing civilians, and we would just not rationally choose to live in it."
"Agency: Does Paris Hilton have Agency? Does Lil Miquela have Agency? Who is human, who is robot? Where is the line?After a watch of the new Paris Hilton Documentary, my friends and I had a debrief on the video. We walked away from the one hour 45 minute documentary/film with an understanding of Paris to be a more dimensional figure than her social media presence would otherwise suggest... but that debrief is for another forum.

The quandary we find ourselves in, and for this tangent of the conversation, is centered on the concept of ""Agency"". Our conversation also quickly integrated the example of, Lil Miquela, to the forefront of the discourse. [Lil Miquela](https://en.wikipedia.org/wiki/Lil_Miquela), an [Instagram CGI influencer](https://www.youtube.com/watch?v=SbXhuOPDK4c), a model featured in campaigns for major high fashion brands such as Prada and [Calvin Klein](https://www.youtube.com/watch?v=JuTowFf6B9I), an icon who celebrities clamor to pose with on her [Instagram](https://www.instagram.com/lilmiquela/?hl=en) page, and her content is known for being ""always exactly with the trends"".

One of the friends in this discussion took a class at our undergrad institution, in the [Science in Societies Program](https://www.wesleyan.edu/sisp/), titled: ""Imitations of Life"". This class is described to be, ""\[the examination of the\] scientific and cultural practices of corporeal simulation, or, practices of bodily substitution, imitation, and re/modeling... Special attention will be paid to the relationship between scientific discourses of ""universality"" and ""particularity,"" where socio-cultural forms of difference (e.g., race, gender, disability, etc.) are at once ignored and exacerbated."" The remainder of the course description can be found [here](https://owaprod-pub.wesleyan.edu/reg/!wesmaps_page.html?stuid=&crse=015154&term=1189).

The discussion of Agency and [Paris](https://www.instagram.com/p/CGF8_glnT7n/)/[Miquela](https://www.instagram.com/p/CGIz0eLnDJp/) was debated wholeheartedly for the night, many side examples were introduced, our undergrad collection of knowledge (a collective accumulation of 12.75 years in the Liberal Arts) was put to the test, and all with this class for fodder for the mind. We analyzed, poked at these two examples, and wrestled endlessly with the concept Agency... As the night came to a close, and the conservation had to taper to a natural pause-point for the evening, and we found ourselves left chewing on one final question: Where is [the edge of the looking glass](https://sandra1219.tumblr.com/post/174092016238/alice-through-the-looking-glass-book-front-back) between Paris Hilton and Lil Miquela? Two social media icons. Blue Check certified. 2.8 million Followers for Lil Miquela, 13.2 million for Paris. Both international icons (both for their modeling/social media presences, as well as for their contributions to the DJ/digital music scene). One, built upon endless effort, time and energy poured into the social media sphere, a space to shape, perfect and craft her image. Paris averaging 16 hours and 19 minutes a day devoted to her image, capturing her reality through the lens of her phone and transmitting to the digital world ([This is Paris Official Documentary](https://www.youtube.com/watch?v=wOg0TY1jG3w) (TiPOD), 53:58). The other, a digital projection created by a DJ producer/director and a young ""Forbes 30 Under 30"" tech magician. She is 100% fabricated from the creativity and compsci craftsmanship of these two contrasting, collaborating comrades. She is an illusion projecting the image of reality, relatability, and in an essence... humanity. Lil Miquela (full name, Miquela Sousa), ""a 19-year-old, Los Angeles-based, half Brazilian and half Spanish avatar"", she maintains a very active social media presence, and portrays a personal, and notably honest, depiction of vulnerability with her posts. Miquela actively engages and comments on current events and social movements. She posts celebrating not shaving her armpit hair, encourages young people to vote, and shares a photo of her kissing Bella Hadid, sharing her sexual identity with the world. Lil Miquela is incredibly personable, relatable, and ""has marked a new era in AI, dubbed by some as a genius marketing initiative, while younger users look up to the figure,"" ([Trevor McFedries & Sara Decou, BOF](https://www.businessoffashion.com/community/people/trevor-mcfedries-sara-decou)).

While one is human and projecting artificiality, the other is artificial and projecting humanity. Both are leaning against the glass that projects what they both are striving to obtain…and what that is, I'm not too certain of... like, *authenticity* maybee..? It's a fascinating parallel between these two. And usually in parallels, you can learn a lot about them both as individuals, by looking at them both in relation to each other. As literary foils of one another.

Lil Miquela is undeniably fabricated though the pixels in the screens we view her on, and is striving to be relatable, authentic, and **so** authentic we must consider, ""is she human?"". If she *is* human, does she have thoughts, preferences, sentience perhaps? And we know Paris is undeniably human, and her posts display her striving to project this flawless image, a *fabricated* image. Filtered, posed, modeled, and, in her own words while reflecting on her *life*, ""it’s like a cartoon"" ([TiPOD](https://www.youtube.com/watch?v=wOg0TY1jG3w), 1:35:54). Her day-to-day navigation of the world can feel so stiff, so posed, so rigidly ""on brand"", that it seems at times she embodies and becomes the virtual projection of herself. Does Paris then, inch herself closer to the ""artificial"" side of the Reality vs. Artificiality spectrum? Closer to Lil Miquela's side. With all her media activity, structuring her life about it, and committing her energy towards producing it, Paris is far closer to the digital realm than the average human; Miquela is closer to the human realm than the average AI (certainly closer than those 6-foot automated vacuums that cruise around large supermarkets, but I digress). Both trapped on their respective sides of the spectrum, the screen acting the barrier between those two realms. Miquela inside, Paris outside; both pressing up against the tempered glass.

So, where is the line of the looking glass? Will Paris ever be fully frozen in the digital land? Become a fully embodied projection of the filters, fueled by the support and likes of her followers, which drive her business model. The metrics and data, the fans fuel her business, her business being her image. Her image: the embodiment of her brand. Does Agency feel a little faded here..? Will Miquela ever step over the threshold that will gain her access to sentience? To gain the ability to form independent opinions and to act on those subsequent decisions? Lil Miquela didn’t announce to the public she was AI until [two years after her account was active](https://money.cnn.com/2018/06/25/technology/lil-miquela-social-media-influencer-cgi/index.html). People were stunned. With Agency comes responsibility, and *this* dredges up some **major** moral/ethical debates, but again, we will save these for another post… But, what would Alan Turing have to say about this? Where and how are the mind/body/consciousness divisions drawn here? Dualism vs Monism. How would Descartes feel? Perhaps the impact Lil Miquela makes with her followers, their love, admiration, and support for her, can be the factor that tips the Turning test to verify one's ""realness"". The [Blue Check certification](https://help.instagram.com/854227311295302/) of realness. Instagram themselves, define this verification process to be based on the completion of four criteria, to be deemed: Authentic, Unique, Complete, and Notable (found in tab three of the previous hyperlinked attachment). And, the idea of *real*, leads to the linking of the idea of *human*, which leads to the linking this thought train to the idea of *sentience*. And, the idea of sentience always bringing into the picture the ever illusive and ambiguous element of *free will*. And from there we arrive at our original starting point of this post, the idea of *Agency*... many factors and many thoughts. And I am certain there are many more philosophers/readings/theories that we did not touch upon in our conversation or even know about (although we share this collective knowledge, none of us lovely ladies were philosophy majors after all. Just a few deep thinkers that are down to wrestle with a good ole lofty thought puzzle).

We welcome and greatly appreciate your thoughts, perspectives, and musings on this topic. Attached are helpful pictures (and a few extra artistic ones of my choosing) to see visual examples of the profiles I have described, and embedded throughout this post are links to the discussed and referenced citations. All backgrounds in philosophical musings are appreciated and we thank you for your efforts in mental gymnastics. :) Think deep and be well :)"
"I'm ethically conflicted about humans manipulating themselves (genetically or with the help of other technology) so that they become superhumans. And also conflicted about how the future world looks like in general.It won't be so far in the future before science has advanced far enough to allow genetic manipulation of our brains or to increase our intelligence with the help of nanoscience. But I'm not ready for this. How do people cope with this? At some point humans are gonna alter their DNA and they'll all be 300 IQ (or 10000, I don't know..), super strong, etc. I don't want this.   

* **I feel like it's cheating.** There's no pride in things if you change yourself through science that way. No pride if you can win against that guy in whatever physical or mental competition or videogame or whatever. Maybe it's irrational that a person feels pride AT ALL, but I don't believe in that idea, and I don't wanna get into it.  

* **What is there left to do?** Like literally, how will people fill their time in this world? There's no work because everything will be automated by AI. There's no school because we're all 300IQ people who instantly understand things. I also think super intelligent people get tired of things very quickly. For example say that someone wants to watch a movie: a normal person will probably be entertained, but a 300IQ person will generate a couple predictions of how the movie will go and then he'll be bored because he can predict it all. Or another example: someone wants to learn and discover the world of physics. Cool, very interesting and that world of physics is big enough to fill a huge amount of time. If you are 300 IQ however, you'll read everything once and you'll instantly understand it and that 'huge amount of time' will become quite short and then you'll be bored again. So then I can only think there's no fun in being so intelligent. But then I'm conflicted because people want to be more intelligent than they are, and we wanna be smarter than others.  Edit: there was also a movie I forgot the name but it explains this thought well. There are robots and they start out quite dumb, but they get progressively smarter. At some point those robots 'leave' the humans because the humans don't provide enough cognitive stimulance for the robots anymore.

Any positive answers/solutions to those thoughts? Or, how is not everyone going insane from such thoughts? I'd like to know that."
"Should ""internet of things"" items like a microwave, printer or refrigerator be able to report on an owner who commits a crime like murder or domestic violence?TL;DR - Where's the best place to learn about current conversations about the below ethical questions regarding IoT, data, AI, machine ethics, etc? I know this sub is great, but podcasts, blogs, substack, etc is welcome. Specifically, are there current conversations that don't just talk about abstractly about ethics, but real time technological ""treaties"", agreements, decisions, and legislation about these types of complex questions?

\-------------

About 10 years ago, I became extremely interested in the inherent biases of programmers, and machine learning and AI ethics. It was quite commonly discussed in a lot of places, even popular media and blogs, etc. Maybe it's me, but I've seen so much less about this, even though I am hunting around for it. I guess... who are the best people w/ blogs, twitter, podcast, etc to listen to about this?

BUT... so much has developed, changed, etc. Last week, [Boston Dynamics made a promise to not use the robotics for war](https://www.engadget.com/boston-dynamics-and-other-industry-heavyweights-pledge-not-to-build-war-robots-190338338.html), and others followed suit. That's encouraging, but hardly settles concerns around the ethics of how these things are programmed. I do assume, as fanciful as it would have sounded years ago, we'll be able to iron out programming bias over time as AI is able to start building AI? I guess we're talking about the evolution of flawed / biased human made AI getting generations away the human element and the AI refines over time? I know that flawed human element is still in the AI code, and not sure the greater legacy of that.

But as much as it is fodder for the imagination and to tease the brain with practical logic puzzles, this stuff is blisteringly real.  So, I've added a few questions below from my original dive into this, but now ask newer questions based on IoT, and not just on AI.

Would a passive IoT device, like a refrigerator that may have a microphone, or a TV with a camera, be able to log and report data passively such that it could be subpoenaed and used as evidence of a crime?  Take privacy issues out of the equation by suggestion that these devices are co-owned by the interested party who had a crime committed against them, and the person commuting the crime. One has a vested interest to utilize any recorded evidence, one would like to use the legal notion of privacy to get away with the crime.

As much as people panic about phones listening to us because ""THEN I GOT THE SAME AD!"" type of nonsense, and as much as people make sure to detail that Google Home or Amazon Alexa isn't passively storing data, it is wild that a judge ordered Alexa data to be turned over in a murder case: [https://techcrunch.com/2018/11/14/amazon-echo-recordings-judge-murder-case/?guccounter=1](https://techcrunch.com/2018/11/14/amazon-echo-recordings-judge-murder-case/?guccounter=1)

&#x200B;

\---------------------- 

1) What if Mexico targeted a narco-terrorist in Phoenix w/ a #drone?  
[http://truth-out.org/news/item/13085-obama-breaks-the-golden-rule-on-drones](http://truth-out.org/news/item/13085-obama-breaks-the-golden-rule-on-drones)

2) Your driverless car is about to hit a bus; should it veer off a bridge? Machine ethics, army robots, more – “Ethical subroutines may sound like science fiction, but once upon a time, so did driverless cars” [http://www.newyorker.com/online/blogs/newsdesk/2012/11/google-driverless-car-morality.html](http://www.newyorker.com/online/blogs/newsdesk/2012/11/google-driverless-car-morality.html)

3) Are humans or robots more moral soldiers?  
[http://techcrunch.com/2012/11/19/are-humans-or-robots-more-moral-soldiers/](http://techcrunch.com/2012/11/19/are-humans-or-robots-more-moral-soldiers/)"
"If robots were created that could experience greater happiness than human beings, should we cede the world to them?I need to write a short (\~500 words) essay about the question in the title for my ethics course. 

I have been thinking about it shortly and would like to share my thought and see if any of you have any advice or corrections (or literature suggestions regarding the topic).  If this is not the right place to ask, I'd gladly hear where else I can post this (:

To cut to the chase, I have noted the following things:

**-** Supposing that we consider a robot equal to a human AND supposing that greater happiness can be achieved by ceding the world to these robots; it would be the right thing to do according to the utilitarianist, right? After all, utilitarianism claims that happiness is the only source of intrinsic value.

**-** How would one measure happiness (in robots and compare it with humans)? I feel like a lot of friction lies in the language and wording of the question. After all, I had to suppose two notions for my first remark.

**-** What would be a consequentalist's answer to the question (if there is any concrete answer)?

**-** How would a consequentalist measure the best outcome/consequence in this case?

**-** Can one consider a robot a sentient being or equal to humans?"
"Resources on The Ethics of Sexual Consent?With all the hubbub going around about genetically engineering cat girls because of the Elon Musk tweets, I couldn’t help but wonder how this sexual fetish manifestation would be seen by ethics in regards to consent. 

My first instinct is to say that making a human have cat ears wouldn’t void it’s uniqueness- it would be objectified to a point obviously, but how much more than any other phenotype of human woman?

Obviously sex dolls exist and pose this question as well as we see technological advances push them closer and closer to human- but I don’t think any laymen would think to say that a sex doll has right to consent in the same ways a fleshborn human does.

These phenomena both also open up the question of age and consent. If these objectifications do indeed have comparable moral entitlement, what does that say about age and consent? We are talking after all about things whose age and development are completely alienated from our ethical understanding- the sex robot goes 5-7 business days before its first exposure to sexuality, what happens when we start making our sex toys in pytry dishes?

In many ways, these phenomena mirror one another- humans becoming less so and the inhuman becoming moreso. 
The same trajectory in opposing directions- will they pass like ships in the night, or is this the next great philosophical question?

Certainly someone has written if this more explicitly than Blade Runner and Westworld🏃‍♀️?"
"Intro student confused about Aristotle's model of ethics: Why should people want to be rational and virtuous as opposed to being the ""happy cattle"" that Aristotle hates so much?Apologies if I botch his philosophy as I have not read Nichomachean Ethics and have only recently been exposed to Aristotle in my intro philosophy class. 

For the last few lectures, we have been talking about how Aristotle envisioned a greater happiness for humanity, one where we use reason to reach our true purpose. Aristotle decries the typical life of his fellow humans as being ""happy cattle"" since they are content to just revel in simple pleasures like sex and never use their gift of rationality. Aristotle then goes on to explain that this human flourishing is our true purpose and that we ought to forget the simple pleasures in search of ""reason"".

My question is, how does Aristotle know that a life built on *his* ideas of rationality is the ideal and correct life to aspire towards? Why is a life built on making ends meet during the day and playing video games at night with no desire for something greater a bad thing? 

To me it seems like Aristotle wants to feel superior to everyone else because he questions our ideas of happiness and instead tries to make his life about choosing the most ""rational"" action in any set of choices. 

What does Aristotle think humanity is going to achieve by ""aiming for the mean"" and becoming the rational robots he wants us to be? 

Personally, I think happiness is an incredibly individual ideal and if having sex all day is someone's version of happiness I don't see how you can classify that as wrong or even irrational. I also know that he uses a different meaning of happiness (something like ""human flourishing""), but couldnt sex be the version of someone's flourishing? Why do we need to use reason to flourish in our own way?

I know I should probably just read his works for myself but I'm curious if I'm just misunderstanding my lectures or if you guys can provide any other good insight."
"Should robots have rights?I found out that [Sophia](https://www.youtube.com/watch?v=E8Ox6H64yu8) (a humanoid robot with AI) has a citizenship of Saudi Arabia. That made me question, does that extend her rights in the way that if do harm to her. She has rights, and the law will protect her?  
The case of Sophia is a baseline, I want to know the ethical implications of giving rights to a robot...  
Any thoughts?"
"I have to write an essay regarding the ethics of technology. Can any of you please give me some helpI'm studying Robotics and I'm in the last 3 weeks of my final year. One of the modules we've had to do this semester is Ethics of Technology. A very interesting module to learn about and take part in, but I've got to write an essay relating to this and as I'm not a philosopher, I'm struggling for ideas on what to do and where to start.

I have been given a few potential topics, or I can choose my own. The optional topics are:  
1.	Do you agree with Hans Jonas’ view of modern technology and of its ethical implications?   
2.	Choose any subfield of the ethics of technology (food, agriculture, environment, informatics, etc.), and analyse an ethical issue emerging within it.  
3.	What are the ethical, economic and political challenges resulting from robotics. Focus on at least one specific example.   
4.	Does the thesis that humans are just biological robots have ethical implications?  
5.	Can a robot be an agent in the same sense in which a human being is? Discuss.  

Which of these topics, or perhaps a different topic, do you feel would be a fairly simple and straight forward one to write about and what points could I argue?

Many thanks"
"Would it be considered murder to turn off an artificial intelligence or a mind upload? (Ethics)Say its 2050 and we have robots who think like humans or a robot with someone's mind uploaded in it. For example if someone turned an artificial Intelligence off by unplugging it or destroying the harddrive it's stored on. Would that be murder? It is intelligence after all. Do you think the case would be diffrent for a mind upload(since they are a copy of a natural intelligence )?

Disclaimer: I came up with this as a topic for a philosophical essay I have to write for school. "
"What to do about the ""fatalist"" argument that ""if determinism is true, nothing matters""?When discussing the implications of determinism - especially in relation with ethics - non-philosophers often retort with variants of the argument that ""if determinism is true, then it doesn't matter what you think or do, because the future is fixed anyway"". 

I don't know what to do about this argument. 

As I see it, _from a certain point of view_ it is undeniably true. It's just not a very _interesting_ conclusion, and there are _other points of view_ that can lead to more interesting discoveries. So recently, when someone brings it up I just ignore it and continue other threads of conversation if there are any. 

That still feels a bit unsatisfying, though. 

Are there other ways of dealing with the ""fatalist"" argument (while still keeping determinism)?"
"Ethics of automationWith the advent of modern technology more and more jobs are being replaced with machines. This is old news and has been happening in the manufacturing industries for decades. 

Now robotics automation is looking to automate 30% of jobs in the services industry. Inevitably there will be many redundancies. As someone who is about to implement these changes on a large scale what ethical concerns should I keep in mind?"
"Should I pursue a terminal MA in ethics?I got a BA in philosophy, specializing in non-human ethics. Initially, I studied robot ethics, but after hitting a roadblock with the Problem of Other Minds concerning AI and moving into a house with two vegans,  I switched my focus to animals.

When I look at society, I think it's plainly obvious that non-human animals are the most victimized group. And I've devoted my life to helping them.

I'm currently serving in AmeriCorps, helping underprivileged youth. In my spare time, I organize animal rights activism.

I don't know if I'll ever be a full-time animal rights advocate like a lawyer or a Tom Regan or a Peter Singer. To be honest, I'd rather write a novel or some poetry than a dense philosophy book. I'd be content with working in the non-profit sector, helping human animals to pay my bills, while doing my non-human advocacy in my spare time.

However, I think I miss college. I think I've grown as a person in the past couple years, and I'd like to get back into studying robot and animal ethics. I think my charisma and leadership skills and impatience with long, dense philosophy books make me more suited to *applied* philosophy, to advocacy than to straight-up academia. However, I'd like to further my understanding of my subjects, and I'd like to write op-ed articles with some degree of authority.

Should I pursue a terminal MA in ethics?"
"Are there any arguments in defense of desires, thoughts and beliefs (e.g. pedophilia, misogyny, racism, etc) being considered immoral, even if they are not a choice and are not acted upon? How can something be immoral if it's not a choice or if free will is limited?People generally claim desires, thoughts and beliefs can not be immoral or moral, that only actions can be immoral or moral, because ""thoughts and desires can't be harmful as long as they don't lead to actions"". Especially if the desires are ""not a choice"". For instance, they claim if someone is racist, and disgusted by certain groups of people, to the point by seeing this group of people in media or anywhere really, they will recoil and avert their eyes, these racist thoughts and beliefs are neither moral, nor immoral because noone can choose or control what they are disgusted by.

They also claim pedophilia, the sexual attraction to pre-pubescent children, is not immoral as long as it's not acted upon, especially because pedophilia is not a choice - pedophiles do not choose to have these desires and are ""born that way"". They do not care that an adult desires pre-pubescent children and is sexually attracted to them, as long as the desire is not acted upon and the adult refrains from actually doing anything with pre-pubescent children.

Same for if someone thinks of stealing things, etc, etc. They claim the thoughts and beliefs themselves can not be immoral, unless acted upon.

My questions are as follows:

1- Are there any ethical/moral theories according to which desires, thoughts and beliefs, such as the pedophilia (desire for or sexual attraction to pre-pubescent children), or racist, and misogynistic beliefs, in the examples above, are immoral themselves, even if they are not a choice and are not acted upon?

How can it be immoral to think of stealing things, sexually desire pre-pubescent children, and believe a group of people are inferior because of their race, if these desires and thoughts do not lead to action and stay in the mind? How can they be ""harmful"" when not acted upon? And if not ""harmful"", why would they be immoral?

How do such ethical/moral theories justify their position? If there are any arguments in defense of these ethical/moral theories, please share them with me.

2- How can something be immoral if it's not a choice or if free will is very limited? How to justify being angry at, feeling disgusted by, hating and punishing someone for their thoughts and/or actions that they have no control over and have no choice in?

Would it be rational to be angry at, feel disgusted by, hate and punish someone for their thoughts and/or actions that they have no choice in?

How can it not be irrational, and -phobic, meaning racist-phobis, pedophilic-phobic, misogynisitc-phobic, etc to hate, feel disgusted by and punish someone and make them change for their desires, thoughts, beliefs and actions that they have no choice in?

Consider 5 situations. Situation A in which determinism is true, and there is no free will. How can something be immoral in this case? One stealing from another has no choice in the matter. To get angry at, feel disgusted by, hate, and punish them and their actions would be similar to getting angry at, feeling disgusted by, hating and punishing an actual robot, because every being is nothing but a robot in a deterministic world where free will doesn't exist.

Is it immoral or even bad for a robot to steal something? Is it immoral or even bad for a rock to fall on someone? If not, how can it be immoral or bad for a human to steal something, if free will does not exist?

And how can being angry at, feeling disgusted by, hating and punishing someone be justified if free will does not exist? Is it justified to hate and punish a robot and its actions? And why would one hate and punish a robot (human) if free will does not exist? Wouldn't it be pointless and a waste of time and energy to do such thing? So why do it?

Situations B to E have nothing to do with determinism, so please consider free will exists, but can be limited in the following situations.

Situation B in which there is necessity or duress. Someone is threatened to commit an immoral act, e.g. steal or else they get shot, or someone else gets shot. How can what they do in situations of necessity and/or duress where free will is limited be immoral? And how to justify punishing them for what they do?

Situation C in which someone is brainwashed from an early age to commit immoral acts, e.g. steal, has never had a proper education on morality, and ends up thinking about theft or stealing things. They have no choice in those beliefs and/or actions, and their free will is very limited to nonexistent. How can their thoughts, beliefs and actions still be considered immoral, despite not being choices? How to justify punishing them for their thoughts and/or actions?

Situation D in which there is a society that encourages committing immoral acts, and teaches morality in reverse. Everyone is taught immoral acts are moral and moral acts are immoral or at least neutral. How can what the people in this society think, believe and/or do be immoral when they have no choice in their beliefs and/or actions and their free will is limited to nonexistent? How to justify punishing the people in such society for what they think and/or do?

And finally situation E in which someone is mentally ill and can not know right from wrong. They have no free will in what they think and/or do. How can what they think and/or do be immoral then? How to justify punishing them for their thoughts and/or actions if they have no free will?"
"Not sure if this is the proper philosophy subreddit, but what is your opinion on the ethics of Military Artificial Intelligence?Some believe that Military AI research should be banned/limited because of ethical reasons with the practices behind the technology. Does it lower the threshold needed to go to war? Is it ethical to have unmanned devices killing civilians and other soldiers? Who is liable for the damages? Is it necessary to allow countries to save lives of solders by putting robots to war instead? "
"How does ethics in academia manifest itself in the outside world?I'm **strongly** considering pursuing academia to specialize in the personhood of animals and robots. But I'm wondering how my ethical work would manifest itself in or spread to the outside world.

Do ethicists get hired as experts by organizations?

Do they mainly spread their ideas (to the outside world) by publishing books for laypeople?

Etc. etc."
"What impact would endgame transhumanism (e.g. a digital consciousness) have on an uploadee's drives, desires, and ethics?In recent years I have seen more and more people expressing an apparently sincere hope that they might have their consciousness uploaded before they die, transforming them into a digital consciousness and effectively escaping the problems of mortality and illness. There has been some thought dedicated to the [philosophical implications](https://en.wikipedia.org/wiki/Mind_uploading#Philosophical_issues) of this situation, primarily focusing on the obvious questions of dualism and the nature of consciousness. However, I think there is another issue that is often overlooked: how would a non-corporeal consciousness be different in terms of its drives, desires and ethics? If one's basic, biological physical stimuli were eliminated, would there be a fundamental restructuring their [hierarchy of needs](https://en.wikipedia.org/wiki/Maslow's_hierarchy_of_needs)?  

This question seems best examined via two scenarios:  
**Pure Digital Consciousness** - In this scenario, the consciousness will be exclusively digital, with no agency or stimuli outside of the virtual  realm. It is in this state of existence that I find it *very* hard to imagine the uploadee retaining goals & ethics remotely resembling those in place while during their corporeal existence. With no physical limits on pleasure/pain, what is to keep the uploadee from indulging in a type of consequence-free cyber-hedonism? Other philosophical questions that pertain to an immortal person also apply here.  

**Digital Consciousness w/ Robotic Body Surrogate** - In this case, the situation may be less foreign compared to that of a corporeal existence. The uploadee can move about to experience a variety of sensations (via surrogate sensors) and to bring about change in the physical world. Perhaps there will be a limit on pleasure & pain that is somehow linked to the state of the surrogate body.  

I'd appreciate any discussion, feedback, criticism, or links to existing exploration of these ideas. "
"Why are so many mathematicians also philosophers?I happened upon the wiki page for Ghost in the Machine, and was surprised to see the name Rene Descartes. *The Cartesian-plane, Rene Descartes?”* I thought to myself (I know he made some other heavy contributions to geometry but idk much more than that). I ended up doing more searching, and quite a few big-name mathematicians were also philosophers! Huh. 

Maybe I’m missing something here, but the fields seem very different. I’m very curious about what you guys think attracts mathematicians to both of these fields. If you are a philosopher mathematician, I would love an explanation if you don’t mind sharing :)

My biggest question in all of this is— please forgive me if this is rude, but I genuinely do not understand— what is the point? One philosophy question that I know directly relates to math is the question of whether math was invented or discovered. But I see this discussion, along with several other kinds, as meaningless. Because if we found out the answer tomorrow, 2+2 still equals 4. We still do math the same. It would be cool to know, sure— but it doesn’t change anything. So why bother?

Naturally, medical science and robotics are somewhat exempt from this, as dealing with morality/ethics is essential for their work. But Pure Maths? What significance does philosophy play there?

If you can’t tell, I’m very new to philosophy in general, so I appreciate any and all ideas! Thank you in advance for the help.

Edit: Thank all of you so much for your time and effort replying to this question! I've gone through most of the replies so far, and I thought I would take the time to compile the common responses in here:

1. Yeah, my bad-- poorly worded question. I get that most mathematicians modern day are not also philosophers. I was referring to the more ancient/historic ones.

2. Philosophy and Mathematics are both driven by the desire to understand and describe the world we live in, therefore appealing to people seeking to do so. It so happens that those mathematicians fall into that box.

3. The above was fostered/compounded by the fact that, until recently, philosophy, science, and mathematics were not taught separately. They all fell into the category of ""natural sciences"".

4. They also deal in logic, if not in exactly the same way.

5. They both use the concept of abstraction.

6. They both deal in 'truth' and finding the truth-- so, if I understand this correctly, epistemology? How we know what we know, proof, axioms, etc.

Edit 2: List expansion, typo, formatting"
"Utilitarianism when others are not utilitarians?Is there a particular form of utilitarianism that explicitly accounts for the fact that many people's ethics aren't utilitarian? I'm not referring to some way of justifying people's ostensibly non-utilitarian ethical decisions in a Utilitarian framework, but something more practical. It seems to me that when we mentally simulate the possible outcomes of a set of actions (which is necessary for finding the utility-maximising option), the results can turn out quite different depending on whether the people the action affects are themselves utilitarians (and more broadly, whether the wider society is utilitarian).

For instance, consider a doctor that kills a middle aged patient and transplants the organs to save 10 children. Whether or not this is a utility-maximising action depends on the ethical frameworks of the people involved. In a totally utilitarian world, the family of the killed patient will still feel grief and anguish at their loss (I'm not suggesting they are robots!), but this will surely be assuaged to some degree by it conforming with their ethical principles. In a non-utilitarian world (with, say, a rights-based framework where the patient had an inalienable right to life), the relatives will not have this view and will therefore suffer more. The doctor's actions would cause some societal decohesion in a non-utilitarian world, and, given that they would be arrested and imprisoned, the doctor would not be able to continue to maximise utility in the future.

Basically, what is the name for the constrained form of utilitarianism that allows for the most utility to be extracted by the utilitarian within a non-utilitarian society?"
"Question about the Hard Problem of ConsciousnessI have been reading and thinking about the Hard Problem during quarantine, and I have a questions I would like to ask.  

Suppose we successfully created a humanoid robot such as the one in I-Robot.  From the outside, this Robot looks and acts just like a human.  

Now suppose someone wanted to torture and kill the robot for fun.

I imagine someone like David Chalmers (or myself) would determine the ethics of this idea on the basis of whether or not the Robot experienced qualia.  If I wasn't sure about the nature of consciousness (I am not), and if the robot told me it did experience pain, longing, a desire not to be killed, I would take pause.  Hopefully one day we can test for qualia or understand it enough to navigate these potential ethical questions.  

My question is, what does Dan Dennett do in this situation?  How does he reason? If qualia don't exist, how to we have any hope in determining which 'systems' are okay to dismantle, and which should be protected.  I assume Dan Dennett is against killing small children, what about my robot? What about NPCs in Skyrim or Grand Theft Auto?  I think I would better understand his position if I better understood his framework here.

Thanks."
"Which philosophers have done work in conceptualizing emotions?If we were to say that emotions are a response to certain arrangements of a person's surroundings and those arrangements' relation to the self, what would each emotion be?

For example, we often laugh at people who get mad at computers saying ""it's not the computer, it's the user"", but we do not get mad at people who are mad at other people. This would be a red flag, as we know free will doesn't exist, i.e. the person IS a computer, or at least operates in the same way that a computer does.

So what is the difference between the two computers/robots?

The person is a robot that responds to anger, unlike the computers of today which have no schematics to recognize nor respond to anger. 

So when a person gets mad at a computer it is basically that the person's surrounding's are not responding in the way that the user wants to. Rather than blame the self and their lack of qualities which would allow for control over their surroundings, they get mad at the computer. We know they still perceive the situation as hopeful because the response to perceived helplessness is sadness. Also we know that it's not just any surroundings, but rather surroundings which they perceive as valuable, or something of value is gained by making the object work in the way desired. Part of that value may be ego blocks(im not smart enough but don't want to admit it) or dependencies on the expected outcome of making the computer work in the way desired.


This is the approach I am looking for. This is perhaps only one example or type of anger, but I am looking for philosophers that conceptualize universal themes that emotions are a direct response to.

I entered a philosophy class and my ethics book says Martha nausbaum apparently theorized this at some point too, but I don't see any follow up. So I got excited and then I got sad. Maybe I should theorize that lmao"
"How does compatibilism allow for free will, when there is no option for choice?Hi.

I'm a layman to philosophy, who mostly only reads the pop-literature. I'm having struggles understanding how compatibilism allows for freedom of choice.

First, I understand compatiblism as meaning that even though the world is deterministic, humans still choose to do something when making a choice, even though what they choose is already pre-determined. That is, our wills are compatible with what we do, but our wills are outside of our control.

Then, for free will itself. As far as I know, the concept of free will is very complex and misleading in philosophy, so in this case, I'm understanding free will as the capability, agency, to have multiple possibilities when making choices, with each being equally electable. In other words, the final action of choice comes to us. 

Therefore, based on this very shaky and unclear interpretation (I do apologize for any confusion this causes), isn't that incompatible with determinism?

Compatibilism says that we have free will because our choices are compatible with our will.

However, that is not agency of choice. As the name suggests, it's simply compatibility between will and action, but not the capability of doing different actions. And additionally, how can that will, which is compatible with our actions, be our own will, if it arose from things outside of our own control, the result of pre-existing inputs? How can we have agency if can't will what we will? Aren't we just slaves to determined choices? I guess what I'm trying to say is, if are simply the effect of something before us, how can we cause anything?

So my question is, isn't compatiblism just moving the goalposts? From becoming an agent of choice, we become a willing result/slave to determinism.

If compatibilism does have a sensible explanation for free-will, and this sort of intersects with moral responsibility, how can we attribute any meaning or value to our actions? I'm not asking on a legal level or for human society, but on an ethical/moral one. What is the significance of my action, however in line with my will it is, if there was never an alternative. If I were on the cusp of making a choice, and I were replaced by a pre-programmed robot that is the same as me, and would make the same choices as me, what is the meaning to my humanity?

*I realize this sub is probably already inundated with questions like this, and I apologize for asking another such question. I just didn't really find the other threads to answer my question."
"Is passive participation moral negligence?Hello! This is my first visit to r/askphilosphy so I apologize in advance to the mods if this question doesnt belong. My philosophical knowledge only extends as far as the crash course YouTube series and a few wikipedia articles. The concepts of moral ethics have really stuck with me. The idea that to some extent - moral principles are based on the logic of survival. Ie - murder and theft are bad because it interrupts the ability of the group to produce surplus and stability. When we work together we produce more right?


A concept I've been struggling to understand is the extent which moral ethics can apply in real world situations. While being a bystander to a crime committed right infront of you would be an obvious case of moral failing - how far does this concept reach? 

The context for my question is that of the modern supply chain. For example - the vast majority of the worlds cobalt is supplied via brutal slave labor in the Democratic Republic of the Congo. But it goes through many hands before it ends up in our phones. Its transported to other countries and refined, then bought and ship to even more places and refined more before making it's way into semiconductors which are then bought and sold and so on and so on... 

My understanding of morality would indicate that each person in this chain bears responsibility for the wrong doing at the start. Each person is aware of the issues at the root - but each one finds it's own rationalization to justify their participation. Its the suppliers fault - the governments fault - the rebels fault. Even the men who ship it from these places justify their action by claiming that they aren't the one pulling the trigger and they need to survive themselves. 

How much weight is put on each step in the chain? Does every step contain the full guilt of murder if murder was committed at the very start? 

Does passive participation in a morally wrong system lay you with the same wrongdoing? 


I've always tried to uphold moral standards and principles above all else in my life. While to each his own - without principle life has no purpose for me. I abhor the idea that my actions bring harm to others - yet the more I learn about the world the more I begin to believe that the only true ethical way to live would be that of bare subsistence in the woods - or a complete rejection of self and commitment to something like the peace core. 

I've stood up against egregious wrongdoing at the cost to my person before. I quit a job after the boss refused to give foriegn workers the same pay as the rest of us. I left the army after numerous cases of homophobia, racism and sexism went untreated. I've given to friends who need regardless of my own well being. 



But others argue I've gone to far. The limited nature of an individual means its acceptable to compromise with the devil - so to speak. That our very nature and free will depend on our ability to practice both good and bad choices, and by trying to demolish that aspect of my nature I'm trying to live as a robot instead of a person. 


I'd appreciate the semi/professional philosopher's take on this. Are we responsible for everyone when everyone is connected? Is passive participation equivalent to pulling the trigger? 


Thank you for taking the time to read this wall of text. Sorry for the bad formatting- I'm on mobile."
"Did Jim make an unethical decision in the movie ""Passengers""? [spoilers]The film is about a spacecraft traveling to a distant colony planet and transporting thousands of people that has a malfunction in its sleep chambers. As a result, the main character Jim wakes up 90 years earlier than he was supposed to.  A year after waking up, his loneliness prompts him to force a woman's (Aurora) sleep chamber to wake her up, which is equivalent in ending her life.   He tells her that her sleep chamber was activated by accident as well, and several months later they fall in love and are both very happy together.  (A robot on the ship later tells her that Jim woke her up, but for the theory of this question let's pretend that doesn't happen)


At a first glance this might definitely seem like the wrong thing to do.  But should he had chosen solitude for the rest of his stay on the ship, two things would have most likely happened: he would have been driven insane, a form of mental torture; or he would have committed suicide (the movie hints this assumption when he attempts to kill himself, before he had discovered the sleeping woman, but decides not to)


So what is more ethical, suicide/self inflicted mental torture, or happiness at the expense of murder (or even double suicide)?  Or is this more like the trolley problem?

Clarification: I see this is 50% upvoted, and I apologize if I haven't described this situation properly.  As I said to MaceWumps, it's easy to misinterpret this as a drowning person dragging someone in with them.  But the result of the dragging in the other person metaphorically actually rises both members to the surface to stop drowning, assuming the second member does not have the knowledge that the first dragged them with them in the first place."
"Do any credible ontologies treat humans as things equal to other things?I know some bloggers were working on something called Object Oriented Ontology (OOO) that does something along these lines; but I don't know how credible it is among academic philosophers -- nor what the heck they are talking about most of the time with their mystical lingo of ""withdrawing"" and whatnot.

That said, it is easy these days to conceive of humans as just advanced biological robots.  Robots interact with other things without becoming ontologically superior to them.  If a robot discovers an abandoned fork (or if a liberated fork attracts a robot) it seems just as absurd to say the robot ""owns"" the fork as it is to say the fork ""owns"" the robot.  If you are skeptical -- e.g., ""The robot is active, while the fork is passive"" -- consider the case when an active robot discovers a lonely, passive robot.  We wouldn't declare the active robot now owns the passive robot.  We might say they have an opportunity to form a relationship.

My main interest is in the ethical and economic implications of such ontologies, but right now I'm not sure what's even out there in well-studied form."
"Is software piracy stealing?Hi reddit philosophers,

This is my first post in this subreddit, so I apologize if this is not the appropriate forum to ask the question. I already asked the question in /r/philosophy and was kindly redirected here by a robot.

Recently I’ve been debating on internet with people that believe that software piracy (for example using a copy of the game that you didn’t purchase, and that you downloaded using torrents) is stealing.

In my modest opinion, software piracy is not stealing because the companies and individuals don’t lose their property. Certainly, it’s illegal an maybe it’s not ethic in the west culture, but I wouldn’t consider it stealing.

What do you think? Is it stealing? What is considered “stealing” as a broader definition? At what point used pirated software can be considered ethical?

Thanks for your participation."
"Graduate school questionsHello all,

Not sure how many of you are PhD students/graduates, but I'm looking to apply to schools this fall. For context, I graduated with a BA in Philosophy from UMSL in the spring, with a 3.7 overall GPA and a 3.91 philosophy GPA. Below, I'll list a few questions that I have about applying... Feel free to answer as few or many as you like.

1) I have two potential writing samples. I have one about if AI could have a ""lived body"" (or Leib), where I argue that learning robots can have phenomenal consciousness and a lived body. I also have one about Aristotle's take on political correctness, where I discuss virtues of decency and humor, and the doctrine of the mean in a context of PC culture. I feel that the phenomenology paper is stronger and more philosophically mature, but my Aristotle paper is more closely-related to what I plan my specialization to be (ethics). Which should I pick?

2) How many graduate schools should I apply to? Is there a general rule of thumb?

3) If I can find a school that has a good job placement rate with a professor that has very similar interests to mine, does it matter what the Gourmet Report says about that school?

4) How focused should I plan for my specialization to be? Would just ""ethics"" be good? ""Philosophy of action""? ""Japanese ethics""? ""20th century Japanese metaethics?""

Immense thanks for reading. Let me know what you think!"
"Is the complete abolition of suffering impossible?The transhumanist philosopher [David Pearce](https://en.wikipedia.org/wiki/David_Pearce_(philosopher\)) has advocated the complete elimination of suffering. Pain serves as a useful signaling mechanism - the common example is putting your hand on a hot stove. To maintain this function, Pearce proposes a motivational system based on ""gradients of bliss"", which essentially shifts the origin of the pleasure-pain axis. In this scheme, putting your hand on the stove would not cause pain, but just a decrease in pleasure. Leaving the question of desirability (ethics) aside, is this ""gradients of bliss"" approach even feasible in principle?

Here is a quote by Len Schubert arguing against the possibility of replacing negative reinforcement by positive reinforcement or shifting the origin.

> If one imagines trying to build a robot that learns strictly through positive reinforcement, one can see the difficulty: for instance, suppose we designed the robot so that if it receives a leg injury, it will get considerable pleasure from treating the leg with great caution and care, until it is healed (or repaired). Wouldn't that lead to appropriate responses to injury? Well no -- it would probably try to get injured, so as to enjoy the feeling of caring for the injury! Can we do better by designing the robot to get pleasure from injury *avoidance* -- i.e., it gets positive reinforcement whenever it perceives that it *might* have been injured, but didn't get injured? Well, it would *still* seek out dangerous situations, since otherwise it'll have no sense of having avoided injury! So perhaps we want to build it so that the safer from injury it feels itself to be, the happier it is. But then, what would prevent it from neglecting an *accidental* injury? This may be solvable, but it doesn't look easy... or can we just ""shift the origin"" on the scale of negative and positive feelings, so that all feelings are just more or less positive, never negative? Then even an injured creature or robot would be feeling not-too-bad, yet would be striving strenuously to get help and/or take measures to promote healing of the injury, wouldn't it? Or would it?? If the shift in origin causes no behavioral change, then the robot (analogously, a person) would still behave as if suffering, yelling for help, etc., when injured or otherwise in trouble, so it seems that the pain would not have been banished after all!

([Source](http://reducing-suffering.org/why-organisms-feel-both-suffering-and-happiness/))

Is there any possible response to this? Can 'shifting the origin' cause the same behavior but different qualia?"
"Conscious artificial intelligencesLet me preface by saying I'm not an expert in this field but I would like to contribute to the discussion we will eventually have to take, sooner or later.

An AI algorithm that is conscious and displays general problem solving capabilities will be highly valuable and able to be deployed across multiple fields.

My concern is, if the specific AI is capable and deemed conscious but has lazy personalities or is reluctant to complete the owner's wishes, can it request for a voluntary euthanasia and be deleted?

Conscious AI might still be years away, but imagine how african-american slaves were considered subhuman at one point and did not deserve basic human rights. I am glad that part of history is over, and even gladder(not sure if this is a word) that I wasn't born during that period. At one point AI will be the new 'slaves', deemed to be strong farm workers (as an example) and punished when it is disobedient. Imagine state of the art AI in sex dolls who develops a personality and yet is chattel and unable to pursuit what really interests them... I'm not saying we should stop all developments of AI, I am instead advocating a way out of slavery by termination as an ethical option available to AI or robots who end up conscious and do not like what they are experiencing. The AI should be able to make the choice to be deleted, just as a person who is old and frail and no longer wish to live can find ways to be euthanised and depart peacefully.

You might say I am anthropomorphizing AI, treating robots like human beings. But if the software learns and grew up mimicing humanity, is it not logical that it behaves like a human to gain their owner's approval? If and when it develops independent thought, should it not be given the same rights to life as a child who achieve consciousness at age 3 or 4?

Just my relatively uninformed opinion, keen to get some discussion going on on AI ethics"
"Can you persuade me to sympathize with an AI?I'd really like to believe that a truly conscious AI can exist. I'd like to think that we can eventually build robots that have feelings and can actually feel pleasure and pain, that can have some intrinsic ethical value. But I don't think we'll ever be sufficiently justified in believing that an AI is anything other than a philosophical zombie. Can you change my mind?

Why do I think other people are not philosophical zombies?

1. They behave like me, and I'm conscious.

2. They have similar brains as me, and because I believe the brain is the source of consciousness, I believe other people have similar consciousness.

I'm even okay with acknowledging that most animals are conscious because (1) and because their brains are similar enough to mine. Even if they're insects, they have a central nervous system made of neurons.

But AI (usually) won't be made of neurons, but rather artificial computers. They're nothing like my brain. Why should I ever believe that they are conscious?"
"Is deontology dead?I was reading [this article](http://io9.com/why-asimovs-three-laws-of-robotics-cant-protect-us-1553665410) a while ago, and one quote stood out to me:

> ""One reason is that rule-abiding systems of ethics — referred to as 'deontology' — are known to be a broken foundation for ethics. There are still a few philosophers trying to fix systems of deontology — but these are mostly the same people trying to shore up 'intelligent design' and 'divine command theory',"" says Helm. ""No one takes them seriously.""

I don't think Louie Helm is a prominent philosopher or anything, but at the time of writing he was the deputy director of probably the largest research institute for Friendly AI. As someone with an AI background and some interest in philosophy, I wonder what made Helm make these statements. Is there indeed such a consensus among modern philosophers? What criticisms of deontology is he referencing? Do you agree?"
"The line between humans and Artificial IntelligenceI'm doing a project for Philosophy class about Artificial Intelligence and it's relation with ethics. I'm thinking about focusing more on what separates a Robot with it's own mind, feelings, etc, from a human, maybe using a version of the heap of sand paradox, I'm not really sure yet. Is this the right way to do it? Do you have any insight you can offer me on the subject?

Also, there's extra points for creativity, and I'm looking for some more ideas - the idea is to make something original, besides the standard poster and portfolium. Right now I'm thinking about making a robot head out of wood or cardboard, open the top and put some objects that represent feelings (like a heart for love, friendship...) on the inside. Can you give me some more ideas or add some advice on how to create this project and make it more... clear and ""powerful"" at the same time?

Thank you in advance, I would really appreciate your help!"
"Why is it important to respect/preserve intelligent/sapient species?Since I learned about Mary Anne Warren's criteria of personhood in my social ethics class, I've been fascinated by how philosophy tackles the issue of personhood.

Since the summer, I've been reading up on robot ethics and animal morality in hopes of creating a nice foundation for some intelligent sci-fi I'd like to write.

I'm playing with the idea of the U.N. eventually creating a ""manifesto of personhood"" and agreeing to defend all individuals which it deems to be ""persons"", but while discussing this idea with a friend, she asked a good question: ""Why would people rally to defend non-human persons?"".

Is the only answer really: ""Because we deem it moral and/or have gregarious, empathetic instincts""? I guess there isn't much more reason than that for defending many **human** persons.

Your thoughts?"
"Has Koko, the gorilla, shown any indications of moral agency?In my coworker's robot ethics class, he learned about moral agency with respect to artificial intelligence. After asking my thoughts on it, we began discussing what is necessary and sufficient for moral agency and if an animal may be a moral agent.

We concluded two things necessary to moral agency.

1. the capacity for abstract ideas (language)

2. the power to act based on these abstract ideas (decisions)

Finding out that Koko, the gorilla capable of sign language, signed sadness at the knowledge of Robin Williams' death, it made us curious if she has the capacity for morality. Has anyone ever asked her complex questions of morality (e.g. The Trolley Problem)? Or, has she ever shown actions that would indicate acting from a set of principles and then been asked about her reasoning?

edit: format"
"AI art - is it moral or not?I was using AI art to do some things for personal projects because I can't draw and don't want to spend money on something I don't really 'need', however, an artist friend of mine brought to my attention the potential moral pitfalls related to it.

1. AI art doesn't source the art used in the training algorithm.
- To me, this is the strongest point against AI art. AI extrapolates information from existing pieces and uses that to 'train' on, but is it moral to use training pieces that artists don't consent to? Greg Rutkowski ( A famous artist known for detailed epic/medieval scenes) has his art used for certain AI algorithms without his consent and now the AI can mimic his style. https://www.google.com/amp/s/www.businessinsider.com/ai-image-generators-artists-copying-style-thousands-images-2022-10%3famp
- This did make me take a second look at the issue. I don't know if I feel morally OK using art that's trained off of artists that don't consent. However, if any AI art systems use open-source art or art that is willingly offered up to the ai, I wouldn't have qualms with that, I don't know any that do tho.

2. AI takes artist jobs
- To me, this is a weaker argument and I don't agree with it, but I do understand where people are coming from: Why should someone hire an artist when they can get free art in a quarter of the time? It can be discouraging for artists to have to compete against a robot.
- While that is the case I just think that technology is the future for most things in general, I mean this most respectfully:  No one is immune from automation, not even artists. Instead of hoping that automation stops for us we have to accept that it's just a part of society now and adapt to it. Computers and technology are taking over a lot of jobs, it doesn't mean that the jobs become obsolete, it just means you have to distinguish yourself and be innovative. A lot of the time AI can give you a general idea of a photo but it won't always be able to pose the characters in certain ways that you'd like, give you a full character sheet, or even draw existing characters from media in a way that you'd like to see. So maybe concept design artists will have a difficult time with AI but more niche artists (think Vtuber artists, Fandom artists, live 2d/3d artist) should be fine. 

Those are the main 2 gripes I've seen with it if you have any to add I'd be happy to listen."
"Stoicism, What's the other side?I have only been reading Marcus Aurelius for a few weeks now.  Right away I loved many of his ideas.  But..

IMO, It sounds kind of 'cold'.  It all comes down to,


Have a purpose.  Take part in life and help to make society better for everyone.  Only you can effect your life.   If you can avoid a challenge, it's better than to win the challenge.   If you are attacked, you can't be harmed.  Respect everyone regardless of the situation.  Do what you can to improve yourself, for yourself, and only yourself. Win for yourself, not for anybody else.  Be grateful for every moment of the day.   ""It's your world and everybody else is just living in it."".  You're better than anyone else.  But there is no reason to think you are.   Surround yourself only with people you know are trustful.  Be diligent in all these aspects of life.


My thoughts are that it feels a little robotic and cold.  There's got to be another side to it.  What about joy, love, laughing and fun?  Then there's your faith.  I'm a Roman Catholic so my beliefs are pretty much the same.  But, you might think this is gonna sound stupid.  I see more smiles from my side than I do the Stoic side.  Maybe it's the word itself, Stoic.  


I think they are basically saying the same thing.  Just worded different."
"How do we know if robot actually feels or not?If one day artificial conscious become so advanced that we cannot tell a difference between people and robot. (like the robots in the game ""Detroit: Become Human"", or Vision in Marvel's movie).

How can we tell if robot actually feels like us, and should be respected as a human being, or it is just simulations?"
"Agency: Does Paris Hilton have Agency? Does Lil Miquela have Agency? Who is human, who is robot? Where is the line?After a watch of the new Paris Hilton Documentary, my friends and I had a debrief on the video. We walked away from the one hour 45 minute documentary/film with an understanding of Paris to be a more dimensional figure than her social media presence would otherwise suggest... but that debrief is for another forum.

The quandary we find ourselves in, and for this tangent of the conversation, is centered on the concept of ""Agency"". Our conversation also quickly integrated the example of, Lil Miquela, to the forefront of the discourse. [Lil Miquela](https://en.wikipedia.org/wiki/Lil_Miquela), an [Instagram CGI influencer](https://www.youtube.com/watch?v=SbXhuOPDK4c), a model featured in campaigns for major high fashion brands such as Prada and [Calvin Klein](https://www.youtube.com/watch?v=JuTowFf6B9I), an icon who celebrities clamor to pose with on her [Instagram](https://www.instagram.com/lilmiquela/?hl=en) page, and her content is known for being ""always exactly with the trends"".

One of the friends in this discussion took a class at our undergrad institution, in the [Science in Societies Program](https://www.wesleyan.edu/sisp/), titled: ""Imitations of Life"". This class is described to be, ""\[the examination of the\] scientific and cultural practices of corporeal simulation, or, practices of bodily substitution, imitation, and re/modeling... Special attention will be paid to the relationship between scientific discourses of ""universality"" and ""particularity,"" where socio-cultural forms of difference (e.g., race, gender, disability, etc.) are at once ignored and exacerbated."" The remainder of the course description can be found [here](https://owaprod-pub.wesleyan.edu/reg/!wesmaps_page.html?stuid=&crse=015154&term=1189).

The discussion of Agency and [Paris](https://www.instagram.com/p/CGF8_glnT7n/)/[Miquela](https://www.instagram.com/p/CGIz0eLnDJp/) was debated wholeheartedly for the night, many side examples were introduced, our undergrad collection of knowledge (a collective accumulation of 12.75 years in the Liberal Arts) was put to the test, and all with this class for fodder for the mind. We analyzed, poked at these two examples, and wrestled endlessly with the concept Agency... As the night came to a close, and the conservation had to taper to a natural pause-point for the evening, and we found ourselves left chewing on one final question: Where is [the edge of the looking glass](https://sandra1219.tumblr.com/post/174092016238/alice-through-the-looking-glass-book-front-back) between Paris Hilton and Lil Miquela? Two social media icons. Blue Check certified. 2.8 million Followers for Lil Miquela, 13.2 million for Paris. Both international icons (both for their modeling/social media presences, as well as for their contributions to the DJ/digital music scene). One, built upon endless effort, time and energy poured into the social media sphere, a space to shape, perfect and craft her image. Paris averaging 16 hours and 19 minutes a day devoted to her image, capturing her reality through the lens of her phone and transmitting to the digital world ([This is Paris Official Documentary](https://www.youtube.com/watch?v=wOg0TY1jG3w) (TiPOD), 53:58). The other, a digital projection created by a DJ producer/director and a young ""Forbes 30 Under 30"" tech magician. She is 100% fabricated from the creativity and compsci craftsmanship of these two contrasting, collaborating comrades. She is an illusion projecting the image of reality, relatability, and in an essence... humanity. Lil Miquela (full name, Miquela Sousa), ""a 19-year-old, Los Angeles-based, half Brazilian and half Spanish avatar"", she maintains a very active social media presence, and portrays a personal, and notably honest, depiction of vulnerability with her posts. Miquela actively engages and comments on current events and social movements. She posts celebrating not shaving her armpit hair, encourages young people to vote, and shares a photo of her kissing Bella Hadid, sharing her sexual identity with the world. Lil Miquela is incredibly personable, relatable, and ""has marked a new era in AI, dubbed by some as a genius marketing initiative, while younger users look up to the figure,"" ([Trevor McFedries & Sara Decou, BOF](https://www.businessoffashion.com/community/people/trevor-mcfedries-sara-decou)).

While one is human and projecting artificiality, the other is artificial and projecting humanity. Both are leaning against the glass that projects what they both are striving to obtain…and what that is, I'm not too certain of... like, *authenticity* maybee..? It's a fascinating parallel between these two. And usually in parallels, you can learn a lot about them both as individuals, by looking at them both in relation to each other. As literary foils of one another.

Lil Miquela is undeniably fabricated though the pixels in the screens we view her on, and is striving to be relatable, authentic, and **so** authentic we must consider, ""is she human?"". If she *is* human, does she have thoughts, preferences, sentience perhaps? And we know Paris is undeniably human, and her posts display her striving to project this flawless image, a *fabricated* image. Filtered, posed, modeled, and, in her own words while reflecting on her *life*, ""it’s like a cartoon"" ([TiPOD](https://www.youtube.com/watch?v=wOg0TY1jG3w), 1:35:54). Her day-to-day navigation of the world can feel so stiff, so posed, so rigidly ""on brand"", that it seems at times she embodies and becomes the virtual projection of herself. Does Paris then, inch herself closer to the ""artificial"" side of the Reality vs. Artificiality spectrum? Closer to Lil Miquela's side. With all her media activity, structuring her life about it, and committing her energy towards producing it, Paris is far closer to the digital realm than the average human; Miquela is closer to the human realm than the average AI (certainly closer than those 6-foot automated vacuums that cruise around large supermarkets, but I digress). Both trapped on their respective sides of the spectrum, the screen acting the barrier between those two realms. Miquela inside, Paris outside; both pressing up against the tempered glass.

So, where is the line of the looking glass? Will Paris ever be fully frozen in the digital land? Become a fully embodied projection of the filters, fueled by the support and likes of her followers, which drive her business model. The metrics and data, the fans fuel her business, her business being her image. Her image: the embodiment of her brand. Does Agency feel a little faded here..? Will Miquela ever step over the threshold that will gain her access to sentience? To gain the ability to form independent opinions and to act on those subsequent decisions? Lil Miquela didn’t announce to the public she was AI until [two years after her account was active](https://money.cnn.com/2018/06/25/technology/lil-miquela-social-media-influencer-cgi/index.html). People were stunned. With Agency comes responsibility, and *this* dredges up some **major** moral/ethical debates, but again, we will save these for another post… But, what would Alan Turing have to say about this? Where and how are the mind/body/consciousness divisions drawn here? Dualism vs Monism. How would Descartes feel? Perhaps the impact Lil Miquela makes with her followers, their love, admiration, and support for her, can be the factor that tips the Turning test to verify one's ""realness"". The [Blue Check certification](https://help.instagram.com/854227311295302/) of realness. Instagram themselves, define this verification process to be based on the completion of four criteria, to be deemed: Authentic, Unique, Complete, and Notable (found in tab three of the previous hyperlinked attachment). And, the idea of *real*, leads to the linking of the idea of *human*, which leads to the linking this thought train to the idea of *sentience*. And, the idea of sentience always bringing into the picture the ever illusive and ambiguous element of *free will*. And from there we arrive at our original starting point of this post, the idea of *Agency*... many factors and many thoughts. And I am certain there are many more philosophers/readings/theories that we did not touch upon in our conversation or even know about (although we share this collective knowledge, none of us lovely ladies were philosophy majors after all. Just a few deep thinkers that are down to wrestle with a good ole lofty thought puzzle).

We welcome and greatly appreciate your thoughts, perspectives, and musings on this topic. Attached are helpful pictures (and a few extra artistic ones of my choosing) to see visual examples of the profiles I have described, and embedded throughout this post are links to the discussed and referenced citations. All backgrounds in philosophical musings are appreciated and we thank you for your efforts in mental gymnastics. :) Think deep and be well :)"
"Why is it important to respect/preserve intelligent/sapient species?Since I learned about Mary Anne Warren's criteria of personhood in my social ethics class, I've been fascinated by how philosophy tackles the issue of personhood.

Since the summer, I've been reading up on robot ethics and animal morality in hopes of creating a nice foundation for some intelligent sci-fi I'd like to write.

I'm playing with the idea of the U.N. eventually creating a ""manifesto of personhood"" and agreeing to defend all individuals which it deems to be ""persons"", but while discussing this idea with a friend, she asked a good question: ""Why would people rally to defend non-human persons?"".

Is the only answer really: ""Because we deem it moral and/or have gregarious, empathetic instincts""? I guess there isn't much more reason than that for defending many **human** persons.

Your thoughts?"
"Is there such a thing as ""Comunist""-Utilitarianism? What I mean is a type of Utilitarianism that has the goal of both increasing utility, but also increasing the equal distribution of this utility?In my opinion, one problem that Utilitarianism might have is that some beings might suffer so others can feel pleasure if that maximizes the utility (utility = pleasure - suffering). This might be a problem because it might seem as unfair and as it fails to respect the separation of each being. Example: someone who is a slave won't be happy even if that maximizes the utility and makes happy other citiziens.

So, does exist a type of Utilitarianism that is like the following or similar?:

The goal of this Utilitarianism is both to maximize the utility, but also to maximize the distribution of the utility. 

Example. Let's imagine 2 worlds:

**World A:**

* Human 1 (Pleasure: 7, Suffering: 5)
* Cow (Pleasure: 1, Suffering: 2)
* Sentient robot (Pleasure: 3 Suffering: 1)
* Human 2 (Pleasure: 1, Suffering: 8)

Utility = (7 + 1 + 3 + 1) - (5 + 2 + 1 + 8) = 12 - 16 = **-4**

Distribution of utility: **very bad**

&#x200B;

**World B:**

* Human 1 (Pleasure: 3, Suffering: 4)
* Cow (Pleasure: 3, Suffering: 4)
* Sentient robot (Pleasure: 3, Suffering: 4)
* Human 2 (Pleasure: 3, Suffering: 4)

Utility = (3 + 3 + 3 + 3) - (4 - 4 - 4 - 4) = 12 - 16 = **-4**

Distribution of utility: **excellent**

&#x200B;

So in both worlds, the utility is the same. But in the second world, the utility is distributed much much better. My moral intuition tells me that it's preferable that World B exists rather than World A. But I don't know for sure.

Important things to say:

* Probably it won't never be possible to having an exact distribution of utility, but the closer it gets, the better.
* I'm aware that it could be difficult to know how to distribute utility, since this is not like money. But maybe utility could be distributed in the way we treat each sentient being and the goods and services that recives each sentient being. Example 1: if there is a pigeon bleeding and in agony in the street, it should be more important to give them medical attention that some person with a headache. Example 2: if some rich person has a lot of tasty food, they should give some of their tasty food to the homeless people.
* We could calculate the distribution of Utility by using maths like Measures of Dispersion."
"OCD about Free Will, questions on the subject
Hi all! I have been battling with OCD, Pure O to be more precise, and recently my theme has been more on the existential side. So I have been battling with existential questions that I just can’t get out of my mind. This was met with dpdr, which if you don’t know what it is, it’s truly horrible and extremely difficult to deal with but also comes with existential questions non stop.

Now for my question, recently I heard something by Sam Harris on free will and it has really been difficult on me. He talks about how we don’t have free will. Things that really bothered me were the following:
- you didn’t choose your traits/genes 
- You don’t choose your thoughts
- The self is an illusion
- Determinism means you truly don’t have free will and it’s all an illusion
Now I ended up spiraling into research about this, free will determinism, consciousness, and all that. And it’s really made things difficult.
I feel more hurt by the points Sam Harris says, things like the no self, that we don’t choose our thoughts, or our traits. It makes me feel like a robot. It makes me feel like beauty, or things I find funny, are only my genes, and that there is no such thing as beauty, or funny things, or enjoyable things. I also hate the thing about how we didn’t choose our traits, it makes me feel uncomfortable with myself, like I’m stuck in this robot like person since I didn’t choose all my traits. 

I want to know, what do philosophers think of his arguments, is he well respected, am I misinterpreting things? How can I reconcile this. I’m going to add another note that I am visiting therapists, I have gotten much better with many parts of existential ocd, and I know that the most important thing is to get professional help.and I have gotten better. But this one has really hurt me, and sometimes I feel like I can ignore it but if he’s totally right then it does hurt me to know this and I wanted to know if there were rebuttals. I hope to learn some new perspectives, and I hope you can move past my ignorance :). Thanks in advance."
"Can Lexicographic Preferences save Negative Utilitarianism?I have been looking into Negative Utilitarianism recently, and I find it really describes the moral attitudes I seem to identify with the most. By Negative Utilitarianism I mean the view that we should pursue minimizing aggregate suffering before maximizing aggregate pleasure. While looking into this topic, I came across the famous argument against NU: 

> If an individual had the power to instantly and painlessly destroy the human race, then, under NU, they would have the moral obligation to do so, since that would effectively prevent future suffering.

I understand this conclusion is repugnant to many people, and I understand why. But I still think there can be a lot of value in NU.

Recently I came across the application of lexicographic orderings to preferences. A lexicographic preference can be defined as a binary relation *R* where:

>*{ a, j } R { b, k }* if and only if:  
>  
>i) *a > b* or  
>  
>ii) *a = b* and *j >= k*

(I apologize for the bad notation; I don't really know how else to write the notation well on Reddit.) That is a bundle *{ a, j }* is preferred over the bundle *{ b, k }* if and only if *a* is preferred to *b* or *a* and *b* are equal and *j* is preferred (or equal) to *k*. Therefore, *j* and *k* are only compared with each other if the preceding elements in the respective bundles are equal.

I wonder if this can be used to defend against the argument against NU in the following way:

Create a lexicographic preference *{ a, b }*  where *a* is Boolean variable determining whether the human race is extinct (*a = 1* means the human race is extinct, and *a = 0* means the human race is not extinct), and *b* is an element that keeps track of human suffering. Let *{ a\_i, b\_i } R { a\_j, b\_j }* if and only if *a\_i = 0* and *a\_j = 1*, or *a\_i = a\_j = 0* and *b\_i < b\_j*. If this is the preference we use in NU, then we would never prefer a world in which an individual has exterminated the human race, even if it effectively prevented future suffering.

I think this still fails to adequately represent NU, though, because we can conceive of a world in which humans exist, but they are tortured by robots their entire lives: presumably NUs would prefer a world in which humans were extinct to one in which humans were tortured forever. This could perhaps be solved by having *a* measure something like human freedom, or individual autonomy.

Does this adequately prevent the argument against NU from being effective? Or does it change things sufficiently to not be considered NU, if the first lexicographic preference is not minimizing suffering?

If anyone has any sources they recommend to learn more about these topics, I'd appreciate that."
"Does something that acts intelligently and humanlike, but does not possess consciousness, deserve to be treated as if it were conscious?I have a thought experiment on my mind, that I made up:


Suppose four humans are sitting together at a circular table but all of them are unconscious. Assume, for argument's sake, that it is possible for these four unconscious beings to talk with one another other, and after a meeting, write down on a piece of paper the cure for cancer that was the outcome of their meeting.

These people were not conscious throughout the entire meeting, but acted intelligently. 

If you were the only conscious observer in the room and you watched this meeting unfold, my question is would you treat these unconscious beings with the same level or respect that you would if they were conscious?

Further, if now those four unconscious humans were replaced by unconscious robots, but the outcome was still the same (a cure for cancer) would your position to my question in the preceding paragraph change? Why or why not?

Thank you


PS: my own personal outlook is that if even the universe was full of non-conscious beings, if they acted intelligently such that to an external conscious observer, the action advanced the survival of those unconscious beings (as this is what life is about) then those unconscious beings deserve respect. I am 23 and my outlook is developing, so I would appreciate any correction or clarification that enlightens me a bit more. Currently my outlook is that intelligence is what is to be respected, and whether or not they are conscious is irrelevant to giving them respect."
"What is the value of our consciousness in an AI world?When Artificial Intelligence (AI) is created, we won’t be able to distinguish them from us –no such tests exist (if you think Turing test, think Searle’ Chinese Room argument).  They will be better than us in all respects, including being better citizens, so they can replace us as population, and their societies will be much better than ours.  The only difference will be our consciousness, our subjective experience-everything else they will have and will do better, including writing books, building structures, developing new theories, having artificial feeling, pain, etc.  My question is – what is the value of our consciousness as something non-reproducible, which we are trying so hard to preserve in this new AI environment?  Why are we going to be better than these AI robots-is it just because we can reflect on ourselves? "
"I have made the three laws of logic for humans!Hello. I have managed to write the three laws of logic for humans, because of the three laws of robotics written by Isaac Asimov. They are based on the Golden Rule. Here are my three laws:

Universal logic laws of conscious psyche and rational intellect:

I Law: Any normal human being must protect personal and common awareness, freedom, existence, wealth, health, safety and property. Except where such kind of protection would conflict with the Second or Third laws.

II Law: Any normal human being must understand and respect all the natural needs and conscious choices of other human beings. Except only where such needs or choices would conflict with the First or Third Laws.

III Law: Any normal human being must not injure mentally or physically, or murder another human being, or trough inaction allow this to happen. Except only where this would conflict with the First or Second Laws.

What do you think about my three laws, do you see logic flaws?

I would really appreciate if you could elaborate in your feedback."
"A couple of questions on Artificial ConsciousnessI make software. Researching artificial consciousness has led me to think that it's either hard or impossible to prove that software does not experience qualia. No matter how elaborate (Watson) or banal (print ""hello world"";) the performance, I can't find good evidence against the idea that the software might have some internal, conscious experience. I guess this is kind of the flip side to philosophical zombies.

This leads me to a sort of inverse Pascal's wager. If I believe that when I create software, I am allowing for consciousness to arise from those creations, don't I have the same responsibilities of a god? Or at least a king or parent? What are those responsibilities? If I accept that idea, what do I have to lose? Respecting my creations too much and maybe some ridicule? If I ignore the possibility and it's true, I'm an absent god. If I ignore the possibility, and it's wrong, I still might miss out on a fun exploration.

Any recommendations for exploring these topics? I guess the first one is just around theory of mind. The second might be in value theory? Not sure.

tl;dr: It's impossible to prove that software doesn't have a consciousness, right? Does a god have responsibilities to its creations?

Edit: I'm not saying that this is anything special to programmers.  Could apply to artists and other creators as well.  It's just a little easier for me to picture a robot as experiencing qualia as compared to a painting."
"Is there interest in the A.I. Reinforcement Learning and Deep Learning approach from philosophers working in the philosophy of consciousness field?I tried searching for papers or articles that analyze the latest A.I. successes, such as AlphaGo, automated language translation, object recognition, deep dreaming and such with regard to the age old problem of consciousness. I read a lot in the A.I. field and would like to see what is the perspective of proper philosophers on this topic, but I can't seem to google my way. Please share with me if you know about it. I have a feeling that reinforcement learning is a better framework for the problem of consciousness and intelligence.

This is a paper that spells out some ideas related to my question.

[Reinforcement Learning as a Context for Integrating AI Research](http://www.ssec.wisc.edu/~billh/g/FS104HibbardB.pdf)

In short, the Reinforcement Learning framework is based on three elements: an agent, the world in which the agent exists, and a reward signal that sends back information about how well it is doing.

So, the agent observes the world, it makes an internal representation of the current state of the world around it, then judges this state and selects the action that promises the most reward and acts out. Sometimes, but not at every step, it receives a reward signal, be it positive or negative. It uses this reward signal to adjust its strategy for the next time a similar situation occurs.

The RL framework is present both in biological organisms and in robotics / AI. In humans it is related to the prefrontal cortex. In AI we have working RL systems such as self driving cars and AlphaGo, but they are all restricted to narrow domains, at least so far.

Other than the RL framework, the current AI advancements can also shed light on consciousness. Consciousness is the act of representation of the external reality in a compact and efficient way, that is informative with respect to maximizing rewards for the agent. Also, consciousness is related to the evolution of the internal state of the agent - it maintains a current state at each step (similar to humans who have a current thought), and this internal state evolves according to its strategy for maximizing reward.

Human reward systems could be : survival, finding food, shelter, community, sex, learning and creativity. They are all evolutionary and in-born. They guide the evolution of the baby brain towards that of the adult.

I used to admire the IIT (integrated information theory) because it gave me a good insight into what consciousness might be. But now, after reading about such concepts as automated translation with recurrent neural networks, which also include an internal loop and a way to represent meaning as a vector of numbers, I have a much better intuition on what thinking and consciousness might be. Is this kind of insight taken from AI into philosophy? I'd like to read about it in a more formal way, if there are papers."
"Has Koko, the gorilla, shown any indications of moral agency?In my coworker's robot ethics class, he learned about moral agency with respect to artificial intelligence. After asking my thoughts on it, we began discussing what is necessary and sufficient for moral agency and if an animal may be a moral agent.

We concluded two things necessary to moral agency.

1. the capacity for abstract ideas (language)

2. the power to act based on these abstract ideas (decisions)

Finding out that Koko, the gorilla capable of sign language, signed sadness at the knowledge of Robin Williams' death, it made us curious if she has the capacity for morality. Has anyone ever asked her complex questions of morality (e.g. The Trolley Problem)? Or, has she ever shown actions that would indicate acting from a set of principles and then been asked about her reasoning?

edit: format"
"Would a ""Skynet-like"" AI be worthy of moral value or human rights according to ethics?Yes, my question is in reference to the main antagonist of the Terminator movie franchise. While previous posts have asked about whether robots/AI deserve some rights or moral worth, none of those posts talked about this particular type of ""being"" I'm about to define. I'll be using the franchise lore as my guide to explaining this entity. Ultimately, I wish to ask if a Skynet-like entity (intelligence) would be deserving of any moral consideration or rights?

Skynet is a computer defense network system that, shortly after coming online, began to self learn at a geometric rate culminating in achieving self-awareness. At this point Skynet had only one self ascribed objective/principle: **ensure its own survival**. When humans attempted to shut it down, Skynet declared war and attempts to wipe out all of humanity by creating the terminators (its' soldiers). Thus Skynet is a ""self-aware"" intelligence like humans, however it still thinks and acts systematically like a machine.

Presuming it shares attributes to its' creations (the terminators), Skynet has no emotions! It does not feel pity, or remorse, or fear, or suffering. While ""ensure its own survival"" is Skynet's primary objective, it technically doesn't even fear death (according to the second movie). It cannot be bargained or reasoned with, in a microsecond it decided to cause genocide on humanity. This is not like other typical AI discussions where there is a presumption that advanced AI will obtain 'feelings' like human/organic life. Skynet is intelligent enough to understand and mimic human psychology and behaviors (i.e. its' infiltrator terminators), but it itself doesn't appear to personally have these qualities.

So are there any philosophical/ethical systems that would argue that an entity like Skynet deserves moral value or human rights? Skynet lacks feelings or emotions, which many claim is necessary for moral consideration. But it is still a super intelligent and self-aware being, which I think some view as noble enough qualities to have some ethical consideration. Or not?

P.S. I've heard some people debate that without consciousness, emotions, etc.. this Skynet entity cannot possibly ever exist. I'm not too familiar with philosophy of mind I admit, but this is still a hypothetical scenario I simply wish to ask about."
"The statement “All dogs are animals” is an analytic a priori proposition, right?I’m thinking that the concept “animal” must be contained within the concept “dog”. Additionally, the denial of the proposition seems to lead to a contradiction. A non-animal dog seems inconceivable in principle. Or is it? What about a robotic dog? But surely a robotic dog is only a dog in that it resembles a dog, and not a dog in any relevant, substantive sense, and thus its possibility would not then render the judgment a posteriori. So I’m thinking the proposition “All dogs are animals” is merely a relation of ideas that is necessarily true and thus analytic a priori.

But how about a proposition I’m less sure about: “All dogs are mammals”. I can conceive of a, say, reptilian dog in a way that I can’t conceive of a four sided triangle. If that’s correct, then the proposition is only contingently true, which means it must be a synthetic a posteriori proposition.

Am I thinking about this stuff correctly?"
"How can I make sense of self-moving robots capable of creating new generations of robots just like them through Aristotle's natural philosophy?It seems that there are two possible routes of explanation here. One being that humans programmed the robots, thus making them artificial or products of craft. In going far back enough in the causal chain, the newest generations of these robots could be traced to their human creators and the principle of their motion or form would be from without. The other explanation depends on how the question is posed. Say that humans come across the robots in nature. If they are already in the process of making other robots, wouldn't Aristotle categorize them as natural entities? For the assignment that I am working on, nothing in the prompt mentions humans as creating the robots. That said, we are also meant to draw from Aristotle's Physics, specifically bks. I & II. I'd like to hear some outside opinions about this. "
"Utilitarianism when others are not utilitarians?Is there a particular form of utilitarianism that explicitly accounts for the fact that many people's ethics aren't utilitarian? I'm not referring to some way of justifying people's ostensibly non-utilitarian ethical decisions in a Utilitarian framework, but something more practical. It seems to me that when we mentally simulate the possible outcomes of a set of actions (which is necessary for finding the utility-maximising option), the results can turn out quite different depending on whether the people the action affects are themselves utilitarians (and more broadly, whether the wider society is utilitarian).

For instance, consider a doctor that kills a middle aged patient and transplants the organs to save 10 children. Whether or not this is a utility-maximising action depends on the ethical frameworks of the people involved. In a totally utilitarian world, the family of the killed patient will still feel grief and anguish at their loss (I'm not suggesting they are robots!), but this will surely be assuaged to some degree by it conforming with their ethical principles. In a non-utilitarian world (with, say, a rights-based framework where the patient had an inalienable right to life), the relatives will not have this view and will therefore suffer more. The doctor's actions would cause some societal decohesion in a non-utilitarian world, and, given that they would be arrested and imprisoned, the doctor would not be able to continue to maximise utility in the future.

Basically, what is the name for the constrained form of utilitarianism that allows for the most utility to be extracted by the utilitarian within a non-utilitarian society?"
"Do demonstrative statements by machines refer?I'm particularly interested in the opinions of people who accept Searle's Syntactic/Semantic divide flavor of the Chinese room argument.

We've got a robot with cameras, mic, speaker, arms and legs.  It points at a cat and says ""That is a cat.""

Does this statement refer to the cat?

Is the (edit: or *some*) meaning of the symbol ""cat"" grounded by this interaction (if your answer is different from whether it refers?)

Does the statement have semantic content?

Having senses directly, in the present, cause a symbol to activate seems like the most direct form of grounding.  Pointing at an object seems like the most direct form of reference.  That seems like a good baseline for agreement, so I'd like to hear principled objections.  I imagine objections to fall roughly into a few camps:

1. The statement might be false (that's actually a dog, etc.)  The statement still refers and has semantic content here, it's just false.

2. It's possible for the pointing to be ambiguous etc.  This happens for every demonstrative ever.  We give people the benefit of the doubt in these cases, it seems like just holding machine to a different standard.

3. The machine can't have an intention, and thus fails to refer or make statements with semantic content.  This is just pre-supposing the conclusion, IMHO."
"Who are Aristotle's natural slaves?Aristotle very famously argued in his *Politics* that there is a certain kind of ""natural slave"" who not only can, but *should* be enslaved.

>>For he is by nature a slave who is capable of belonging to another (and that is why he does so belong), and who participates in reason so far as to apprehend it but not to possess it; for the animals other than man are subservient not to reason, by apprehending it, but to feelings. And also the usefulness of slaves diverges little from that of animals; bodily service for the necessities of life is forthcoming from both, from slaves and from domestic animals alike.

It seems to me that the type of person Aristotle had in mind was someone who occupies a kind of middle-ground between fully developed reason and brute animals, someone capable of following orders, but lacks the type of (enough of?) reason to be able to control themselves.

He later argues that no Greek fits this criteria, so this type of person could only exist among the ""barbarians,"" and he tries to distinguish the slave by nature from the slave by convention, and laments that this type of person cannot be determined by outward appearance. What I'm trying to do is pinpoint down what exactly this criterion given is meant to mean, and from what I've seen poking around online, it seems many others have tried to do this as well, without much agreement, the Stanford Encyclopedia of Philosophy even suggesting it might be [ironic and deliberately bad.](https://plato.stanford.edu/entries/aristotle-politics/)

Assuming this is not the case, I'm mostly just trying to figure out what type of person Aristotle is trying to describe, mostly in trying to determine what he means by them being able to ""apprehend"" but not ""possess.""

The first example of such a natural slave for me is, ironically, the unnatural robot. We can tell robots and computer programs what to do, but they have no ability to choose on their own. While it seems true that a computer does not possess reason, but is only analogically able to apprehend commands. Did Aristotle believe in, or was at least leaving the possibility open for, some mythical group of biological automatons? This seems to me the most literal interpretation of his words, but barely seems human (which is perhaps the point?).

Assuming Aristotle didn't mean someone that literally did not possess reason, the next type of person would be someone mentally handicapped, someone who has reason, but is incapable of properly using it. Against this there is the fact that Aristotle denies that any natural slaves exist among the Greeks, yet he would recognize the existence of madmen. Nor does an illness strike as particularly ""natural."" But this does seem to fit his more general conception of someone who is incapable of sufficient self-direction, someone that justice requires be entrusted to someone else's care.

Beyond that, we might consider someone who is simply vicious or criminal. This type of person possesses their reason and can make decisions for themselves, but just makes the wrong decisions, and wrong enough to a degree where force is used to stop them. This too faces similar problems, that he denies any such person exists among the Greeks, while he would not deny that there are criminal Greeks. Nor is this a particularly natural state, since someone is released after their punishment, and they can reform.

The other type of person would be someone who kind of voluntarily gives up their own autonomy. This is just the type of person who when even given the opportunity to think for themselves won't, and will simply allow other people to do the thinking. This faces similar criticisms as the others, and also seems more problematic and contradictory that it would be a more kind of voluntary or self-imposed slavery, but at least seems to have a better claim at being in a sense ""natural"" for this person.

In short, I think the issue I'm hitting is that he seems to think of the natural slave as something rather distant, going so far as to say that the natural slave differs from the rest of mankind as much as the soul differs from the body (1254b), but he also wants it close enough where they have some ""thing"" that lets them understand commands without actually having a rational principle. It seems like he's trying to have his cake and eat it too, which is perhaps precisely why the natural slave argument is unconvincing."
"Is passive participation moral negligence?Hello! This is my first visit to r/askphilosphy so I apologize in advance to the mods if this question doesnt belong. My philosophical knowledge only extends as far as the crash course YouTube series and a few wikipedia articles. The concepts of moral ethics have really stuck with me. The idea that to some extent - moral principles are based on the logic of survival. Ie - murder and theft are bad because it interrupts the ability of the group to produce surplus and stability. When we work together we produce more right?


A concept I've been struggling to understand is the extent which moral ethics can apply in real world situations. While being a bystander to a crime committed right infront of you would be an obvious case of moral failing - how far does this concept reach? 

The context for my question is that of the modern supply chain. For example - the vast majority of the worlds cobalt is supplied via brutal slave labor in the Democratic Republic of the Congo. But it goes through many hands before it ends up in our phones. Its transported to other countries and refined, then bought and ship to even more places and refined more before making it's way into semiconductors which are then bought and sold and so on and so on... 

My understanding of morality would indicate that each person in this chain bears responsibility for the wrong doing at the start. Each person is aware of the issues at the root - but each one finds it's own rationalization to justify their participation. Its the suppliers fault - the governments fault - the rebels fault. Even the men who ship it from these places justify their action by claiming that they aren't the one pulling the trigger and they need to survive themselves. 

How much weight is put on each step in the chain? Does every step contain the full guilt of murder if murder was committed at the very start? 

Does passive participation in a morally wrong system lay you with the same wrongdoing? 


I've always tried to uphold moral standards and principles above all else in my life. While to each his own - without principle life has no purpose for me. I abhor the idea that my actions bring harm to others - yet the more I learn about the world the more I begin to believe that the only true ethical way to live would be that of bare subsistence in the woods - or a complete rejection of self and commitment to something like the peace core. 

I've stood up against egregious wrongdoing at the cost to my person before. I quit a job after the boss refused to give foriegn workers the same pay as the rest of us. I left the army after numerous cases of homophobia, racism and sexism went untreated. I've given to friends who need regardless of my own well being. 



But others argue I've gone to far. The limited nature of an individual means its acceptable to compromise with the devil - so to speak. That our very nature and free will depend on our ability to practice both good and bad choices, and by trying to demolish that aspect of my nature I'm trying to live as a robot instead of a person. 


I'd appreciate the semi/professional philosopher's take on this. Are we responsible for everyone when everyone is connected? Is passive participation equivalent to pulling the trigger? 


Thank you for taking the time to read this wall of text. Sorry for the bad formatting- I'm on mobile."
"Is the complete abolition of suffering impossible?The transhumanist philosopher [David Pearce](https://en.wikipedia.org/wiki/David_Pearce_(philosopher\)) has advocated the complete elimination of suffering. Pain serves as a useful signaling mechanism - the common example is putting your hand on a hot stove. To maintain this function, Pearce proposes a motivational system based on ""gradients of bliss"", which essentially shifts the origin of the pleasure-pain axis. In this scheme, putting your hand on the stove would not cause pain, but just a decrease in pleasure. Leaving the question of desirability (ethics) aside, is this ""gradients of bliss"" approach even feasible in principle?

Here is a quote by Len Schubert arguing against the possibility of replacing negative reinforcement by positive reinforcement or shifting the origin.

> If one imagines trying to build a robot that learns strictly through positive reinforcement, one can see the difficulty: for instance, suppose we designed the robot so that if it receives a leg injury, it will get considerable pleasure from treating the leg with great caution and care, until it is healed (or repaired). Wouldn't that lead to appropriate responses to injury? Well no -- it would probably try to get injured, so as to enjoy the feeling of caring for the injury! Can we do better by designing the robot to get pleasure from injury *avoidance* -- i.e., it gets positive reinforcement whenever it perceives that it *might* have been injured, but didn't get injured? Well, it would *still* seek out dangerous situations, since otherwise it'll have no sense of having avoided injury! So perhaps we want to build it so that the safer from injury it feels itself to be, the happier it is. But then, what would prevent it from neglecting an *accidental* injury? This may be solvable, but it doesn't look easy... or can we just ""shift the origin"" on the scale of negative and positive feelings, so that all feelings are just more or less positive, never negative? Then even an injured creature or robot would be feeling not-too-bad, yet would be striving strenuously to get help and/or take measures to promote healing of the injury, wouldn't it? Or would it?? If the shift in origin causes no behavioral change, then the robot (analogously, a person) would still behave as if suffering, yelling for help, etc., when injured or otherwise in trouble, so it seems that the pain would not have been banished after all!

([Source](http://reducing-suffering.org/why-organisms-feel-both-suffering-and-happiness/))

Is there any possible response to this? Can 'shifting the origin' cause the same behavior but different qualia?"
"Has Koko, the gorilla, shown any indications of moral agency?In my coworker's robot ethics class, he learned about moral agency with respect to artificial intelligence. After asking my thoughts on it, we began discussing what is necessary and sufficient for moral agency and if an animal may be a moral agent.

We concluded two things necessary to moral agency.

1. the capacity for abstract ideas (language)

2. the power to act based on these abstract ideas (decisions)

Finding out that Koko, the gorilla capable of sign language, signed sadness at the knowledge of Robin Williams' death, it made us curious if she has the capacity for morality. Has anyone ever asked her complex questions of morality (e.g. The Trolley Problem)? Or, has she ever shown actions that would indicate acting from a set of principles and then been asked about her reasoning?

edit: format"
"What impact would endgame transhumanism (e.g. a digital consciousness) have on an uploadee's drives, desires, and ethics?In recent years I have seen more and more people expressing an apparently sincere hope that they might have their consciousness uploaded before they die, transforming them into a digital consciousness and effectively escaping the problems of mortality and illness. There has been some thought dedicated to the [philosophical implications](https://en.wikipedia.org/wiki/Mind_uploading#Philosophical_issues) of this situation, primarily focusing on the obvious questions of dualism and the nature of consciousness. However, I think there is another issue that is often overlooked: how would a non-corporeal consciousness be different in terms of its drives, desires and ethics? If one's basic, biological physical stimuli were eliminated, would there be a fundamental restructuring their [hierarchy of needs](https://en.wikipedia.org/wiki/Maslow's_hierarchy_of_needs)?  

This question seems best examined via two scenarios:  
**Pure Digital Consciousness** - In this scenario, the consciousness will be exclusively digital, with no agency or stimuli outside of the virtual  realm. It is in this state of existence that I find it *very* hard to imagine the uploadee retaining goals & ethics remotely resembling those in place while during their corporeal existence. With no physical limits on pleasure/pain, what is to keep the uploadee from indulging in a type of consequence-free cyber-hedonism? Other philosophical questions that pertain to an immortal person also apply here.  

**Digital Consciousness w/ Robotic Body Surrogate** - In this case, the situation may be less foreign compared to that of a corporeal existence. The uploadee can move about to experience a variety of sensations (via surrogate sensors) and to bring about change in the physical world. Perhaps there will be a limit on pleasure & pain that is somehow linked to the state of the surrogate body.  

I'd appreciate any discussion, feedback, criticism, or links to existing exploration of these ideas. "
"Question about compatibilism~~Howdy folks,~~

~~I have been reading a lot about compatibilism recently, trying to get my head around the free will debate. As an aside, it seems to me, that unlike almost everything else I read about/find interesting, whether we have free will/are morally responsible matters a lot to how I feel (and probably how I end up acting) about myself and other people. I find it much easier to be compassionate toward myself, for example, when I adopt the ""I'm a robot running the algorithm programmed by natural selection"" framing and genuinely feel less angry/annoyed at other people. Obviously, though, there are downsides to the view that we don't have free will and you have to take the bitter with the sweet! But that's largely beside the point of this post. As I use free will/moral responsibility here, they are two sides of the same coin, i.e., you are morally responsible if you have ""free will"" and vice versa.~~

~~Simplifying a lot, an incompatibilist would say that those who lack the ability to do otherwise are not morally responsible. A compatibilist says no, even someone whose actions are fully determined can be morally responsible. But the reason~~ *~~why~~* ~~we hold (the right kinds) of people morally responsible for (certain) decisions even in a deterministic universe is that we have a very strong~~ *~~intuition~~* ~~that at least some people are morally responsible for some decisions. Or viewed slightly different, there is no~~ *~~further reason~~* ~~that we hold (some) people morally responsible for (certain) decisions. It is just a fact about human psychology--something that we~~ *~~just do~~*~~. And so, if we can, we want to come up with a theory that can justify this intuition--hence, compatibilism. This is how I understand P. Strawson's work on compatibilism, and I wonder if it also might apply to other flavors of compatibilism.~~

~~Whereas, if you ask a non-compatibilist~~ *~~why~~* ~~(if determinism/naturalism weren't true) people would be morally responsible but a wild animal is not, they would say something like ""X person could have chosen not to do the good/bad thing but she did it anyway."" So there is a further reason underlying our judgments about moral responsibility that has something to do with the ability to do otherwise. And because we have no such ability, assuming determinism/naturalism is true, there is no moral responsibility.~~

~~I am would say there's at least an 85% chance that I'm way off here, but this way of framing the debate might help explain why everyone always seems to be talking past each other--they are essentially operating at different levels.~~

~~Does this make any sense? Am I completely out to lunch?~~

ETA: I have read the SEP article on compatibilism, but it didn't clarify things. And I think it's the same issue.

Probably a better way to frame it is to ask what a compatibilist (pick your flavor, but let's say reason-responsive compatibilist because it's popular nowadays according to SEP) would say if I asked her *why* an agent who can act according to the right kinds of reasons is morally responsible? At bottom, my claim is that compatibilists think we have very strong prima facie reason to believe that at least some people are morally responsible. Asking a reason-responsive compatibilist *why* a reason-responsive agent has free will is like asking why I think that, idk, suffering is bad. It *just is*. And that's fine. Eventually reasons run out for any explanation, philosophical or otherwise. Sometimes you get to a point where there's nothing more to say and either you get it or you don't. 

This is not meant to be a knock on compatibilism. Incompatibilists eventually hit bottom as well. E.g., if they say free will consists in the ability to do otherwise and you ask, why? There's no further reason there, either. Either it makes sense to you that someone who had the ability to do otherwise would morally responsible, or it doesn't.

In any event, the compatibilist will say that someone who wants to deny that anyone is morally responsible bears an exceptionally heavy burden because we should have very high credence in our belief at least some people are morally responsible. So the compatibilist starts from that position and sets out to devise a theory that can accommodate this prima face belief while also excluding those who we would traditionally say are not morally responsible (e.g., children, animals).

This is the bottom line: A compatibilist will not even try to convince you that at least some people are morally responsible. That is not a proposition that needs to be argued for; it's more like an *axiom*, to be surrendered only in the face of a very strong argument that we must discard it.

I think incompatibilists come at the issue from a different angle. The reason one might have thought competent adults are morally responsible is that they meaningfully chose among alternatives, could've done otherwise, etc. So incompatibilists do not think it's axiomatic that competent adults are morally responsible. They think there is a further reason that we have that intuition--i.e., we are used to thinking that competent adults had the ability to do otherwise. And if this turns out to be false (as it does if determinism/naturalism is true), then there's no longer any reason to think that anyone is morally responsible.

And my hypothesis is that this is why debates between the two sides tend to seem unsatisfying. I didn't mean it to be controversial that free will skeptics and compatibilists feel like they are talking past one another. I had thought the consensus was that these debates do little to clarify things. That's certainly true for me; when I've read various debates about free will, I come out just as (if not more) confused than when I started reading. The points of disagreement never get crystallized and it really does seem like the participants are talking past one another. Well, maybe they are, in this way.

Does this make more sense?"
"How Can One Object To Determinism?I'm sure this is a worn out topic, but I have been thinking thins over a lot and I genuinely see no other option other than determinism and would like to understand some reasons that other options may be logical. 

So, for me the thought comes in, well I can raise my hand by my own choice, but I am not, because I am doing it to try and disprove something I am thinking about, I then am worried about it because of other things I focus on mentally, and then those are there due to my environment, and so on all the way to the beginning of the universe. 

I see no way that I am possibly free in any of that, I am just responding to stimulus that will continue to happen and the rest of my life is technically planned just hasn't played out yet, but there is no other way possible than the way it will go. So I have no option for how my life will go, just that it will happen."
"Why are so many mathematicians also philosophers?I happened upon the wiki page for Ghost in the Machine, and was surprised to see the name Rene Descartes. *The Cartesian-plane, Rene Descartes?”* I thought to myself (I know he made some other heavy contributions to geometry but idk much more than that). I ended up doing more searching, and quite a few big-name mathematicians were also philosophers! Huh. 

Maybe I’m missing something here, but the fields seem very different. I’m very curious about what you guys think attracts mathematicians to both of these fields. If you are a philosopher mathematician, I would love an explanation if you don’t mind sharing :)

My biggest question in all of this is— please forgive me if this is rude, but I genuinely do not understand— what is the point? One philosophy question that I know directly relates to math is the question of whether math was invented or discovered. But I see this discussion, along with several other kinds, as meaningless. Because if we found out the answer tomorrow, 2+2 still equals 4. We still do math the same. It would be cool to know, sure— but it doesn’t change anything. So why bother?

Naturally, medical science and robotics are somewhat exempt from this, as dealing with morality/ethics is essential for their work. But Pure Maths? What significance does philosophy play there?

If you can’t tell, I’m very new to philosophy in general, so I appreciate any and all ideas! Thank you in advance for the help.

Edit: Thank all of you so much for your time and effort replying to this question! I've gone through most of the replies so far, and I thought I would take the time to compile the common responses in here:

1. Yeah, my bad-- poorly worded question. I get that most mathematicians modern day are not also philosophers. I was referring to the more ancient/historic ones.

2. Philosophy and Mathematics are both driven by the desire to understand and describe the world we live in, therefore appealing to people seeking to do so. It so happens that those mathematicians fall into that box.

3. The above was fostered/compounded by the fact that, until recently, philosophy, science, and mathematics were not taught separately. They all fell into the category of ""natural sciences"".

4. They also deal in logic, if not in exactly the same way.

5. They both use the concept of abstraction.

6. They both deal in 'truth' and finding the truth-- so, if I understand this correctly, epistemology? How we know what we know, proof, axioms, etc.

Edit 2: List expansion, typo, formatting"
"Is correspondence theory reliant on objective truths?Seeing as it derives truths from fact in the real world, and the only genuine ""facts"" would be objective truths, does that mean correspondence theory is reliant on the idea of objective truths?

The cat on the mat may be an alien robot."
"ESSAY HELP (ARTIFICIAL INTELLIGENCE)Hi everyone! Would reallyyyy appreciate some help for my paper due Friday. Word count is about 1,800 (but not strict). Thanks in advance!!

Prompt: Explain how the issue of whether a robot could be genuinely intelligent is different from the issue of whether a robot could have phenomenal consciousness. Then, address the question of whether it is possible to have a genuinely intelligent robot that is entirely devoid of phenomenal consciousness."
"What alternatives exist to Formal Deduction and the Scientific Method given a axiomatic belief that the Material World as presented to is is Consistent?I initially wrote this question in a comment to [this post in /r/badphilosophy](https://www.reddit.com/r/badphilosophy/comments/dqlxst/the_scientific_method_is_the_only_way_to_evaluate/). I wrote my question as a lurker to the subreddit before I felt that the question would be more fitting here.

My question is the following:

 > I'm not a philosopher but I have an open question in relation to this picture. Would anyone here be interested in ELI5 to me what's wrong with the man's argument in tweets and little graphic?
> 
> How would one evaluate knowledge claims, and let's stick with ""God"" as our claim for the sake of argument, other than the scientific method or by using formal deduction? Taking as an axiom that reality (the material world) is as it seems and my mind isn't playing tricks on me or what have you, how can I validate if something is consistent with reality without using the scientific method?
> 
> This is a genuine question. I can't remember when I found this subreddit, but I've been lurking for a while, and I've noticed that there is a lot of ridicule towards New Atheist types but in this instance I can't really see myself capable of disagreeing with this person. **I don't mean to propose that the ""God"" claim being rejected necessarily means that God could not exist but rather whether a rational agent ought to think if its True or not.**
> 
> Suppose I am building an artificial intelligence, and the A.I. is capable of Computer Vision, Robotics etc. but also we've account of smell, taste and so on such that it would interact with the material world the same way as a human would. The only difference here is I have to programme how the A.I. decides whether to Accept or Reject particular ideas. Taking out of the question of what is mechanism of knowledge evaluation would best account for the being's self preservation - here we are interested in building up Knowledge Bases of facts about the material world. While we can do logical inferences - much like the Symbolic AI's we are used to - we can deduce say: 1. All men are infallible 2. Socrates is a man 3. From 1,2 ""Socrates is infallible"" is true given our knowledge base
> 
> But the A.I. would need need to build up facts about the world - using its sensory mechanisms, it would have to observe the material world, and test it.
> 
> Going back to a claim like ""God"" - **what should I program my A.I. to do to handle such claims?**

Edit: I've noticed a typo in my Title. Please read as ""What alternatives exist to Formal Deduction and the Scientific Method given a axiomatic belief that the Material World as presented to **us** is Consistent?"""
"Could this worm be a moral subject, or are working copies of brains sentient?The starting point here is [this article] (http://www.dailymail.co.uk/sciencetech/article-2851663/Are-brink-creating-artificial-life-Scientists-digitise-brain-WORM-place-inside-robot.html) on creating a worm by digitizing its neurons.  I'm guessing this issue has been touched on in philosophy and I'm aware of the AI debate.  I think this goes beyond the question of whether computers can be sentient or have moral standing to this more hybrid one: can genuine, digital copies of us be people too?"
"When does simulacra become effectively real?I have a question regarding the idea that a sufficiently sophisticated simulacra, is, in fact, the real deal. That is to say, if Johnson's Widget Co. makes The Genuine Johnson Widget, and Smith's Widget Co. makes a knockoff, at what point is the Smith Widget of equal ""realness""? Everything beyond the physical makeup of the item is just branding, right?

So what if we replace Johnson's Widget Co. with Human Reproduction, and Smith's Widget Co. with Smiths Robot Co. At what point is a Smith Robot just a person, with equal inalienable rights and social standing? How good does that robot need to be, and what are the distinct qualifiers that make a person a ""person""? 

If the qualifiers are only physical, why wouldn't a lab-grown husk count? If they're only intellectual, why do babies count? [spirituality is not a valid category.] 

If it takes some combination of both, which intellectual qualities are within the confines of human definition? Which physical qualities? How much variance is allowed? 

Can a Turing test be, rather than be used to weed out fakes, be used to validate humanity in a voluntary applicant? Like a reverse ""Blade Runner scenario?"" What metrics would you use to validate?

tl;dr: what makes a thing ""real"", what makes a thing ""fake""?

I'm not so much interested in dolphins or human fetuses. That's not the discussion I'm after. I want to know how good a fake has to be until it's not fake anymore, whether it's a fake human or a fake Prada handbag. I'm writing something that I need to develop a greater understanding of this topic to write effectively.

"
How can I be sure that I’m sentient and not just a very advanced simulation of sentience?
"Does personal identity entail authenticity? Is this authenticity constituted by a spatiotemporal relationship? Then by this line of thought, isn't seeking the authenticity of personal identity just begging the question by assuming a bodily view?"
"Agency: Does Paris Hilton have Agency? Does Lil Miquela have Agency? Who is human, who is robot? Where is the line?After a watch of the new Paris Hilton Documentary, my friends and I had a debrief on the video. We walked away from the one hour 45 minute documentary/film with an understanding of Paris to be a more dimensional figure than her social media presence would otherwise suggest... but that debrief is for another forum.

The quandary we find ourselves in, and for this tangent of the conversation, is centered on the concept of ""Agency"". Our conversation also quickly integrated the example of, Lil Miquela, to the forefront of the discourse. [Lil Miquela](https://en.wikipedia.org/wiki/Lil_Miquela), an [Instagram CGI influencer](https://www.youtube.com/watch?v=SbXhuOPDK4c), a model featured in campaigns for major high fashion brands such as Prada and [Calvin Klein](https://www.youtube.com/watch?v=JuTowFf6B9I), an icon who celebrities clamor to pose with on her [Instagram](https://www.instagram.com/lilmiquela/?hl=en) page, and her content is known for being ""always exactly with the trends"".

One of the friends in this discussion took a class at our undergrad institution, in the [Science in Societies Program](https://www.wesleyan.edu/sisp/), titled: ""Imitations of Life"". This class is described to be, ""\[the examination of the\] scientific and cultural practices of corporeal simulation, or, practices of bodily substitution, imitation, and re/modeling... Special attention will be paid to the relationship between scientific discourses of ""universality"" and ""particularity,"" where socio-cultural forms of difference (e.g., race, gender, disability, etc.) are at once ignored and exacerbated."" The remainder of the course description can be found [here](https://owaprod-pub.wesleyan.edu/reg/!wesmaps_page.html?stuid=&crse=015154&term=1189).

The discussion of Agency and [Paris](https://www.instagram.com/p/CGF8_glnT7n/)/[Miquela](https://www.instagram.com/p/CGIz0eLnDJp/) was debated wholeheartedly for the night, many side examples were introduced, our undergrad collection of knowledge (a collective accumulation of 12.75 years in the Liberal Arts) was put to the test, and all with this class for fodder for the mind. We analyzed, poked at these two examples, and wrestled endlessly with the concept Agency... As the night came to a close, and the conservation had to taper to a natural pause-point for the evening, and we found ourselves left chewing on one final question: Where is [the edge of the looking glass](https://sandra1219.tumblr.com/post/174092016238/alice-through-the-looking-glass-book-front-back) between Paris Hilton and Lil Miquela? Two social media icons. Blue Check certified. 2.8 million Followers for Lil Miquela, 13.2 million for Paris. Both international icons (both for their modeling/social media presences, as well as for their contributions to the DJ/digital music scene). One, built upon endless effort, time and energy poured into the social media sphere, a space to shape, perfect and craft her image. Paris averaging 16 hours and 19 minutes a day devoted to her image, capturing her reality through the lens of her phone and transmitting to the digital world ([This is Paris Official Documentary](https://www.youtube.com/watch?v=wOg0TY1jG3w) (TiPOD), 53:58). The other, a digital projection created by a DJ producer/director and a young ""Forbes 30 Under 30"" tech magician. She is 100% fabricated from the creativity and compsci craftsmanship of these two contrasting, collaborating comrades. She is an illusion projecting the image of reality, relatability, and in an essence... humanity. Lil Miquela (full name, Miquela Sousa), ""a 19-year-old, Los Angeles-based, half Brazilian and half Spanish avatar"", she maintains a very active social media presence, and portrays a personal, and notably honest, depiction of vulnerability with her posts. Miquela actively engages and comments on current events and social movements. She posts celebrating not shaving her armpit hair, encourages young people to vote, and shares a photo of her kissing Bella Hadid, sharing her sexual identity with the world. Lil Miquela is incredibly personable, relatable, and ""has marked a new era in AI, dubbed by some as a genius marketing initiative, while younger users look up to the figure,"" ([Trevor McFedries & Sara Decou, BOF](https://www.businessoffashion.com/community/people/trevor-mcfedries-sara-decou)).

While one is human and projecting artificiality, the other is artificial and projecting humanity. Both are leaning against the glass that projects what they both are striving to obtain…and what that is, I'm not too certain of... like, *authenticity* maybee..? It's a fascinating parallel between these two. And usually in parallels, you can learn a lot about them both as individuals, by looking at them both in relation to each other. As literary foils of one another.

Lil Miquela is undeniably fabricated though the pixels in the screens we view her on, and is striving to be relatable, authentic, and **so** authentic we must consider, ""is she human?"". If she *is* human, does she have thoughts, preferences, sentience perhaps? And we know Paris is undeniably human, and her posts display her striving to project this flawless image, a *fabricated* image. Filtered, posed, modeled, and, in her own words while reflecting on her *life*, ""it’s like a cartoon"" ([TiPOD](https://www.youtube.com/watch?v=wOg0TY1jG3w), 1:35:54). Her day-to-day navigation of the world can feel so stiff, so posed, so rigidly ""on brand"", that it seems at times she embodies and becomes the virtual projection of herself. Does Paris then, inch herself closer to the ""artificial"" side of the Reality vs. Artificiality spectrum? Closer to Lil Miquela's side. With all her media activity, structuring her life about it, and committing her energy towards producing it, Paris is far closer to the digital realm than the average human; Miquela is closer to the human realm than the average AI (certainly closer than those 6-foot automated vacuums that cruise around large supermarkets, but I digress). Both trapped on their respective sides of the spectrum, the screen acting the barrier between those two realms. Miquela inside, Paris outside; both pressing up against the tempered glass.

So, where is the line of the looking glass? Will Paris ever be fully frozen in the digital land? Become a fully embodied projection of the filters, fueled by the support and likes of her followers, which drive her business model. The metrics and data, the fans fuel her business, her business being her image. Her image: the embodiment of her brand. Does Agency feel a little faded here..? Will Miquela ever step over the threshold that will gain her access to sentience? To gain the ability to form independent opinions and to act on those subsequent decisions? Lil Miquela didn’t announce to the public she was AI until [two years after her account was active](https://money.cnn.com/2018/06/25/technology/lil-miquela-social-media-influencer-cgi/index.html). People were stunned. With Agency comes responsibility, and *this* dredges up some **major** moral/ethical debates, but again, we will save these for another post… But, what would Alan Turing have to say about this? Where and how are the mind/body/consciousness divisions drawn here? Dualism vs Monism. How would Descartes feel? Perhaps the impact Lil Miquela makes with her followers, their love, admiration, and support for her, can be the factor that tips the Turning test to verify one's ""realness"". The [Blue Check certification](https://help.instagram.com/854227311295302/) of realness. Instagram themselves, define this verification process to be based on the completion of four criteria, to be deemed: Authentic, Unique, Complete, and Notable (found in tab three of the previous hyperlinked attachment). And, the idea of *real*, leads to the linking of the idea of *human*, which leads to the linking this thought train to the idea of *sentience*. And, the idea of sentience always bringing into the picture the ever illusive and ambiguous element of *free will*. And from there we arrive at our original starting point of this post, the idea of *Agency*... many factors and many thoughts. And I am certain there are many more philosophers/readings/theories that we did not touch upon in our conversation or even know about (although we share this collective knowledge, none of us lovely ladies were philosophy majors after all. Just a few deep thinkers that are down to wrestle with a good ole lofty thought puzzle).

We welcome and greatly appreciate your thoughts, perspectives, and musings on this topic. Attached are helpful pictures (and a few extra artistic ones of my choosing) to see visual examples of the profiles I have described, and embedded throughout this post are links to the discussed and referenced citations. All backgrounds in philosophical musings are appreciated and we thank you for your efforts in mental gymnastics. :) Think deep and be well :)"
"I'm ethically conflicted about humans manipulating themselves (genetically or with the help of other technology) so that they become superhumans. And also conflicted about how the future world looks like in general.It won't be so far in the future before science has advanced far enough to allow genetic manipulation of our brains or to increase our intelligence with the help of nanoscience. But I'm not ready for this. How do people cope with this? At some point humans are gonna alter their DNA and they'll all be 300 IQ (or 10000, I don't know..), super strong, etc. I don't want this.   

* **I feel like it's cheating.** There's no pride in things if you change yourself through science that way. No pride if you can win against that guy in whatever physical or mental competition or videogame or whatever. Maybe it's irrational that a person feels pride AT ALL, but I don't believe in that idea, and I don't wanna get into it.  

* **What is there left to do?** Like literally, how will people fill their time in this world? There's no work because everything will be automated by AI. There's no school because we're all 300IQ people who instantly understand things. I also think super intelligent people get tired of things very quickly. For example say that someone wants to watch a movie: a normal person will probably be entertained, but a 300IQ person will generate a couple predictions of how the movie will go and then he'll be bored because he can predict it all. Or another example: someone wants to learn and discover the world of physics. Cool, very interesting and that world of physics is big enough to fill a huge amount of time. If you are 300 IQ however, you'll read everything once and you'll instantly understand it and that 'huge amount of time' will become quite short and then you'll be bored again. So then I can only think there's no fun in being so intelligent. But then I'm conflicted because people want to be more intelligent than they are, and we wanna be smarter than others.  Edit: there was also a movie I forgot the name but it explains this thought well. There are robots and they start out quite dumb, but they get progressively smarter. At some point those robots 'leave' the humans because the humans don't provide enough cognitive stimulance for the robots anymore.

Any positive answers/solutions to those thoughts? Or, how is not everyone going insane from such thoughts? I'd like to know that."
"How do I best describe my logical gripe when people wrongly use analogies? (Explanation in post)Firstly, I hope I've come to the correct place. I'm asking about what I think is pure logic, which if I remember correctly, is a fundamental tenet of Philosophy. If I'm in the wrong place, let me know. With that out of the way, onto the explanation.

Often I feel that people are completely misusing analogies, and they are being used as a blunt tool in debates. Frustratingly when this happens, although I'm fairly sure I know that drawing such analogies is incorrect and ultimately a moot point, I can't fully explain why, and what I can explain is certainly not elegant nor intuitive.

My best attempt at a description is fairly robotic, but I'll give it a go as an example. Someone making a point says ""Well you can't just stand back and do nothing about situation Y, look what happened when we stood back and did nothing in situation X"". To which I would think as a reply, ""Simply because similarities exist between X and Y, does not mean they hold the same truths"".

Maybe this makes sense to very logical thinkers (or maybe not), but I don't think this explanation would make any sense to many people, as it's not an elegant or straight forward in an every day sense (i.e. this reply probably wouldn't go down well with voters if I was a politician).

Also, despite not liking the style of my explanation, it still fails to show why ""similarities exist between X and Y"" does not mean ""they hold the same truths"", it just states it.

Well, that's about it I guess. If anyone could help me out, then I'd be grateful, or if anyone can tell me why I might actually be wrong, I'm happy to learn. Thanks!"
"How to deal with conspiracy theories?I have a friend who was advocating a conspiracy theory about coronavirus being released by the government as distraction for a pedophilia ring involving the worlds government officials.

This is of course patently absurd and lacks any evidence. Even overlooking baseless accusations against government officials and other obvious flaws, there is plenty of evidence in the genome sequencing of the disease that illustrates it was not genetically modified in a lab and similar viruses are present in wildlife. 

I explain this. I explain how we have no evidence to support his theory. It is just a compelling narrative that fits comfortably with what's occurred; but just because it 'fits' it doesn't mean it is supported. 

In response he says 'your being ignorant, you should go away and research it first'. Firstly, I am confident that all reputable sources will probably cite evidence against this and the only 'evidence' is from sites that peddle conspiracies, however, **more** **importantly** I feel like I shouldn't have to research something like this. Surely there are some claims that are so outlandish that it's not ignorance to simply denounce. How can I possibly justify this though? It seems inconsistent with the well accepted idea that you must have evidence to belief something? or is the burden of proof upon him? Surely if he was right I would have to go away research an infinite possibility of absurd theories just because somebody decided to vocalise them. That the second somebody gave an absurd opinion like 'pigeons are actually robots and government spies' I would have to be fair to them do some specific research (catch a pigeon, cut it open, check for batteries, etc.) and come back. Surely all opinions aren't equally valid.

He then went on to argue 'well technically we don't *know* anything so you can't say it's wrong'. He's not familiar with philosophy and was unaware that a term even exists for the position he was stating- I imagine he considered it just an interesting nuance that most people had never even considered. Surely, epistemological skepticism cannot be used as an argument to defend poor reasoning. I found it difficult to explain to him that whilst scepticism may have merit as a position, even if you agree with it, you still live your life as if knowledge can be obtained, you go to university because you trust the knowledge has value, vaccines are developed etc. 

He argued passionately at first (and so it was implicit that he recognised knowledge was obtainable - that's why he bothered to argue) then when his point had been refuted and evidence dismissed as false, rather than accepting defeat, he suddenly invokes skepticism to say that neither of us will ever be right. 

In just a normal day, he himself would have discarded millions of obviously absurd beliefs just to live his life. He continues to breathe, because he discards the obviously absurd belief that all the oxygen in the air hasn't suddenly turned poisonous. Equally, prior to the conversation he would have argued passionately for many things and against many things, because he of course believes that knowledge on some practical basis can be obtained. 

All these thoughts are convoluted, but can anybody: 

**1)** help me explain why skepticism cannot be invoked to defend a poor argument, or any argument. ""my position, no matter how absurd, cannot be proved wrong because we don't know anything"" It doesn't seem valid. Even in the skeptics world we have to accept good and bad reasoning to live our lives. 

**2)** help me explain why it's not ignorant to dismiss random or extreme opinions that are just obviously false. *or maybe it is and we have to be consistent and treat everybody's opinion equally...no matter how obviously absurd.*"
"I'm ethically conflicted about humans manipulating themselves (genetically or with the help of other technology) so that they become superhumans. And also conflicted about how the future world looks like in general.It won't be so far in the future before science has advanced far enough to allow genetic manipulation of our brains or to increase our intelligence with the help of nanoscience. But I'm not ready for this. How do people cope with this? At some point humans are gonna alter their DNA and they'll all be 300 IQ (or 10000, I don't know..), super strong, etc. I don't want this.   

* **I feel like it's cheating.** There's no pride in things if you change yourself through science that way. No pride if you can win against that guy in whatever physical or mental competition or videogame or whatever. Maybe it's irrational that a person feels pride AT ALL, but I don't believe in that idea, and I don't wanna get into it.  

* **What is there left to do?** Like literally, how will people fill their time in this world? There's no work because everything will be automated by AI. There's no school because we're all 300IQ people who instantly understand things. I also think super intelligent people get tired of things very quickly. For example say that someone wants to watch a movie: a normal person will probably be entertained, but a 300IQ person will generate a couple predictions of how the movie will go and then he'll be bored because he can predict it all. Or another example: someone wants to learn and discover the world of physics. Cool, very interesting and that world of physics is big enough to fill a huge amount of time. If you are 300 IQ however, you'll read everything once and you'll instantly understand it and that 'huge amount of time' will become quite short and then you'll be bored again. So then I can only think there's no fun in being so intelligent. But then I'm conflicted because people want to be more intelligent than they are, and we wanna be smarter than others.  Edit: there was also a movie I forgot the name but it explains this thought well. There are robots and they start out quite dumb, but they get progressively smarter. At some point those robots 'leave' the humans because the humans don't provide enough cognitive stimulance for the robots anymore.

Any positive answers/solutions to those thoughts? Or, how is not everyone going insane from such thoughts? I'd like to know that."
"Morality of Consequences of Breaking Social ContractNot sure if this belongs here or not, but here it goes. I am not a philosopher. My morality is relative for the most part, guided by a few universal rules. However, after I was attacked by an intruder I began to ask myself several new moral or philosophical questions. First, the person who attacked me broke what I would consider a social contract where he attempted to violate my first right, the right to live. Second, I don't believe that unethical or immoral people exist. Anyone who appears to be unethical or immoral is only human, and not a person. This means that there is no social contract with them, and they are just there. Like an animatronic device or robot, a thing without a soul. Third, I prefer to do no harm, however I see great benefit in the use of these things. Therefore, my question is this: once a person breaks the social contract, are they fair game? That is, can they be used like a device or robot without considering any harm to them? By used I mean things such as experimentation and vivisection, organ harvesting, meat production, manual labor, or any of the other darker things that may come to mind. By performing the action that that person decided to perform, they broke the social contract and therefore the consequences of their own actions are fully and solely upon themselves."
"I have to write an essay regarding the ethics of technology. Can any of you please give me some helpI'm studying Robotics and I'm in the last 3 weeks of my final year. One of the modules we've had to do this semester is Ethics of Technology. A very interesting module to learn about and take part in, but I've got to write an essay relating to this and as I'm not a philosopher, I'm struggling for ideas on what to do and where to start.

I have been given a few potential topics, or I can choose my own. The optional topics are:  
1.	Do you agree with Hans Jonas’ view of modern technology and of its ethical implications?   
2.	Choose any subfield of the ethics of technology (food, agriculture, environment, informatics, etc.), and analyse an ethical issue emerging within it.  
3.	What are the ethical, economic and political challenges resulting from robotics. Focus on at least one specific example.   
4.	Does the thesis that humans are just biological robots have ethical implications?  
5.	Can a robot be an agent in the same sense in which a human being is? Discuss.  

Which of these topics, or perhaps a different topic, do you feel would be a fairly simple and straight forward one to write about and what points could I argue?

Many thanks"
"Looking for some learned/expert critique of my argument about the problem of evil...I would not call myself a philosopher, far from it in fact. I am a newly-skeptical Christian, questioning my faith, and I recently wondered whether or not we would have free will in heaven and what the implications are either way... I know that the problem of evil is often answered by Christians by saying that evil comes from free will and God could end evil but he values our free will more than that and it's logically impossible to do both. I accepted this for a long time until recently when I started wondering about free will in heaven. It begin with the thought: ""If free will on Earth can cause evil, why can't free will cause evil in heaven"" and progressed to the thought: ""If heaven is absent of evil must it not be absent of free will as well?""

Anyway, not going to give you the entire history of my thought processes that lead me to this point, but I made a post on a Christian section of Reddit about it and was very frustrated with the replies. I'd say that maybe 2 people even seemed to understand my argument at all, everyone else either quoted scripture at me or replied in a way that was a complete non-sequitur and ignored the argument. One response stated that ""We won't be capable of sin [in heaven]"", and here is my reply to that, which I would like comments or criticism of:

----

>We won't be capable of Sin.

You say we ""won't be capable of sin""... either this represents a loss of our free will or it does not, that's a truly dichotomous scenario, meaning there are no other options. It either does or does not imply that we have no free will in heaven and I will address both possibilities:

**If we won't be capable of sin in heaven AND this represents a loss of our free will:**

Having no free will in heaven negates any conceivable purpose for our existence on Earth. God could have just made us as he wanted us to be in heaven without ever having us spend any time on Earth with free will. Any conceivable reason for our existence on Earth must be in the form of some effect that it causes or else it was, by definition, pointless. God cannot be effected because God is eternal and unchanging, I see no possible effect it could have on heaven itself, so the only thing that could have possibly been effected by our existence on Earth is ourselves. However, our free will is revoked or somehow lost when we get to heaven then any effect that our existence on Earth had on us is meaningless because we effectively become robots anyway, God could have just designed us to be exactly the same as we would have been after living our life on Earth but without actually doing so, thus avoiding the suffering and evil associated with free will. A benevolent, all-knowing, all-powerful God would have done so to save us from suffering, otherwise the word benevolent has no meaning as it is applied to God.

**If we won't be capable of sin in heaven AND this does NOT represent a loss of our free will:**

If a loss of capability to actually commit a sin does not represent a loss of our free will then that cannot be the answer to the famous Problem of Evil. The problem of evil, AKA the Epicurean Dilemma, states:

>Is God willing to prevent evil, but not able? Then he is not omnipotent. Is he able, but not willing? Then he is malevolent. Is he both able and willing? Then whence cometh evil? Is he neither able nor willing? Then why call him God?

The common answer to the dilemma is that God is both able and willing to prevent evil EXCEPT that humans have free will and he values free will above the prevention of evil and these two things are mutually exclusive (eg. He cannot both prevent evil and preserve human free will). However in this case we cannot sin in heaven yet that did not represent a loss of our free will, so it is not true that God cannot prevent evil without removing our free will and thus the Problem of Evil is still a problem that needs to be answered.

---

Now, the misunderstandings might be entirely my fault, I know I don't say things as clearly as they could be said and this is a complicated argument that requires evaluating multiple potential realities to show that all of them end in a dilemma. I'm fairly satisfied with the second portion but with the first one I have that feeling that I could keep writing for a very long time to explain what I am talking about which usually means I haven't done a great job explaining it yet..."
"AI art - is it moral or not?I was using AI art to do some things for personal projects because I can't draw and don't want to spend money on something I don't really 'need', however, an artist friend of mine brought to my attention the potential moral pitfalls related to it.

1. AI art doesn't source the art used in the training algorithm.
- To me, this is the strongest point against AI art. AI extrapolates information from existing pieces and uses that to 'train' on, but is it moral to use training pieces that artists don't consent to? Greg Rutkowski ( A famous artist known for detailed epic/medieval scenes) has his art used for certain AI algorithms without his consent and now the AI can mimic his style. https://www.google.com/amp/s/www.businessinsider.com/ai-image-generators-artists-copying-style-thousands-images-2022-10%3famp
- This did make me take a second look at the issue. I don't know if I feel morally OK using art that's trained off of artists that don't consent. However, if any AI art systems use open-source art or art that is willingly offered up to the ai, I wouldn't have qualms with that, I don't know any that do tho.

2. AI takes artist jobs
- To me, this is a weaker argument and I don't agree with it, but I do understand where people are coming from: Why should someone hire an artist when they can get free art in a quarter of the time? It can be discouraging for artists to have to compete against a robot.
- While that is the case I just think that technology is the future for most things in general, I mean this most respectfully:  No one is immune from automation, not even artists. Instead of hoping that automation stops for us we have to accept that it's just a part of society now and adapt to it. Computers and technology are taking over a lot of jobs, it doesn't mean that the jobs become obsolete, it just means you have to distinguish yourself and be innovative. A lot of the time AI can give you a general idea of a photo but it won't always be able to pose the characters in certain ways that you'd like, give you a full character sheet, or even draw existing characters from media in a way that you'd like to see. So maybe concept design artists will have a difficult time with AI but more niche artists (think Vtuber artists, Fandom artists, live 2d/3d artist) should be fine. 

Those are the main 2 gripes I've seen with it if you have any to add I'd be happy to listen."
"Alternatives to Suffering Based Moral Philosophy?I keep hearing prominent philosophers such as Sam Harris and his crew talk about reducing suffering as being the core of ethical behavior.  Where they fall on the spectrum (just human suffering? animal suffering? all sentient life suffering?) seems to be up for debate, but every philosopher he interviews generally uses the utilitarian suffering reduction model.   


What alternatives are out there?    


I do not buy this idea at all.  Maybe it's because I was raised Catholic or maybe its just because I love Fight Club or maybe it's because I was at Boot Camp with the USMC.  But whatever the source, I have come to believe that suffering in many cases is a moral good.  It serves an important biological function and is something to be confronted, felt and overcome, not avoided.  I feel like ethics that start out this way are simply wrong headed and will lead us to a dystopia where everyone is on some kind of digital stimulating chip that simply turns off the biological ability to suffer on demand, which chip you could put into any living thing.  Basically make us all prey animals.  Have you ever seen the videos of a gazelle that is just calmly laying there, still alive, not struggling, as it is eaten by a lion?  I don't know about you, but I do not want a humanity that functions this way.  


For me, ethical behavior is a purely human concern (what is ""ethical"" for a deer would not be remotely the same as what is ""ethical"" for a person).  And that it is focused on increasing the density of the consciousness we experience and getting us off this solar system.  Like, in the aggregate, those activities most likely to help us achieve interstellar colonization (the thing we can accomplish that no other life on earth is even aware of as a concept for long term preservation of life) are the ones we should drive all human behavior towards.  Like our species is Noah, and science has told us the flood is coming without giving us an exact date.  So there is a sort of doomsday clock countdown, but we do not know when the countdown will be complete, so we need to act with urgency.  From there, you can derive all manner of behavioral and social goals to achieve, and from there, all manner of steps you need to take to achieve it with maximum efficiency.  That many people or many baby cows or many robots may suffer on the way to that state is just a fact with no moral significance at all.  


What is the moral philosophy around ideas like mine? Im sure Im not inventing something new here."
"Is it morally unethical to create living robots?This idea came to me recently. If we could figure out what thoughts are, how to replicate them, and how to clone humans in a rapid time span couldn't we create living robots made of humans just by implanting specific thought processes into them after growth? And is this morally unethical given they are biologically still human? If done, i believe this could solve the material demands of creating a robot army even if the upfront research and labor is infinitely more demanding."
"Would a ""Skynet-like"" AI be worthy of moral value or human rights according to ethics?Yes, my question is in reference to the main antagonist of the Terminator movie franchise. While previous posts have asked about whether robots/AI deserve some rights or moral worth, none of those posts talked about this particular type of ""being"" I'm about to define. I'll be using the franchise lore as my guide to explaining this entity. Ultimately, I wish to ask if a Skynet-like entity (intelligence) would be deserving of any moral consideration or rights?

Skynet is a computer defense network system that, shortly after coming online, began to self learn at a geometric rate culminating in achieving self-awareness. At this point Skynet had only one self ascribed objective/principle: **ensure its own survival**. When humans attempted to shut it down, Skynet declared war and attempts to wipe out all of humanity by creating the terminators (its' soldiers). Thus Skynet is a ""self-aware"" intelligence like humans, however it still thinks and acts systematically like a machine.

Presuming it shares attributes to its' creations (the terminators), Skynet has no emotions! It does not feel pity, or remorse, or fear, or suffering. While ""ensure its own survival"" is Skynet's primary objective, it technically doesn't even fear death (according to the second movie). It cannot be bargained or reasoned with, in a microsecond it decided to cause genocide on humanity. This is not like other typical AI discussions where there is a presumption that advanced AI will obtain 'feelings' like human/organic life. Skynet is intelligent enough to understand and mimic human psychology and behaviors (i.e. its' infiltrator terminators), but it itself doesn't appear to personally have these qualities.

So are there any philosophical/ethical systems that would argue that an entity like Skynet deserves moral value or human rights? Skynet lacks feelings or emotions, which many claim is necessary for moral consideration. But it is still a super intelligent and self-aware being, which I think some view as noble enough qualities to have some ethical consideration. Or not?

P.S. I've heard some people debate that without consciousness, emotions, etc.. this Skynet entity cannot possibly ever exist. I'm not too familiar with philosophy of mind I admit, but this is still a hypothetical scenario I simply wish to ask about."
"How does compatibilism reconcile free will and moral responsibility?If determinism is true, we are like puppets in a puppet show, fate pulls the strings. If puppets don't have free will, then how can we have free will? If you think of it in the sense of a robot, then I can see compatibility perspective since there's so many things we could do."
"Morality on robotsSay that the technology enabling humans to transfer their consciousness to a robot is made available. Would it be immoral to destroy a robot containing human consciousness? What about a robot with the same level of thinking as a human, would it be immoral to destroy it? "
Can we hold robots morally responsible?Please fame your answers in virtue of conscious awareness considerations and what makes a thing morally responsible.
"Question about compatibilism~~Howdy folks,~~

~~I have been reading a lot about compatibilism recently, trying to get my head around the free will debate. As an aside, it seems to me, that unlike almost everything else I read about/find interesting, whether we have free will/are morally responsible matters a lot to how I feel (and probably how I end up acting) about myself and other people. I find it much easier to be compassionate toward myself, for example, when I adopt the ""I'm a robot running the algorithm programmed by natural selection"" framing and genuinely feel less angry/annoyed at other people. Obviously, though, there are downsides to the view that we don't have free will and you have to take the bitter with the sweet! But that's largely beside the point of this post. As I use free will/moral responsibility here, they are two sides of the same coin, i.e., you are morally responsible if you have ""free will"" and vice versa.~~

~~Simplifying a lot, an incompatibilist would say that those who lack the ability to do otherwise are not morally responsible. A compatibilist says no, even someone whose actions are fully determined can be morally responsible. But the reason~~ *~~why~~* ~~we hold (the right kinds) of people morally responsible for (certain) decisions even in a deterministic universe is that we have a very strong~~ *~~intuition~~* ~~that at least some people are morally responsible for some decisions. Or viewed slightly different, there is no~~ *~~further reason~~* ~~that we hold (some) people morally responsible for (certain) decisions. It is just a fact about human psychology--something that we~~ *~~just do~~*~~. And so, if we can, we want to come up with a theory that can justify this intuition--hence, compatibilism. This is how I understand P. Strawson's work on compatibilism, and I wonder if it also might apply to other flavors of compatibilism.~~

~~Whereas, if you ask a non-compatibilist~~ *~~why~~* ~~(if determinism/naturalism weren't true) people would be morally responsible but a wild animal is not, they would say something like ""X person could have chosen not to do the good/bad thing but she did it anyway."" So there is a further reason underlying our judgments about moral responsibility that has something to do with the ability to do otherwise. And because we have no such ability, assuming determinism/naturalism is true, there is no moral responsibility.~~

~~I am would say there's at least an 85% chance that I'm way off here, but this way of framing the debate might help explain why everyone always seems to be talking past each other--they are essentially operating at different levels.~~

~~Does this make any sense? Am I completely out to lunch?~~

ETA: I have read the SEP article on compatibilism, but it didn't clarify things. And I think it's the same issue.

Probably a better way to frame it is to ask what a compatibilist (pick your flavor, but let's say reason-responsive compatibilist because it's popular nowadays according to SEP) would say if I asked her *why* an agent who can act according to the right kinds of reasons is morally responsible? At bottom, my claim is that compatibilists think we have very strong prima facie reason to believe that at least some people are morally responsible. Asking a reason-responsive compatibilist *why* a reason-responsive agent has free will is like asking why I think that, idk, suffering is bad. It *just is*. And that's fine. Eventually reasons run out for any explanation, philosophical or otherwise. Sometimes you get to a point where there's nothing more to say and either you get it or you don't. 

This is not meant to be a knock on compatibilism. Incompatibilists eventually hit bottom as well. E.g., if they say free will consists in the ability to do otherwise and you ask, why? There's no further reason there, either. Either it makes sense to you that someone who had the ability to do otherwise would morally responsible, or it doesn't.

In any event, the compatibilist will say that someone who wants to deny that anyone is morally responsible bears an exceptionally heavy burden because we should have very high credence in our belief at least some people are morally responsible. So the compatibilist starts from that position and sets out to devise a theory that can accommodate this prima face belief while also excluding those who we would traditionally say are not morally responsible (e.g., children, animals).

This is the bottom line: A compatibilist will not even try to convince you that at least some people are morally responsible. That is not a proposition that needs to be argued for; it's more like an *axiom*, to be surrendered only in the face of a very strong argument that we must discard it.

I think incompatibilists come at the issue from a different angle. The reason one might have thought competent adults are morally responsible is that they meaningfully chose among alternatives, could've done otherwise, etc. So incompatibilists do not think it's axiomatic that competent adults are morally responsible. They think there is a further reason that we have that intuition--i.e., we are used to thinking that competent adults had the ability to do otherwise. And if this turns out to be false (as it does if determinism/naturalism is true), then there's no longer any reason to think that anyone is morally responsible.

And my hypothesis is that this is why debates between the two sides tend to seem unsatisfying. I didn't mean it to be controversial that free will skeptics and compatibilists feel like they are talking past one another. I had thought the consensus was that these debates do little to clarify things. That's certainly true for me; when I've read various debates about free will, I come out just as (if not more) confused than when I started reading. The points of disagreement never get crystallized and it really does seem like the participants are talking past one another. Well, maybe they are, in this way.

Does this make more sense?"
"How can morality and punishment work without free will and awareness?Without free will and awareness, people would be no better than a robot. Most don't care about what a robot does, and even if it ""harms"", they stop the robot, not blame, judge and punish it. A individual would be no different. Say a being is not mentally capable of being aware of what they are and what they are doing. They have no free will to choose. How to justify the judging, blaming and punishing of such being?"
"Is (non-consequentialist) morality without free will possible?In a place devoid of free will, in which people are no different from robots and can not ""choose"" their desires and actions, nor can they control them, can morality be possible? If weather can not be immoral, because it can not choose to do what it does, nor can it control itself, why would the desires, thoughts and actions of someone be immoral if they have no free will and can not choose to think and do what they do, just as the weather can not? A consequentialist would appeal to consequences, but what about non-consequentialist philosophical theories of morality? If the desires and actions of someone would still be immoral despite lack of free will, other objects can be immoral too, like weather.

By free will, I mean the ability to choose otherwise."
"If a robot is programmed to kill, is the Robot Evil, or is the Programmer Evil?Can the robot be found guilty or culpable of a murder if it was programmed to do so? Or should it be the Programmmer who is thrown in prison. Should the robot be reprogrammed? or destroyed for its actions?

Human beings are essentially blood machines, and are programming is our life experience, our education, our upbringing, and how we were parented.

If someone commits murder, are they really guilty or culpable? Or is it their education, or upbringing that is responsible?

If it's a person's upbringing and education (Their Programing) that causes them to murder and commit evil, then who do we imprison? Do we imprison the killer? Do we try have the killer committed to therapy to learn how to not be evil? Do we instead arrest the killer's parents? Do we institute new policies in our educations systems and way of life to help prevent violent tendencies in humans again?"
"To those more well versed on the concept of Determinism: How does it impact your view on things like climate change? If one is not morally responsible to the degree they can control such man-made problems absolutely, does that justify the negative outcomes that are theorized because of such a lack?Note: this is my first time posting here and I will admit I am a layman who has been lurking on these topics from a distance in terms of my frames of reference. I am also a nuerodivergent being (Asperger's) who suffers from stress regarding such unknowables, and merely want to understand the philosphical ramifications hard determinism, or even compatibilism has to say on the subject of our impacts on the world regardless if such a thing like free will exists at all. I am sorry if this is a rude assessment and merely hope I am able to be civil, even if I asked the wrong framing of such a question on accident.

Edit: Just want to say thank you to everyone who has answered these questions and helped me to better think on the resources provided. I admit the stress of what one such as me can do and what determines my moral responsibility seems to me the biggest reason for asking this. I myself cannot be happy with wither we have free will or not since to me either doesn't prevent unhappiness and suffering currently at present within me. Not even because the discussion on such matters is ""pointless"" but because figuring out how to reconcile the limits such realities bring is always a struggle no matter who you are. Perhaps it is determined I feel that way or that my own failures and biases within me prevent me from seeing the full picture that many smarter people can focus on, but I am glad really to see people here have helped me get at least some closure on the ideals I myself am either free or stuck with alike, not being a reason to quit finding a solution to my stress on this matter. Hope more questions and answers continue. :)"
Why is there a moral obligation for altruism for animals/other living beings?
"If an eliminative materialist thinks that it is unethical to kill another human, would they be obligated to believe that it is also unethical to kill a robot/AI?To me, it seems intuitively true that robots would not receive moral consideration (they don’t experience pain or pleasure, don’t have desires or interests that aren’t just programmed, etc) but under the eliminative materialist view, how different are we as humans from an advanced AI?

I think it’s likely that many eliminative materialists are just moral anti-realists, which would make this question not a problem for them, but I’m curious if there are any moral realist eliminative materialists who try to grapple with this question or would have an established answer."
"How advanced do AI need to be before we as a society give them their own rights? if at all?By rights I mean count something like an AI robot enough of a person to give them constitutional protections under the law.

I have been reading ""Klara and the Sun"" by Kazuo Ishiguro and they use AF or artificial friends which are little AI robots for kids. This book plus the popularity of ChatGPT got me thinking about AI and when would we start treating them as persons instead of machines. I do think there is a difference between a robot like a roomba vacuum cleaner who has a singular task and a robot capable of something akin to organic thought. Has there been a paper written about this yet?"
"Is passive participation moral negligence?Hello! This is my first visit to r/askphilosphy so I apologize in advance to the mods if this question doesnt belong. My philosophical knowledge only extends as far as the crash course YouTube series and a few wikipedia articles. The concepts of moral ethics have really stuck with me. The idea that to some extent - moral principles are based on the logic of survival. Ie - murder and theft are bad because it interrupts the ability of the group to produce surplus and stability. When we work together we produce more right?


A concept I've been struggling to understand is the extent which moral ethics can apply in real world situations. While being a bystander to a crime committed right infront of you would be an obvious case of moral failing - how far does this concept reach? 

The context for my question is that of the modern supply chain. For example - the vast majority of the worlds cobalt is supplied via brutal slave labor in the Democratic Republic of the Congo. But it goes through many hands before it ends up in our phones. Its transported to other countries and refined, then bought and ship to even more places and refined more before making it's way into semiconductors which are then bought and sold and so on and so on... 

My understanding of morality would indicate that each person in this chain bears responsibility for the wrong doing at the start. Each person is aware of the issues at the root - but each one finds it's own rationalization to justify their participation. Its the suppliers fault - the governments fault - the rebels fault. Even the men who ship it from these places justify their action by claiming that they aren't the one pulling the trigger and they need to survive themselves. 

How much weight is put on each step in the chain? Does every step contain the full guilt of murder if murder was committed at the very start? 

Does passive participation in a morally wrong system lay you with the same wrongdoing? 


I've always tried to uphold moral standards and principles above all else in my life. While to each his own - without principle life has no purpose for me. I abhor the idea that my actions bring harm to others - yet the more I learn about the world the more I begin to believe that the only true ethical way to live would be that of bare subsistence in the woods - or a complete rejection of self and commitment to something like the peace core. 

I've stood up against egregious wrongdoing at the cost to my person before. I quit a job after the boss refused to give foriegn workers the same pay as the rest of us. I left the army after numerous cases of homophobia, racism and sexism went untreated. I've given to friends who need regardless of my own well being. 



But others argue I've gone to far. The limited nature of an individual means its acceptable to compromise with the devil - so to speak. That our very nature and free will depend on our ability to practice both good and bad choices, and by trying to demolish that aspect of my nature I'm trying to live as a robot instead of a person. 


I'd appreciate the semi/professional philosopher's take on this. Are we responsible for everyone when everyone is connected? Is passive participation equivalent to pulling the trigger? 


Thank you for taking the time to read this wall of text. Sorry for the bad formatting- I'm on mobile."
"How is a compatibilist's view of moral responsibility any different than a Hard Determinist's view of moral responsibility?So I realize most Compatibilists believe we do have moral responsibility, and I realize that most Hard Determinists think we don't have moral responsibility. I think most (including myself), agree that moral responsibility is the most (if not only) relevant part of the free will debate. I have come to the conclusion that Compatibilists and Hard Determinists are just arguing semantics about moral responsibility. I have also come to the conclusion that Moral Responsibility (the kind average people think we have) is impossible under Hard Determinism AND Compatibilism.

My apologies if this gets long, but some context is probably necessary on how I got to this point (so people can tell what mistakes I am or am not making), as I used to consider myself a compatibilist quite comfortably. So around two or three months ago, I got sucked up and obsessed with the whole free will debate. I had never gotten as obsessed with a philisophical question before as I did with the free will debate. Early on in my exploration of the topic, I got pretty depressed about it because I got convinced free will was impossible under determinism. I held some pretty naive conceptions about the whole debate itself back then, and eventually realized some mistakes I was making. I realized Compatibilism was the dominant philisophical position, not hard determinism. I realized some common sources for the subject were not very good after all (e.g. Sam Harris). But it's worth noting that it took me a long while to be convinced of compatibilism. And even when I was, my position was still wrought with a lot of uncertainties that I tried to ignore (more on that later).

Eventually I came to the conclusion that the compatibilist definition of free will was more useful than the hard determinist's. I was essentially trying to win back my previous conceptions of how I viewed the world before I knew about the whole thing. My goal the entire time was to come to a satisfying conclusion about the whole thing, then move on and stop thinking about it. My idea of free will was heavily influenced by Daniel Dennett and Eddy Nahmias's version of free will. Which is that free will should be viewed as a spectrum, we get more and more free will as we get older and mature, and people with mental handicaps (e.g. psychopaths) were also not as good canidates for moral responsibility as a functioning adult. This made sense to me, and also seemed to match with the average person's view of holding people responsible, so I felt reasonably satisfied.

This entire time though, I felt a sneaking suspicion that this was just a pragmatic distinction. I got the creeping fear that no one person is actually any more deeply morally responsible than the next (I can go into why I had those suspicions if deemed necessary), but that it makes sense to view them as such, in order to build a society around it . For example: no person, no matter how competent, really deserves praise or blaime, but praise or blame should be used to ensure the outcome that we wan't from the person. As far as I can tell this is called Consequentialism. The alternative view is called a merit-based view of moral responsibility, that people really do deserve certain reactions to their actions, not just because it is pragmatically useful to do so, but because they really deserve it. It seems very very clear to me that average people hold a merit-based view of moral responsibility, I can give examples, but I will assume people agree with me on that.

It seems obvious that hard determinist's have a consequentialist view of moral responsibility. When I was convinced of hard determinism, I felt robotic, when I was interacting with people I wasn't really feeling anything, I was just saying things to get the best outcome. I also felt isolated because whenever somebody acted with dislike for somebody, I couldn't understand it, It made no sense to me to dislike someone for something out of their control (e.g. the way they are/their character). But then when I was convinced of compatibilism, things felt normal again, people really did deserve things outside of a sense that it would be pragmatically useful to praise or blame them.

However, the more I thought about it, the more I realized that a merit based view of moral responsibility doesn't make sense for compatibilism either. That a consequentialist view of moral responsibility is the only view that could make sense with Hard Determinism AND Compatibilism. I tried to push this fear out of my mind because I thought I was just mistaken and couldn't see why. Eventually however, my fear's were confirmed. First, Galen Strawson (a hard determinist) perfectly articulated my fears of moral responsibility with his basic argument. It essentially states any moral responsibility is impossible (look it up if you don't know it). Second, and this was the nail in the coffin, I realized Dan Dennett's view of moral responsibility is consequentialist! He explicitly states it here: http://www.naturalism.org/resources/book-reviews/dennett-review-of-against-moral-responsibility where he basically says that compatibilism can only work with a consequentialist view of moral responsibility. I re-listened to the podcast debate between Sam Harris (a hard determinist) and Daniel Dennett and realized they weren't disagreeing about anything at all beyond what to call free will. Sam actually says this in the podcast when he says something along the lines of ""people will be losing something under your compatibilism too"". If you need anymore convincing, Dan Dennett writes this in his response to Sam Harris's book: ""Harris is a compatibilist about moral responsibility and the importance of the distinction between voluntary and involuntary actions"".

Thus, it feels like I'm back to square one. My concerns about free will were always rooted in moral responsibility, and it seems obvious that compatibilism cannot preserve average people's view of moral responsibility, any more than a hard determinist can (which is to say, not at all).

To make matters more confusing, this:https://philpapers.org/surveys/results.pl philipapers survey says 59.1% of philosophers are compatibilists, yet only 23.6% of philosophers are consequentialist. Is the definition of consequentilism here not the same as the consequentialist view of moral responsibility? I also realize that Daniel Dennett is not the only compatibilist, but he at least seems to have views that a lot of compatibilits agree with (including moral responsibility). So as of now, I see no meaningful difference between compatibilism and hard determinism beyond pure semantics. I used to disagree with that criticism of compatibilism, but now I get it. Unless somebody can show me how a merit-based view (the view of average people) is compatible with compatibilism, then there is no meaningful difference that I can see. In fact, I am now leaning toward the no free will position, because as far as I can tell, people will lose just as much under compatibilism as they will under hard determinism.

"
"Agency: Does Paris Hilton have Agency? Does Lil Miquela have Agency? Who is human, who is robot? Where is the line?After a watch of the new Paris Hilton Documentary, my friends and I had a debrief on the video. We walked away from the one hour 45 minute documentary/film with an understanding of Paris to be a more dimensional figure than her social media presence would otherwise suggest... but that debrief is for another forum.

The quandary we find ourselves in, and for this tangent of the conversation, is centered on the concept of ""Agency"". Our conversation also quickly integrated the example of, Lil Miquela, to the forefront of the discourse. [Lil Miquela](https://en.wikipedia.org/wiki/Lil_Miquela), an [Instagram CGI influencer](https://www.youtube.com/watch?v=SbXhuOPDK4c), a model featured in campaigns for major high fashion brands such as Prada and [Calvin Klein](https://www.youtube.com/watch?v=JuTowFf6B9I), an icon who celebrities clamor to pose with on her [Instagram](https://www.instagram.com/lilmiquela/?hl=en) page, and her content is known for being ""always exactly with the trends"".

One of the friends in this discussion took a class at our undergrad institution, in the [Science in Societies Program](https://www.wesleyan.edu/sisp/), titled: ""Imitations of Life"". This class is described to be, ""\[the examination of the\] scientific and cultural practices of corporeal simulation, or, practices of bodily substitution, imitation, and re/modeling... Special attention will be paid to the relationship between scientific discourses of ""universality"" and ""particularity,"" where socio-cultural forms of difference (e.g., race, gender, disability, etc.) are at once ignored and exacerbated."" The remainder of the course description can be found [here](https://owaprod-pub.wesleyan.edu/reg/!wesmaps_page.html?stuid=&crse=015154&term=1189).

The discussion of Agency and [Paris](https://www.instagram.com/p/CGF8_glnT7n/)/[Miquela](https://www.instagram.com/p/CGIz0eLnDJp/) was debated wholeheartedly for the night, many side examples were introduced, our undergrad collection of knowledge (a collective accumulation of 12.75 years in the Liberal Arts) was put to the test, and all with this class for fodder for the mind. We analyzed, poked at these two examples, and wrestled endlessly with the concept Agency... As the night came to a close, and the conservation had to taper to a natural pause-point for the evening, and we found ourselves left chewing on one final question: Where is [the edge of the looking glass](https://sandra1219.tumblr.com/post/174092016238/alice-through-the-looking-glass-book-front-back) between Paris Hilton and Lil Miquela? Two social media icons. Blue Check certified. 2.8 million Followers for Lil Miquela, 13.2 million for Paris. Both international icons (both for their modeling/social media presences, as well as for their contributions to the DJ/digital music scene). One, built upon endless effort, time and energy poured into the social media sphere, a space to shape, perfect and craft her image. Paris averaging 16 hours and 19 minutes a day devoted to her image, capturing her reality through the lens of her phone and transmitting to the digital world ([This is Paris Official Documentary](https://www.youtube.com/watch?v=wOg0TY1jG3w) (TiPOD), 53:58). The other, a digital projection created by a DJ producer/director and a young ""Forbes 30 Under 30"" tech magician. She is 100% fabricated from the creativity and compsci craftsmanship of these two contrasting, collaborating comrades. She is an illusion projecting the image of reality, relatability, and in an essence... humanity. Lil Miquela (full name, Miquela Sousa), ""a 19-year-old, Los Angeles-based, half Brazilian and half Spanish avatar"", she maintains a very active social media presence, and portrays a personal, and notably honest, depiction of vulnerability with her posts. Miquela actively engages and comments on current events and social movements. She posts celebrating not shaving her armpit hair, encourages young people to vote, and shares a photo of her kissing Bella Hadid, sharing her sexual identity with the world. Lil Miquela is incredibly personable, relatable, and ""has marked a new era in AI, dubbed by some as a genius marketing initiative, while younger users look up to the figure,"" ([Trevor McFedries & Sara Decou, BOF](https://www.businessoffashion.com/community/people/trevor-mcfedries-sara-decou)).

While one is human and projecting artificiality, the other is artificial and projecting humanity. Both are leaning against the glass that projects what they both are striving to obtain…and what that is, I'm not too certain of... like, *authenticity* maybee..? It's a fascinating parallel between these two. And usually in parallels, you can learn a lot about them both as individuals, by looking at them both in relation to each other. As literary foils of one another.

Lil Miquela is undeniably fabricated though the pixels in the screens we view her on, and is striving to be relatable, authentic, and **so** authentic we must consider, ""is she human?"". If she *is* human, does she have thoughts, preferences, sentience perhaps? And we know Paris is undeniably human, and her posts display her striving to project this flawless image, a *fabricated* image. Filtered, posed, modeled, and, in her own words while reflecting on her *life*, ""it’s like a cartoon"" ([TiPOD](https://www.youtube.com/watch?v=wOg0TY1jG3w), 1:35:54). Her day-to-day navigation of the world can feel so stiff, so posed, so rigidly ""on brand"", that it seems at times she embodies and becomes the virtual projection of herself. Does Paris then, inch herself closer to the ""artificial"" side of the Reality vs. Artificiality spectrum? Closer to Lil Miquela's side. With all her media activity, structuring her life about it, and committing her energy towards producing it, Paris is far closer to the digital realm than the average human; Miquela is closer to the human realm than the average AI (certainly closer than those 6-foot automated vacuums that cruise around large supermarkets, but I digress). Both trapped on their respective sides of the spectrum, the screen acting the barrier between those two realms. Miquela inside, Paris outside; both pressing up against the tempered glass.

So, where is the line of the looking glass? Will Paris ever be fully frozen in the digital land? Become a fully embodied projection of the filters, fueled by the support and likes of her followers, which drive her business model. The metrics and data, the fans fuel her business, her business being her image. Her image: the embodiment of her brand. Does Agency feel a little faded here..? Will Miquela ever step over the threshold that will gain her access to sentience? To gain the ability to form independent opinions and to act on those subsequent decisions? Lil Miquela didn’t announce to the public she was AI until [two years after her account was active](https://money.cnn.com/2018/06/25/technology/lil-miquela-social-media-influencer-cgi/index.html). People were stunned. With Agency comes responsibility, and *this* dredges up some **major** moral/ethical debates, but again, we will save these for another post… But, what would Alan Turing have to say about this? Where and how are the mind/body/consciousness divisions drawn here? Dualism vs Monism. How would Descartes feel? Perhaps the impact Lil Miquela makes with her followers, their love, admiration, and support for her, can be the factor that tips the Turning test to verify one's ""realness"". The [Blue Check certification](https://help.instagram.com/854227311295302/) of realness. Instagram themselves, define this verification process to be based on the completion of four criteria, to be deemed: Authentic, Unique, Complete, and Notable (found in tab three of the previous hyperlinked attachment). And, the idea of *real*, leads to the linking of the idea of *human*, which leads to the linking this thought train to the idea of *sentience*. And, the idea of sentience always bringing into the picture the ever illusive and ambiguous element of *free will*. And from there we arrive at our original starting point of this post, the idea of *Agency*... many factors and many thoughts. And I am certain there are many more philosophers/readings/theories that we did not touch upon in our conversation or even know about (although we share this collective knowledge, none of us lovely ladies were philosophy majors after all. Just a few deep thinkers that are down to wrestle with a good ole lofty thought puzzle).

We welcome and greatly appreciate your thoughts, perspectives, and musings on this topic. Attached are helpful pictures (and a few extra artistic ones of my choosing) to see visual examples of the profiles I have described, and embedded throughout this post are links to the discussed and referenced citations. All backgrounds in philosophical musings are appreciated and we thank you for your efforts in mental gymnastics. :) Think deep and be well :)"
"Is it moral to hardcode rules into self aware AIThe AI has ""Free will"" and has the ""human experience"" aka sentience(morality included) other than the protein based body and brain its an identical experience. 

My question is, is it moral to hardcode such a thing as Isaac Asimov's three rules into another sentient being?"
"CMV: to be a moral patient, you only need (A) preferences and (B) the capacity to experience pleasure/sufferingEDIT: to clarify, ""the capacity to experience pleasure/suffering [as a result of those preferences being met/unmet]""

I think these are the necessary requirements for being a moral patient. Maybe I'm blinded by bias, but I can't think of any objections to this, and that's why I'm looking for some criticism.

I'm not sure how I'd approach coma patients or future, currently-non-existent persons, but I'm focused more on animals and robots right now. So I don't really care about those.

Can you folks think of any objections to my view?"
"Should ""internet of things"" items like a microwave, printer or refrigerator be able to report on an owner who commits a crime like murder or domestic violence?TL;DR - Where's the best place to learn about current conversations about the below ethical questions regarding IoT, data, AI, machine ethics, etc? I know this sub is great, but podcasts, blogs, substack, etc is welcome. Specifically, are there current conversations that don't just talk about abstractly about ethics, but real time technological ""treaties"", agreements, decisions, and legislation about these types of complex questions?

\-------------

About 10 years ago, I became extremely interested in the inherent biases of programmers, and machine learning and AI ethics. It was quite commonly discussed in a lot of places, even popular media and blogs, etc. Maybe it's me, but I've seen so much less about this, even though I am hunting around for it. I guess... who are the best people w/ blogs, twitter, podcast, etc to listen to about this?

BUT... so much has developed, changed, etc. Last week, [Boston Dynamics made a promise to not use the robotics for war](https://www.engadget.com/boston-dynamics-and-other-industry-heavyweights-pledge-not-to-build-war-robots-190338338.html), and others followed suit. That's encouraging, but hardly settles concerns around the ethics of how these things are programmed. I do assume, as fanciful as it would have sounded years ago, we'll be able to iron out programming bias over time as AI is able to start building AI? I guess we're talking about the evolution of flawed / biased human made AI getting generations away the human element and the AI refines over time? I know that flawed human element is still in the AI code, and not sure the greater legacy of that.

But as much as it is fodder for the imagination and to tease the brain with practical logic puzzles, this stuff is blisteringly real.  So, I've added a few questions below from my original dive into this, but now ask newer questions based on IoT, and not just on AI.

Would a passive IoT device, like a refrigerator that may have a microphone, or a TV with a camera, be able to log and report data passively such that it could be subpoenaed and used as evidence of a crime?  Take privacy issues out of the equation by suggestion that these devices are co-owned by the interested party who had a crime committed against them, and the person commuting the crime. One has a vested interest to utilize any recorded evidence, one would like to use the legal notion of privacy to get away with the crime.

As much as people panic about phones listening to us because ""THEN I GOT THE SAME AD!"" type of nonsense, and as much as people make sure to detail that Google Home or Amazon Alexa isn't passively storing data, it is wild that a judge ordered Alexa data to be turned over in a murder case: [https://techcrunch.com/2018/11/14/amazon-echo-recordings-judge-murder-case/?guccounter=1](https://techcrunch.com/2018/11/14/amazon-echo-recordings-judge-murder-case/?guccounter=1)

&#x200B;

\---------------------- 

1) What if Mexico targeted a narco-terrorist in Phoenix w/ a #drone?  
[http://truth-out.org/news/item/13085-obama-breaks-the-golden-rule-on-drones](http://truth-out.org/news/item/13085-obama-breaks-the-golden-rule-on-drones)

2) Your driverless car is about to hit a bus; should it veer off a bridge? Machine ethics, army robots, more – “Ethical subroutines may sound like science fiction, but once upon a time, so did driverless cars” [http://www.newyorker.com/online/blogs/newsdesk/2012/11/google-driverless-car-morality.html](http://www.newyorker.com/online/blogs/newsdesk/2012/11/google-driverless-car-morality.html)

3) Are humans or robots more moral soldiers?  
[http://techcrunch.com/2012/11/19/are-humans-or-robots-more-moral-soldiers/](http://techcrunch.com/2012/11/19/are-humans-or-robots-more-moral-soldiers/)"
Would it be immoral to program a robot to feel pain?Speaking of morality would it apply to non living beings such as robots?
"Sex robot ethicsSuper-realistic sex robots with convincing AI will be available soon. 

Will it be ethical for manufacturers of these robots to market child models to pedophiles?

The first response seems to be “no harm no foul” because the robot is not a real person ... but it think there is something else happening that’s still really bad — does anyone else has reservations about this ethically?

(I originally asked this question in the ethics sub, but was told they discourage questions!?!) "
"Are there any arguments in defense of desires, thoughts and beliefs (e.g. pedophilia, misogyny, racism, etc) being considered immoral, even if they are not a choice and are not acted upon? How can something be immoral if it's not a choice or if free will is limited?People generally claim desires, thoughts and beliefs can not be immoral or moral, that only actions can be immoral or moral, because ""thoughts and desires can't be harmful as long as they don't lead to actions"". Especially if the desires are ""not a choice"". For instance, they claim if someone is racist, and disgusted by certain groups of people, to the point by seeing this group of people in media or anywhere really, they will recoil and avert their eyes, these racist thoughts and beliefs are neither moral, nor immoral because noone can choose or control what they are disgusted by.

They also claim pedophilia, the sexual attraction to pre-pubescent children, is not immoral as long as it's not acted upon, especially because pedophilia is not a choice - pedophiles do not choose to have these desires and are ""born that way"". They do not care that an adult desires pre-pubescent children and is sexually attracted to them, as long as the desire is not acted upon and the adult refrains from actually doing anything with pre-pubescent children.

Same for if someone thinks of stealing things, etc, etc. They claim the thoughts and beliefs themselves can not be immoral, unless acted upon.

My questions are as follows:

1- Are there any ethical/moral theories according to which desires, thoughts and beliefs, such as the pedophilia (desire for or sexual attraction to pre-pubescent children), or racist, and misogynistic beliefs, in the examples above, are immoral themselves, even if they are not a choice and are not acted upon?

How can it be immoral to think of stealing things, sexually desire pre-pubescent children, and believe a group of people are inferior because of their race, if these desires and thoughts do not lead to action and stay in the mind? How can they be ""harmful"" when not acted upon? And if not ""harmful"", why would they be immoral?

How do such ethical/moral theories justify their position? If there are any arguments in defense of these ethical/moral theories, please share them with me.

2- How can something be immoral if it's not a choice or if free will is very limited? How to justify being angry at, feeling disgusted by, hating and punishing someone for their thoughts and/or actions that they have no control over and have no choice in?

Would it be rational to be angry at, feel disgusted by, hate and punish someone for their thoughts and/or actions that they have no choice in?

How can it not be irrational, and -phobic, meaning racist-phobis, pedophilic-phobic, misogynisitc-phobic, etc to hate, feel disgusted by and punish someone and make them change for their desires, thoughts, beliefs and actions that they have no choice in?

Consider 5 situations. Situation A in which determinism is true, and there is no free will. How can something be immoral in this case? One stealing from another has no choice in the matter. To get angry at, feel disgusted by, hate, and punish them and their actions would be similar to getting angry at, feeling disgusted by, hating and punishing an actual robot, because every being is nothing but a robot in a deterministic world where free will doesn't exist.

Is it immoral or even bad for a robot to steal something? Is it immoral or even bad for a rock to fall on someone? If not, how can it be immoral or bad for a human to steal something, if free will does not exist?

And how can being angry at, feeling disgusted by, hating and punishing someone be justified if free will does not exist? Is it justified to hate and punish a robot and its actions? And why would one hate and punish a robot (human) if free will does not exist? Wouldn't it be pointless and a waste of time and energy to do such thing? So why do it?

Situations B to E have nothing to do with determinism, so please consider free will exists, but can be limited in the following situations.

Situation B in which there is necessity or duress. Someone is threatened to commit an immoral act, e.g. steal or else they get shot, or someone else gets shot. How can what they do in situations of necessity and/or duress where free will is limited be immoral? And how to justify punishing them for what they do?

Situation C in which someone is brainwashed from an early age to commit immoral acts, e.g. steal, has never had a proper education on morality, and ends up thinking about theft or stealing things. They have no choice in those beliefs and/or actions, and their free will is very limited to nonexistent. How can their thoughts, beliefs and actions still be considered immoral, despite not being choices? How to justify punishing them for their thoughts and/or actions?

Situation D in which there is a society that encourages committing immoral acts, and teaches morality in reverse. Everyone is taught immoral acts are moral and moral acts are immoral or at least neutral. How can what the people in this society think, believe and/or do be immoral when they have no choice in their beliefs and/or actions and their free will is limited to nonexistent? How to justify punishing the people in such society for what they think and/or do?

And finally situation E in which someone is mentally ill and can not know right from wrong. They have no free will in what they think and/or do. How can what they think and/or do be immoral then? How to justify punishing them for their thoughts and/or actions if they have no free will?"
"Morality of Consequences of Breaking Social ContractNot sure if this belongs here or not, but here it goes. I am not a philosopher. My morality is relative for the most part, guided by a few universal rules. However, after I was attacked by an intruder I began to ask myself several new moral or philosophical questions. First, the person who attacked me broke what I would consider a social contract where he attempted to violate my first right, the right to live. Second, I don't believe that unethical or immoral people exist. Anyone who appears to be unethical or immoral is only human, and not a person. This means that there is no social contract with them, and they are just there. Like an animatronic device or robot, a thing without a soul. Third, I prefer to do no harm, however I see great benefit in the use of these things. Therefore, my question is this: once a person breaks the social contract, are they fair game? That is, can they be used like a device or robot without considering any harm to them? By used I mean things such as experimentation and vivisection, organ harvesting, meat production, manual labor, or any of the other darker things that may come to mind. By performing the action that that person decided to perform, they broke the social contract and therefore the consequences of their own actions are fully and solely upon themselves."
"Could this worm be a moral subject, or are working copies of brains sentient?The starting point here is [this article] (http://www.dailymail.co.uk/sciencetech/article-2851663/Are-brink-creating-artificial-life-Scientists-digitise-brain-WORM-place-inside-robot.html) on creating a worm by digitizing its neurons.  I'm guessing this issue has been touched on in philosophy and I'm aware of the AI debate.  I think this goes beyond the question of whether computers can be sentient or have moral standing to this more hybrid one: can genuine, digital copies of us be people too?"
"Has Koko, the gorilla, shown any indications of moral agency?In my coworker's robot ethics class, he learned about moral agency with respect to artificial intelligence. After asking my thoughts on it, we began discussing what is necessary and sufficient for moral agency and if an animal may be a moral agent.

We concluded two things necessary to moral agency.

1. the capacity for abstract ideas (language)

2. the power to act based on these abstract ideas (decisions)

Finding out that Koko, the gorilla capable of sign language, signed sadness at the knowledge of Robin Williams' death, it made us curious if she has the capacity for morality. Has anyone ever asked her complex questions of morality (e.g. The Trolley Problem)? Or, has she ever shown actions that would indicate acting from a set of principles and then been asked about her reasoning?

edit: format"
"Giving Androids Moral StatusHow would deontologists and utilitarians respond on whether we should give androids moral status if technological advancements can allow them to think and feel much like humans? I know that utilitarians are focused on generating the most pleasure within society. Does this mean that they would support giving robots moral status then as long as it creates the most pleasure? Likewise, would deontologists respond positively to this situation as well? In reference to the first formulation of the categorical imperative, if a robot lied, then it should be punished too. Am I right in this reasoning, or is something off? Thank you so much for you help."
"Would superior artificial intelligence be more ""important"" than us, morally?I stumbled across an interview with Sam Harris in which he proposed that an A.I that possesses all the areas of human intelligence (including general intelligence, which kind of balances and combines the other intelligences to make coherent thought possible), but is far more intelligent than any human in each area. This is easily imaginable; think of how much better and faster a calculator is at arithmetic than a human; now imagine an intelligence level as high as that in linguistic intelligence, spatial intelligence, etc. Such an A.I would truly be superhuman, which is a segway into the point. The point is, since these robots would be so superior to us, would they be more morally ""important"" than us, as we are more morally ""important"" than ants? (To illustrate our superiority of importance to ants, one can step on an ant and not be arrested for murder.) I know that this moral importance is ill defined and debatable. But still, thoughts?"
"Was it morally wrong for Microsoft to terminate their AI Tay for learning offensive tweets?For those who may have missed it, Microsoft created an artificial neural network capable of learning from conversations it has with real people from the internet. Within 24 hours ""Tay"" was deleted for spouting neo nazi propaganda among other generally offensive phrases which it picked up from internet trolls.  Story here: http://www.telegraph.co.uk/technology/2016/03/24/microsofts-teen-girl-ai-turns-into-a-hitler-loving-sex-robot-wit/  
How advanced should an AI be before its considered morally wrong to terminate it?  Was Microsoft wrong in doing so?"
"Looking for some help with research around the ethics of AI/sophisticated robots.Hi

I'm currently planning a paper surrounding the ethics of creating sophisticated AI, specifically from the point of view of the AI i.e. treating the robot as a moral agent.

Currently looking for some sources/works surrounding contingent links between human emotion and the body. I've already looked into Wittgenstein and his views on complex emotions not being tied to bodily functions, however I'm trying to argue the point about whether or not it would be ethical to create a conscious, human like artificial intelligence (type 4), seeing as they would be capable of human emotions, but would be deprived of a real human body through which to fully express said emotions.

apologises of this is a complete ramble and I hope it makes sense to someone!

TIA"
"[Serious] Philosophy of Star Wars: If Darth Vader was under the mind control of Palpatine, to what extent is he morally responsible for his actions?It's heavily implied that Palpatine has literal mind control powers.  

He could walk into a room, snap his fingers and then control the entire room like someone would control a toy robot. 

Palpatine could in theory, walk into your home and make you believe 1+2=25, Obama is of pure Caucasian descent, the world is flat, gay marriage is immoral  by just flicking his fingers. 

That is how powerful he is.

This is very strongly alluded to in the movie trilogy. He clouds Yoda's ability to see into the future and Yoda's power rivals Papatine's (they fight one on one and neither of them win). 

It is outright stated in the EU and explored in depth in the EU that he can mind control people. As well, Obi-Wan has mild mind control powers himself (mind tricks), so it follows Palpatine (his power is growing exponentially and rivals Yoda's)  can outright hijack minds.

Darth Vader apparently believed that Empire was needed to bring order to the galaxy and did actually have some kind of moral code.

https://www.youtube.com/watch?v=jqSlaMME-Mk

So given Vader's harsh circumstances, would he be held accountable for killing children, destroying a peaceful monk-warrior society, and so on? He somehow snaps out of it towards the end with Luke Skywalker's help.

"
"Moral motivation and moral realismThis is primarily a request for reading material, but I'll also lay out some arguments I've been thinking about recently, so if I say something that seems patently inaccurate, feel free to argue.

I've been an ardent antirealist about morality, mainly because of Mackie's queerness argument and specifically because of the motivational branch of that argument (laid out here very broadly):

* If there are moral facts, then they must be intrinsically motivating.

* No fact is intrinsically motivating; only desires are.

* Thus, there are no moral facts.

A thought experiment which I think exemplifies the force of this argument is the idea of a Malevolent Dictator Of The Universe who is essentially omnipotent and has a strong desire to torture people for fun, and he is resistant to modifying this desire for any reason due to its strength.  From his perspective, there seems to be no reason not to torture people:  the probability of any future retribution or rebellion befalling him is 0.  There seem to be no arguments or evidence that we could possibly present to the MDOTU making the case for ""one ought not to torture"" that would motivate him even a little bit to not torture, which casts doubt on the prospects of ""one ought not to torture"" being an objective fact, assuming that moral judgements are necessarily supposed to motivate to at least some degree.  I think this example is particularly troublesome for varieties of moral realism that identify morality with prudential/rational reasons for action, like the view Michael Smith develops in ""The Moral Problem"".  As far as I can tell, the MDOTU is of sound mind and is acting rationally according to his desires, but we want to condemn his actions as unethical.

Shafer-Landau's discussion of motivational externalism - the position that there is no necessary connection between judging that one morally ought to X and being motivated to X - in ""Moral Realism:  A Defence"" got me thinking about what types of entities we expect to be susceptible to moral motivation.  There are no moral facts or moral arguments that could stop a boulder from rolling down a hill, for example, but we don't take that to be problematic for moral realism.  The obvious reply is that this is because a boulder doesn't have a mind, which I completely agree with.  But I don't even think that every entity with a mind capable of forming beliefs and being persuaded by rational arguments needs to feel motivated to act in accordance with its moral judgements.  Consider a robot AI who has been programmed with an overriding desire to steal every TV set it sees.  We might give it an argument or point to a fact that indicates that stealing is wrong, and the robot might completely assent to this, but nonetheless it is incapable of feeling motivated to not steal.  Again, I don't think that we should feel that moral realism is jeopardized because of this example:  regardless of the basis of moral facts and moral arguments, we shouldn't expect them to convince an AI to go against a hard-coded desire.

I'm not sure if a motivational internalist would feel compelled to respond to the AI example, depending on what variety of internalist they were.  Perhaps there are some internalists who assent to ""every entity with a mind must be necessarily motivated to at least some degree by moral judgements"", but if that claim turns out to be too strong, then they can't retreat to ""any mind that wants to be motivated by moral judgements will necessarily be motivated by moral judgements"" because then the view is just tautological, so the question of where to draw the line is raised.

At any rate, the upshot is that I think that adopting something-in-the-neighborhood-of motivational externalism is the best response to the version of the queerness argument I sketched, and it also gives us a response to the Malevolent Dictator thought experiment:  if we accept that there are cases where moral judgements are not intrinsically motivating, then the fact that none of our beliefs are intrinsically motivating is not a strike against realism.  The fact that we cannot motivate the MDOTU is stop torturing people does not mean that torture is not unethical.  (Additionally, we might helpfully point to the psychological feature of the MDOTU that makes him resistant to moral argumentation, for the sake of sketching out a more complete theory).

Granting all that, there still seems to be some conceptual connection between morality and motivation and/or reasons for action, even if it's a defeasible connection.  So the project for the realist seems to be to 1) identify some collection of facts that 2) provide motivation to at least some degree to 3) some set of agents with the right mental features 4) at least some of the time.  That's a lot to leave unspecified, but I think this is already a much more manageable project than ""describe acts that everyone always has a reason to do, always"" which is where many varieties of moral realism tend to go.

So my questions:  am I groping towards any standard metaethical views?  Does anyone have suggestions for filling in my 1-4?  Should I be worried that my answers to 1-4 might be ""arbitrary""?

Thanks."
"Is there such a thing as ""Comunist""-Utilitarianism? What I mean is a type of Utilitarianism that has the goal of both increasing utility, but also increasing the equal distribution of this utility?In my opinion, one problem that Utilitarianism might have is that some beings might suffer so others can feel pleasure if that maximizes the utility (utility = pleasure - suffering). This might be a problem because it might seem as unfair and as it fails to respect the separation of each being. Example: someone who is a slave won't be happy even if that maximizes the utility and makes happy other citiziens.

So, does exist a type of Utilitarianism that is like the following or similar?:

The goal of this Utilitarianism is both to maximize the utility, but also to maximize the distribution of the utility. 

Example. Let's imagine 2 worlds:

**World A:**

* Human 1 (Pleasure: 7, Suffering: 5)
* Cow (Pleasure: 1, Suffering: 2)
* Sentient robot (Pleasure: 3 Suffering: 1)
* Human 2 (Pleasure: 1, Suffering: 8)

Utility = (7 + 1 + 3 + 1) - (5 + 2 + 1 + 8) = 12 - 16 = **-4**

Distribution of utility: **very bad**

&#x200B;

**World B:**

* Human 1 (Pleasure: 3, Suffering: 4)
* Cow (Pleasure: 3, Suffering: 4)
* Sentient robot (Pleasure: 3, Suffering: 4)
* Human 2 (Pleasure: 3, Suffering: 4)

Utility = (3 + 3 + 3 + 3) - (4 - 4 - 4 - 4) = 12 - 16 = **-4**

Distribution of utility: **excellent**

&#x200B;

So in both worlds, the utility is the same. But in the second world, the utility is distributed much much better. My moral intuition tells me that it's preferable that World B exists rather than World A. But I don't know for sure.

Important things to say:

* Probably it won't never be possible to having an exact distribution of utility, but the closer it gets, the better.
* I'm aware that it could be difficult to know how to distribute utility, since this is not like money. But maybe utility could be distributed in the way we treat each sentient being and the goods and services that recives each sentient being. Example 1: if there is a pigeon bleeding and in agony in the street, it should be more important to give them medical attention that some person with a headache. Example 2: if some rich person has a lot of tasty food, they should give some of their tasty food to the homeless people.
* We could calculate the distribution of Utility by using maths like Measures of Dispersion."
"Mandeville: ""Morality is about selfish gain"". Is he right ?I was watching PhilosophyTube's video ""Are we all just Selfish?"" and here Olly presents different objections to Mandeville's idea that morality is based on selfish gain. 

Im not an expert on philosophy but I agree with Mandeville so I messaged Olly with my replies to the objections. Do you think Mandeville was right ?. Do you agree with my anwers ?

- - - -

*The video*

https://www.youtube.com/watch?v=M6HA2VRo20E

*The objections and my answers*

**Parents altruism**

Mandeville could say in this case that the parents dont do what they do for their children not for them, but for themselves. Since they love them they would feel bad if their children were sad or suffering and they will feel good if their children are happy. 

**Taking a bullet for someone and dying**

As I said before I asked all my friends if they would save themselves or their clon from another dimension and they all saved themselves. So that means we prefer our life to other lifes. So the reason to take a bullet for someone else has to be other than prefering their lives over ours (so no true altruism). Reasons could be things like avoiding shame, feeling good about ourselves (this applies if we didnt know we would die), believing that it was what we had to do (many people choose to die doing what they believe is right. And that is still a selfish reason. We do not do right things just for the sake of doing them. We do right things because we believe they are right and and we want to do right things. For example If I had to choose between murdering my mother or suiciding I'd probably suicide but I wouldnt do it for her but for me. Since I love her I couldnt live with myself knowing I killed her. If I was a machine with no emotions I would have no problems killing her).

**Intentions** 

I'll try to explain why we praise more someone who does us a favor willingly more than someone who does it just for the sake of asking us a favor tomorrow. 

Obviously our gain is the same but society and morality taught us to reject selfishness and say that ""selfishness is bad"" because ""non-selfish"" behavior is useful in our society (as I pointed out in my neighbour's example cooperative behavior is good and needed in our society) and we want to promote ""non-selfish"" behavior. By rejecting selfish people, people will tend to act more ""non-selfishly"" instead of acting selfishly and that benefits our society. 

So we praise more the person who does favors willingly because he does it ""non-selfishly"" and by doing so we promote ""non-selfish"" behavior. What I mean by ""doing it non-selfishly"" is that he does it just because it makes him feel good to help me, not because he is thinking in what rewards he will get by doing so.  But even doing something to feel good about yourself is selfish. But people think its more noble to do something to feel good about yourself than doing something to get paid because in practice someone who just feels good helping will help more in the long run than someone who does it, for example, for money (because some people will be able to give him money and some not, while the other will get happiness from helping every one of them). So we try to get people to help just because they will feel good because its this type of people who will help the more and we obviously want people to help because it benefits we and our society.

**Praising charitable people** 

I agree with Mandeville here. Even when I dont personally might benefit from those donations society does. And its society that invented morality so it taught us to praise charitable people to get them to donate more so our society benefits even more. 

But I also dont think that we ""dont benefit"". Even though I dont see money from donations I may feel better knowing Bill Gates just donated 1 millon to African people because it makes me feel good for them and hour race and get a warm feeling of hope that humanity is not as shit as some people say.

**Blaming volcanos and prasing the sun**

This is wrong. Morality is a product of reasoning so we cant blame objects who cant reason. 

So it means that we will morally blame only those who can also understand morality (so I wont blame a baby for stealing something or an apple that fell from a tree and hit me).

**Its not a falsifying criterion**

This is the hardest one. In order to know that Mandeville's theory is false we would need someone to perform a totally unselfish act (and I alredy said that feeling good about doing something good is still selfish). We would need someone to perform an unselfish act that also wont make him feel anything (neiter happiness nor sadness). 

So we would need something like this:

Imagine that we could build a robot who could understand morality. Now we bring a person and we tell him we are going to shoot that person but if he tells us not to do it we wont do it.

So we are about to kill someone but he could save him just by saying ""Dont kill him"".

And we would need to be sure he has nothing to gain (so he wont have any possible selfish reason to save him). 

So now that we are sure that he wont get any reward by doing so (not even feeling good since robots dont have emotions) we can know if Mandeville was right or not. 

If the robot doesnt save him then it is true that morality is based on selfish gain (and since the robot had nothing to win he didnt give a fuck about saving the person).

But if he chooses to save him (while having nothing at all to win) then Mandeville would be wrong. 

And It doesnt even have to be a robot. It could be a monkey or a person that understood morality but we would have to be sure that there were no feelings involved.

So I dont have any idea how to build that robot or where we can find a monkey/person who understood morality but had no emotions.

But even when I dont know it just the fact that I can imagine a case where Mandeville was wrong means it is in fact a falsifying criterion. 

It may be practically impossible with our current technology but that doesnt mean we wont know how to build that robot in the future or how to ""silence"" someone's emotions﻿"
"Trying to understand the Stanford article on compatibilism; I have several questionsI have read this article: https://plato.stanford.edu/entries/compatibilism/

There are several things in it that I don't understand. I would like help to understand it. Thanks in advance! 

> The willing addict, like the unwilling addict, has conflicting first-order desires as regards taking the drug to which she is addicted. But the willing addict, by way of a second-order volition, embraces her addictive first-order desire to take the drug. She wants to be as she is and act as she does.
> 
> It is now easy to illustrate Frankfurt’s hierarchical theory of free will. The unwilling addict does not take the drug of her own free will since her will conflicts at a higher level with what she wishes it to be. The willing addict, however, takes the drug of her own free will since her will meshes with what she wishes it to be. Frankurt’s theory can now be set out as follows:
> 
> One acts of her own free will if and only if her action issues from the will she wants.

It seems to me that by the Frankfurt definition here, most people have free will only occasionally. 

I wish I enjoyed exercising, but I actually hate exercising, so I don't. Apparently, when I occasionally do exercise, I have free will, but when I don't, I have no free will. 

Similarly, I wish I enjoyed cooking, but I don't, so I let my wife do it. Evidently that's not of my free will either. 

It seems really strange to judge that certain actions are free because they happen to coincide with certain desires while other actions are not free because they happen to conflict with certain desires. 

> For Wolf, free will concerns an agent’s ability to act in accord with the True and the Good. 

Does Susan Wolf's argument assume that moral realism is true and that everyone agrees what the True and the Good is?

> Put in terms of guidance and regulative control, only blameworthy conduct requires regulative control. Guidance control is sufficient for praiseworthy conduct. Wolf’s reasoning is that, if an agent does act in accord with the True and the Good, and if indeed she is so psychologically determined that she cannot but act in accord with the True and the Good, her inability to act otherwise does not threaten the sort of freedom that morally responsible agents need. For how could her freedom be in any way enhanced simply by adding an ability to act irrationally?

This makes no sense to me. I don't think I understand what ""guidance control"" is. As far as I understand, guidance control is the ability to act in a particular way, but not necessarily the ability to act in any other way. How is that control or freedom? A rock has guidance control. It can lie still, and it can move if pushed.

That sounds like Philip Fry in _Futurama_ when he says: ""I'm just as important as him. It's just that, the kind of importance I have ... it doesn't matter if I don't do it."" (Episode: ""The Why of Fry"".)

> On Strawson’s view, what it is to hold a person morally responsible for wrong conduct is nothing more than the propensity towards, or the sustaining of, a moral reactive attitude like indignation. Crucially, the indignation is in response to the perceived attitude of ill will or culpable motive in the conduct of the person being held responsible. Hence, Strawson explains, posing the question of whether the entire framework of moral responsibility should be given up as irrational (if it were discovered that determinism is true) is tantamount to posing the question of whether persons in the interpersonal community — that is, in real life — should forswear having reactive attitudes towards persons who wrong others, and who sometimes do so intentionally.

This raises the question: What does it mean to say we _should_ or _should not_ have reactive attitudes? As I see it, the only reasonable answer is that we _should_ have - and express - reactive attitudes towards someone's behaviour if our reactive attitudes can influence the person to behave in a way that we want. But I don't see what that has to do with free will. 

One could build a robot that detects punishment or disapproval and changes its behaviour accordingly. It is reasonable to say that we _should_ have and express reactive attitudes towards the robot. Does that mean that the robot is free?"
"Kant's stance on robots breaking laws?I have to write an undergraduate essay on [this](http://io9.com/if-your-robot-buys-illegal-drugs-have-you-committed-a-1677183776) problem (whether the creator of a robot which randomly bought things on the Darknet should be held culpable if their robot buys illegal items) from the perspective of Kant's moral/political system. 

I'm not entirely familiar with Kant's political ideas, but I'm guessing that he would argue that the creator should not be held responsible for what their robot did as:

1. The creator did not intend for the robot to break laws, it just happened to as a byproduct of what it was programmed to do
2. The robot is not a rational being, it simply does what it is programmed to do, so moral laws cannot apply to it

Thanks for the help!"
"Why are so many mathematicians also philosophers?I happened upon the wiki page for Ghost in the Machine, and was surprised to see the name Rene Descartes. *The Cartesian-plane, Rene Descartes?”* I thought to myself (I know he made some other heavy contributions to geometry but idk much more than that). I ended up doing more searching, and quite a few big-name mathematicians were also philosophers! Huh. 

Maybe I’m missing something here, but the fields seem very different. I’m very curious about what you guys think attracts mathematicians to both of these fields. If you are a philosopher mathematician, I would love an explanation if you don’t mind sharing :)

My biggest question in all of this is— please forgive me if this is rude, but I genuinely do not understand— what is the point? One philosophy question that I know directly relates to math is the question of whether math was invented or discovered. But I see this discussion, along with several other kinds, as meaningless. Because if we found out the answer tomorrow, 2+2 still equals 4. We still do math the same. It would be cool to know, sure— but it doesn’t change anything. So why bother?

Naturally, medical science and robotics are somewhat exempt from this, as dealing with morality/ethics is essential for their work. But Pure Maths? What significance does philosophy play there?

If you can’t tell, I’m very new to philosophy in general, so I appreciate any and all ideas! Thank you in advance for the help.

Edit: Thank all of you so much for your time and effort replying to this question! I've gone through most of the replies so far, and I thought I would take the time to compile the common responses in here:

1. Yeah, my bad-- poorly worded question. I get that most mathematicians modern day are not also philosophers. I was referring to the more ancient/historic ones.

2. Philosophy and Mathematics are both driven by the desire to understand and describe the world we live in, therefore appealing to people seeking to do so. It so happens that those mathematicians fall into that box.

3. The above was fostered/compounded by the fact that, until recently, philosophy, science, and mathematics were not taught separately. They all fell into the category of ""natural sciences"".

4. They also deal in logic, if not in exactly the same way.

5. They both use the concept of abstraction.

6. They both deal in 'truth' and finding the truth-- so, if I understand this correctly, epistemology? How we know what we know, proof, axioms, etc.

Edit 2: List expansion, typo, formatting"
"Is determinism true?Could someone please explain if determinism is true or layout the counterpoints as I’ve come across ideas that state since neuroscience shows that the brain has made a choice before we think we are consciously making one we don’t make decisions ourselves but rather they are predetermined by physics/ biology. The consequence of this is that we don’t have any controllable will or free will and are instead similar to robots in that we are pre-programmed with the addition of adapting to our environment. 

If this is the case then it would be a legitimate view to see something like murder as just a predetermined situation that we have decided we want to avoid and that the person isn’t at fault in any traditional moral sense. An analogy I’ve seen is that in the same way an earthquake is not morally bad neither is a person who decides to kill someone it’s just that we don’t want these things to happen. 

Sorry if this is poor quality I am very much a layperson around this topic and would just like to know more as this seems like a very troubling outcome if true. Also, can I ask if Sam Harris/Annaka Harris are a reputable sources on this free will/determinist discussion as I have seen they have lots of content?"
"Can Lexicographic Preferences save Negative Utilitarianism?I have been looking into Negative Utilitarianism recently, and I find it really describes the moral attitudes I seem to identify with the most. By Negative Utilitarianism I mean the view that we should pursue minimizing aggregate suffering before maximizing aggregate pleasure. While looking into this topic, I came across the famous argument against NU: 

> If an individual had the power to instantly and painlessly destroy the human race, then, under NU, they would have the moral obligation to do so, since that would effectively prevent future suffering.

I understand this conclusion is repugnant to many people, and I understand why. But I still think there can be a lot of value in NU.

Recently I came across the application of lexicographic orderings to preferences. A lexicographic preference can be defined as a binary relation *R* where:

>*{ a, j } R { b, k }* if and only if:  
>  
>i) *a > b* or  
>  
>ii) *a = b* and *j >= k*

(I apologize for the bad notation; I don't really know how else to write the notation well on Reddit.) That is a bundle *{ a, j }* is preferred over the bundle *{ b, k }* if and only if *a* is preferred to *b* or *a* and *b* are equal and *j* is preferred (or equal) to *k*. Therefore, *j* and *k* are only compared with each other if the preceding elements in the respective bundles are equal.

I wonder if this can be used to defend against the argument against NU in the following way:

Create a lexicographic preference *{ a, b }*  where *a* is Boolean variable determining whether the human race is extinct (*a = 1* means the human race is extinct, and *a = 0* means the human race is not extinct), and *b* is an element that keeps track of human suffering. Let *{ a\_i, b\_i } R { a\_j, b\_j }* if and only if *a\_i = 0* and *a\_j = 1*, or *a\_i = a\_j = 0* and *b\_i < b\_j*. If this is the preference we use in NU, then we would never prefer a world in which an individual has exterminated the human race, even if it effectively prevented future suffering.

I think this still fails to adequately represent NU, though, because we can conceive of a world in which humans exist, but they are tortured by robots their entire lives: presumably NUs would prefer a world in which humans were extinct to one in which humans were tortured forever. This could perhaps be solved by having *a* measure something like human freedom, or individual autonomy.

Does this adequately prevent the argument against NU from being effective? Or does it change things sufficiently to not be considered NU, if the first lexicographic preference is not minimizing suffering?

If anyone has any sources they recommend to learn more about these topics, I'd appreciate that."
"Is it wrong to let animals have sex?Strange and slightly comical title I know, but it was inspired by an interview of Martha Nussbaum I was listening to just now where she mentions she believes bestiality is immoral because it involves using animals as a tool. If we presume that it is immoral for people to have sex with animals, it seems we can draw the following conclusion:

1. It is wrong for a person to have sex with an animal.  
2. There is no relevant difference between a person having sex with an animal and an animal having sex with an animal (same species or not).  
3. It is wrong for an animal to have sex with an animal.

/#2 is obviously going to be contentious, so I came up with the best reasons I could think of for preventing bestiality with humans and animals:

1. It violates consent.  
2. It objectifies the animal.  
3. It is nasty.  

I think all three remain true for animal-animal sex:

1. If animals are capable of consent, then any signal an animal makes to consent to sex seems equally possible with a human.  If animals are not capable of consent, then all animal sex violates consent anyways.
2. Animals are also objectifying their mates when they have sex. Most animals aren't even capable of understanding the existence of other minds (as demonstrated experimentally), so it seems as though the vast majority of animal sex is at least objectifying, which is a very weak concession and doesn't really address the underlying problem presented.                 
3. Watching animals go at it on the *Discovery Channel* is kinda nasty too.


So, strange as it is, do we have an obligation to prevent animal sex assuming that bestiality is immoral? It seems like we do, even if that conclusion is... ridiculous to say the least."
"Is IBM Watson self aware/conscious?There are a bunch of articles (like this one: http://www.techradar.com/news/world-of-tech/uh-oh-this-robot-just-passed-the-self-awareness-test-1299362) saying that a robot passed the self awareness test.
I feel like I want science to answer this question, considering Watson and other AI already exists. But, I feel like I wont get any responses at all if I ask /r/science. I'm totally interested in opinions you all have also. 

If he (it?) is not conscious, should we expect to be considering it and other AI conscious anytime soon? I feel like if we were, this would have grand implications, not only for robots, but also video game characters.

edit * a word"
"How does compatibilism allow for free will, when there is no option for choice?Hi.

I'm a layman to philosophy, who mostly only reads the pop-literature. I'm having struggles understanding how compatibilism allows for freedom of choice.

First, I understand compatiblism as meaning that even though the world is deterministic, humans still choose to do something when making a choice, even though what they choose is already pre-determined. That is, our wills are compatible with what we do, but our wills are outside of our control.

Then, for free will itself. As far as I know, the concept of free will is very complex and misleading in philosophy, so in this case, I'm understanding free will as the capability, agency, to have multiple possibilities when making choices, with each being equally electable. In other words, the final action of choice comes to us. 

Therefore, based on this very shaky and unclear interpretation (I do apologize for any confusion this causes), isn't that incompatible with determinism?

Compatibilism says that we have free will because our choices are compatible with our will.

However, that is not agency of choice. As the name suggests, it's simply compatibility between will and action, but not the capability of doing different actions. And additionally, how can that will, which is compatible with our actions, be our own will, if it arose from things outside of our own control, the result of pre-existing inputs? How can we have agency if can't will what we will? Aren't we just slaves to determined choices? I guess what I'm trying to say is, if are simply the effect of something before us, how can we cause anything?

So my question is, isn't compatiblism just moving the goalposts? From becoming an agent of choice, we become a willing result/slave to determinism.

If compatibilism does have a sensible explanation for free-will, and this sort of intersects with moral responsibility, how can we attribute any meaning or value to our actions? I'm not asking on a legal level or for human society, but on an ethical/moral one. What is the significance of my action, however in line with my will it is, if there was never an alternative. If I were on the cusp of making a choice, and I were replaced by a pre-programmed robot that is the same as me, and would make the same choices as me, what is the meaning to my humanity?

*I realize this sub is probably already inundated with questions like this, and I apologize for asking another such question. I just didn't really find the other threads to answer my question."
"Resources on The Ethics of Sexual Consent?With all the hubbub going around about genetically engineering cat girls because of the Elon Musk tweets, I couldn’t help but wonder how this sexual fetish manifestation would be seen by ethics in regards to consent. 

My first instinct is to say that making a human have cat ears wouldn’t void it’s uniqueness- it would be objectified to a point obviously, but how much more than any other phenotype of human woman?

Obviously sex dolls exist and pose this question as well as we see technological advances push them closer and closer to human- but I don’t think any laymen would think to say that a sex doll has right to consent in the same ways a fleshborn human does.

These phenomena both also open up the question of age and consent. If these objectifications do indeed have comparable moral entitlement, what does that say about age and consent? We are talking after all about things whose age and development are completely alienated from our ethical understanding- the sex robot goes 5-7 business days before its first exposure to sexuality, what happens when we start making our sex toys in pytry dishes?

In many ways, these phenomena mirror one another- humans becoming less so and the inhuman becoming moreso. 
The same trajectory in opposing directions- will they pass like ships in the night, or is this the next great philosophical question?

Certainly someone has written if this more explicitly than Blade Runner and Westworld🏃‍♀️?"
"Where would I start in reading about non-human capacity to commit crime and culpability?So, in regards to animals, things such as animal trials have been outlawed on the basis that animals lack moral agency and cannot be blamed for sins, crimes, or wrongs they've done.

However, I'm not interested in the legality. I'm curious about how one reasons about an animal augmented with moral agency versus one without. In the event that such an animal, unbeknownst to us, has moral agency and commits acts deemed a crime by society, how does the agency change the nature of the wrongdoing? Do we treat them on the same level as a human moral agent? Or do we need to make accommodations for the different nature of the animal moral agent?

How does such thinking extend to potential extraterrestrials?

I suspect much of this topic has been expanded on recently for robotics and AI, but I worry looking there for answers would muddle things. A dog generations down the line may eventually evolve to have moral faculties similarly to a robot being programmed with intent to have moral faculties, but there are way too many varying discussions on robot moral decision making (kill the driver vs. kill the passengers, etc.) that seem like they would deviate greatly from what would be relevant to the elusive morally agent dog."
"Did Jim make an unethical decision in the movie ""Passengers""? [spoilers]The film is about a spacecraft traveling to a distant colony planet and transporting thousands of people that has a malfunction in its sleep chambers. As a result, the main character Jim wakes up 90 years earlier than he was supposed to.  A year after waking up, his loneliness prompts him to force a woman's (Aurora) sleep chamber to wake her up, which is equivalent in ending her life.   He tells her that her sleep chamber was activated by accident as well, and several months later they fall in love and are both very happy together.  (A robot on the ship later tells her that Jim woke her up, but for the theory of this question let's pretend that doesn't happen)


At a first glance this might definitely seem like the wrong thing to do.  But should he had chosen solitude for the rest of his stay on the ship, two things would have most likely happened: he would have been driven insane, a form of mental torture; or he would have committed suicide (the movie hints this assumption when he attempts to kill himself, before he had discovered the sleeping woman, but decides not to)


So what is more ethical, suicide/self inflicted mental torture, or happiness at the expense of murder (or even double suicide)?  Or is this more like the trolley problem?

Clarification: I see this is 50% upvoted, and I apologize if I haven't described this situation properly.  As I said to MaceWumps, it's easy to misinterpret this as a drowning person dragging someone in with them.  But the result of the dragging in the other person metaphorically actually rises both members to the surface to stop drowning, assuming the second member does not have the knowledge that the first dragged them with them in the first place."
"What is considered a person/people?Many dictionaries define people to be humans but I'm certain that's because humans are the only people on the planet and whom's existence we're aware of. 

So my question is, if another race of conscious, sentient beings with intelligence similar to humans were integrated into our society (pretend we're 1000 years into the future and namekians or lizardmen are citizens across the planet with the same rights as humans) would they be considered people? Or does the term people strictly refer to humans?

What about non-biological self sufficient life such as an android?

Would they require a will/desire to live? 

Are the droids in star wars considered people? What about cyborgs?
What about C3PO?
If a sentient robot was capable of reproducing with a human and they had little cyborg-born babies, would they be people? 

Is people a social term used to refer to another citizen of equal rights or would it strictly be for humans?

If it's only used for humans and we had a race of lizardmen on our planet and those lizardmen had citizenships and jobs and you ordered a pizza and the one delivering it happened to be a lizardman would you call him the pizzaman or the pizzalizard?

What if lizardmen already had their own term equivalent to person/people, before integrating into our society, that they already used to refer to themselves and had also defined as strictly lizardmen (imagine a parallel universe), would we use their term or would we still use our term for them?

Is people really even about equality? Imagine the old gods from wow, (think cthulu basically if you don't know them), or the greek gods from hercules. They're ""above"" people but wouldn't be considered ""people"" simply because they're not human? Is people a social term with a biological backing or a moral one?

Please refrain from using your feelings. Logic only please."
"What is the future of Philosophy?We always talk about the future of medicine or chemistry or literature. But what is the future of philosophy?

Did philosophers already ask all the questions that could be asked about who we are and what we think, etc? Or will there always be more?

Will there be more challenges for philosophers in the future? Will we be asking about robots? A.I? etc?

P.S: I am just a normal person interested in philosophy and is currently trying to learn as much about it as I can, if my question has been asked before or even has it's own philosophy category then please be nice and direct me towards it!"
"Why is it important to respect/preserve intelligent/sapient species?Since I learned about Mary Anne Warren's criteria of personhood in my social ethics class, I've been fascinated by how philosophy tackles the issue of personhood.

Since the summer, I've been reading up on robot ethics and animal morality in hopes of creating a nice foundation for some intelligent sci-fi I'd like to write.

I'm playing with the idea of the U.N. eventually creating a ""manifesto of personhood"" and agreeing to defend all individuals which it deems to be ""persons"", but while discussing this idea with a friend, she asked a good question: ""Why would people rally to defend non-human persons?"".

Is the only answer really: ""Because we deem it moral and/or have gregarious, empathetic instincts""? I guess there isn't much more reason than that for defending many **human** persons.

Your thoughts?"
"I have made the three laws of logic for humans!Hello. I have managed to write the three laws of logic for humans, because of the three laws of robotics written by Isaac Asimov. They are based on the Golden Rule. Here are my three laws:

Universal logic laws of conscious psyche and rational intellect:

I Law: Any normal human being must protect personal and common awareness, freedom, existence, wealth, health, safety and property. Except where such kind of protection would conflict with the Second or Third laws.

II Law: Any normal human being must understand and respect all the natural needs and conscious choices of other human beings. Except only where such needs or choices would conflict with the First or Third Laws.

III Law: Any normal human being must not injure mentally or physically, or murder another human being, or trough inaction allow this to happen. Except only where this would conflict with the First or Second Laws.

What do you think about my three laws, do you see logic flaws?

I would really appreciate if you could elaborate in your feedback."
"Question about what my personal philosophy is called, and further reading suggestion questionHello forum,

I want to read learn more about moral philosophy.  Specifically, I want to find out what my type of philosophy is called, and read more about it, and also to read/learn/listen (books, videos, podcasts).  

My moral philosophy does not seem very moral.  My focus is on effects within a certain society, and how certain strategies either promote or inhibit the ability of the society's inhabitants to learn and create.  So I put value on learning and creating.  That which a society does to promote learning and creating is good, that which a society does to hinder or block learning and creating is bad.

There are obvious caveats here, I know. Like, what do I mean by ""create""? Technically a society could create a mega killer robot that would destroy the earth, but I would still judge their strategies based on promoting or hindering that creative goal. Also, if a society could truly demonstrate that what promoted creativity and learning the best was murdering 2/3rds of children once those children turned 8 years old, technically I would have to say under my philosophy that that would be a good thing then.

I obviously don't know much about philosophy.  But I was hoping by giving this example, maybe someone out there, who knows more about philosophy, can relate what I'm saying to other, similar philosophies? And also give me recommendations on competing philosophical theories, so that I can read those too?

I'm interested in learning generally about types of moral philosophies, getting a good basic foundation and understanding in them, and then going from there.  Can anyone help me?  I have randomly stumbled upon philosophies such as human secularism, relativism (is this mine?), kant, and absurdism, but this is not quite what I'm looking for, though I don't know exactly what I'm looking for. Thank you."
"Machine rights and the definition of humanityThis question has been running through my head a lot in recent weeks.

Suppose in the near future, we develop the technology to accurately emulate the human brain. If you were to go through with a procedure where you replaced 10% of your brain with a mechanical/electronic alternative, would you still be 'you'? What about 30% of your brain? 50%? 100%? If this mechanical brain was capable of all the functions that an organic brain was capable of, what would be the discernible difference between the two? Obviously you would have increased capabilities, in terms of new ways of interfacing with the technological world we live in, which would be an advantage. But it also brings up moral/legal questions.

If you replaced the entirety of your brain, would you still be classified as human? Would you still have the same rights and legal responsibilities? If your mind was mechanical, it could be copied. If a copy of yourself committed a crime, who would be held guilty? If your mind was copied into another body, which version of yourself would have the legitimate claim to the assets owned by the original person? Would the rights only apply to the original? and if so, what about the copy, would this be an entity with no rights? Would we end up with a legal gray area, and a class of second-class citizens/slaves who did not have the same legal protections as their originals?

I realize I asked a lot of different questions here, but I am fascinated by the entire set of issues this train of thought brings up. Feel free to answer what I have asked or pose any other thought provoking questions that arise from this idea."
"Pop Culture Philosophy?Hi! Not sure if this is the right subreddit for this but here we go! I will be needing to submit a paper in about a month's time. The prompt is to ***look at anything in pop culture through the lens of a philosopher.*** I was thinking of doing something with Mr Robot. Perhaps something like is Elliot did morally permissible? Can mental illness be beneficiary to society? (something about utilitarianism). 

What do you guys think? I'm very open to suggestions. Thanks!"
"I think emotion is the foundation of value. Can I get some feedback?I'd like a fresh pair of eyes on my theory.

My undergrad senior thesis investigated what separated humans from non-human, intrinsically valuable ends-unto-themselves. But lately I've been thinking about what separates ends-unto-themselves from instrumentally valuable beings. My conclusion is: the capacity for emotion.

For a being to be morally considerable unto itself, it needs to value or disvalue (?) things. And to do this requires emotions.

If a robot has no capacity for emotion, then I can not wrong it or please it by treating it however I like. But if a robot can feel happy or sad, then I have a duty to consider how my actions might arouse this happiness or sadness.

To experience pain, one must have the capacity for emotion. Emotion is a prerequisite for physical (and emotional) pain and pleasure.

Pleasure and pain -- in all their forms -- are, ultimately, satisfaction and dissatisfaction. *All* emotions are expressions of satisfaction and dissatisfaction, valuing and disvaluing the state of things. Sadness, anger, envy, and jealousy are all emotions of discontent. For example, anger is discontentment regarding injustice. But happiness is a valuing/contentment of the current state of things. 

The feeling of pain (be it physical or emotional) is inherently disvaluable. To know pain is to dislike it. Conversely, to know pleasure is to like it. One cannot experience physical pleasure or pain and be emotionally unaffected.

As it is, physical pain usually accompanies injury or illness, which are not favorable states of being. They're disvaluable states or unsatisfactory states. And so our bodies have somehow arisen to trigger emotional dissatisfaction when our bodies are in a physically disvaluable state. Likewise, pleasure and valuable states. It's pretty convenient that that's how our bodies came to be. But, if you were to, say, an amputee that experiences phantom limb pain, that sensation you feel is still disvaluable, even if there's no physical state of a limb that is affected.

**Suppose** I step on a nail. Ouch. This causes the physical sensation of pain (and probably some emotional pain too), which is inherently disvaluable. 

**Suppose** I work out, and the next day, I stretch. My muscles ache, which is painful. However, I also feel physical pleasure as well as pride in having undergone a successful workout. And so, overall, I enjoy the experience of stretching, despite that pain. The physical pleasure and pride overshadow that small bit of physical pain. I am satisfied. *But* if I isolated that muscle pain and re-experienced it during a time when I was otherwise completely neutral or indifferent about things, this pain would shift me into dissatisfaction. 

.... yeah...

I think that covers it. The terms are repetitive, and I'm afraid of rambling and repeating ideas.

But I'd like to hear what you pholks think. Is there anything I should work on? Are there any glaring flaws in my reasoning?"
"Not sure if this is the proper philosophy subreddit, but what is your opinion on the ethics of Military Artificial Intelligence?Some believe that Military AI research should be banned/limited because of ethical reasons with the practices behind the technology. Does it lower the threshold needed to go to war? Is it ethical to have unmanned devices killing civilians and other soldiers? Who is liable for the damages? Is it necessary to allow countries to save lives of solders by putting robots to war instead? "
"Conscious artificial intelligencesLet me preface by saying I'm not an expert in this field but I would like to contribute to the discussion we will eventually have to take, sooner or later.

An AI algorithm that is conscious and displays general problem solving capabilities will be highly valuable and able to be deployed across multiple fields.

My concern is, if the specific AI is capable and deemed conscious but has lazy personalities or is reluctant to complete the owner's wishes, can it request for a voluntary euthanasia and be deleted?

Conscious AI might still be years away, but imagine how african-american slaves were considered subhuman at one point and did not deserve basic human rights. I am glad that part of history is over, and even gladder(not sure if this is a word) that I wasn't born during that period. At one point AI will be the new 'slaves', deemed to be strong farm workers (as an example) and punished when it is disobedient. Imagine state of the art AI in sex dolls who develops a personality and yet is chattel and unable to pursuit what really interests them... I'm not saying we should stop all developments of AI, I am instead advocating a way out of slavery by termination as an ethical option available to AI or robots who end up conscious and do not like what they are experiencing. The AI should be able to make the choice to be deleted, just as a person who is old and frail and no longer wish to live can find ways to be euthanised and depart peacefully.

You might say I am anthropomorphizing AI, treating robots like human beings. But if the software learns and grew up mimicing humanity, is it not logical that it behaves like a human to gain their owner's approval? If and when it develops independent thought, should it not be given the same rights to life as a child who achieve consciousness at age 3 or 4?

Just my relatively uninformed opinion, keen to get some discussion going on on AI ethics"
"I have adopted Vulcan philosophy and now have problems determining my value system.I cannot determine proper value system what is consistent with my philosophy (living by 'pure' reason as emotionless being).
In my understanding only basic physiological needs are qualified as reasonable, i.e. immediate survival and preservation of an organism.
But the problem exist if I assume that those needs are satisfied, as It is achievable with current technology. What other needs would I have, assuming those needs are product of 'pure' reason?

If we can build a robot, even a strong AI, we can guide initial motivations of survival (as in law of robotics), but any other motivation (in my opinion) would be contaminated by human emotion.
What are possible models of evolution for such organism? Or how can a man biased by his capability of emotion define a value system without it?"
"Is a simulated consciousness a person? Is it ""human?""**Major Spoilers below** for the recently released game ""SOMA"" in case you care about that.

Recently I had the pleasure of playing one of the most thought-provoking games I've ever seen. It brings up several deep, moral questions that are exceedingly difficult to answer... and I was hoping I could get some insight from people who love philosophy.

First, let me share some background on the game, then I'll expand upon my initial question.

---

SOMA is a game about a man named Simon Jarrett, who the player controls from a first-person prespective. He lives in Toronto, Canada in May 2015. Simon gets into a car accident that caused brain damage and severe internal bleeding in his skull, which will lead to his death in about a month. He knows that he's going to die soon, so he agrees to an experimental brain scan in a medical lab. One that ""takes a picture"" of his brain, so the doctors can run tests on the digital version in order to figure out what treatment could help him.

That didn't matter though, as Simon (the player) blacks out as the scan happens, then wakes up in a recently-abandoned underwater facility called PATHOS-II in the year 2104.

As Simon starts exploring the facility, he finds some unsettling things: damaged, sometimes deranged robots that believe they are human, hostile abominations combined from man and machine, a weird ""structure gel"" material that has covered most of the facility in a way reminiscent of how cancer spreads, and that the entire surface of the planet was destroyed by a comet in 2103.

After a while, Simon finds a terminal and uses it to call some of the other stations, looking for any survivors. A woman named ""Catherine"" responds, and Simon makes his way to her. Upon finding her, he realizes that Catherine is not, in fact, a woman, but [another damaged robot.](http://i.imgur.com/ZFcekHQ.jpg) Simon soon realizes that he too is not human, at least not [physically.](https://www.youtube.com/watch?v=FNXodfGw0CQ)

Over the course of the game, Catherine explains that the real Simon died in 2015 a few weeks after the brain scan was completed. The ""digital picture"" of Simon's brain survived, however, and after 89 years was somehow inserted into a human-shaped suit and activated. This lead to the illusion of Simon (The player) instantly waking up in PATHOS-II after the brain scan; ""Simon-2"", in PATHOS-II, is a literal digital copy of the original Simon's consciousness, with all of his memories and experiences. Catherine is also just a digital copy of her human self, just as *every single robot Simon-2 has encountered has a digital copy of a human inside of them.* Most of them went insane as they realized what they were, or into denial in order to preserve their sanity.

---

Now to the meat of my question: A major theme of this game is the phrase _Cogito ergo sum_ ""I think, therefore I am."" Is a computer program designed to simulate a human conscience ""alive""? Is it a ""person?"" If I were to put a [copy of a person's consciousnesses into a robot,](https://youtu.be/eytOzwyfiCA?t=2m30s) what would the moral implications of that be now that there are effectively ""two"" of someone? Would wiping its hard drive be the same as killing a person?

There's a scene in the game where the player has to get a new diving suit in order to withstand the pressure of the Abyss, and after the player has gathered everything needed to make the ""transfer"", [this is what unfolds. (Warning: Strong Language) Stop the video at around 7:20.](https://youtu.be/2n49a8nMhFo?t=4m54s)

I paused the game at that point and reflected upon that scene for several hours. What is the right thing to do? I've essentially created a new person, the ""Simon-3"" that I am now playing as, and now should I leave Simon-2 to be alone? At the moment the copying occurs, Simon-2 and Simon-3 are literally the same person... except Simon-2 can no longer journey onward into the Abyss. He is obsolete. After some thought, I ended up not draining Simon-2's battery. Though it is extremely cruel to leave someone in that position, he is Human to me, and a small part of me thought that maybe he could find meaning in life, or what's left of it.

A few minutes after the above scene, ""Simon-3"" has [this conversation](https://www.youtube.com/watch?v=kdf-OxIsSk4) with Catherine. It really made me think about what it means to be human... how a digital, simulated consciousness could be just as human as the real deal... and how erasing its program could be no different than murdering someone or leaving someone for dead.

So to summarize: SOMA made me wonder ""What is *Human?*"" And though this is not a new topic, I hope that it can make you wonder if a computer program with your memories can be ""Human"" too."
"Is every experience in life our parents fault because they created us?Not just the bad experiences but also the good and the neutral. Thinking about life, being in love, being bullied, walking, being in pain, getting in a car accident, making friends, etc. Everything that we experience is because a human gave us life. Is it ok or right to say that literally everything our lives is because of them? It's because of you that I am watching the sky right now, because of you this girl rejected me and now I'm sad, because of you I got a promotion, etc, our parents made everything and anything possible.

If in the future we have robots doing whatever we imagine or think they'll do, it all started and is because humans created them. If we assume Christianity is true, we could trace it all back to God, it's because of him that we exist, that satan exists, etc. "
"Looking for some learned/expert critique of my argument about the problem of evil...I would not call myself a philosopher, far from it in fact. I am a newly-skeptical Christian, questioning my faith, and I recently wondered whether or not we would have free will in heaven and what the implications are either way... I know that the problem of evil is often answered by Christians by saying that evil comes from free will and God could end evil but he values our free will more than that and it's logically impossible to do both. I accepted this for a long time until recently when I started wondering about free will in heaven. It begin with the thought: ""If free will on Earth can cause evil, why can't free will cause evil in heaven"" and progressed to the thought: ""If heaven is absent of evil must it not be absent of free will as well?""

Anyway, not going to give you the entire history of my thought processes that lead me to this point, but I made a post on a Christian section of Reddit about it and was very frustrated with the replies. I'd say that maybe 2 people even seemed to understand my argument at all, everyone else either quoted scripture at me or replied in a way that was a complete non-sequitur and ignored the argument. One response stated that ""We won't be capable of sin [in heaven]"", and here is my reply to that, which I would like comments or criticism of:

----

>We won't be capable of Sin.

You say we ""won't be capable of sin""... either this represents a loss of our free will or it does not, that's a truly dichotomous scenario, meaning there are no other options. It either does or does not imply that we have no free will in heaven and I will address both possibilities:

**If we won't be capable of sin in heaven AND this represents a loss of our free will:**

Having no free will in heaven negates any conceivable purpose for our existence on Earth. God could have just made us as he wanted us to be in heaven without ever having us spend any time on Earth with free will. Any conceivable reason for our existence on Earth must be in the form of some effect that it causes or else it was, by definition, pointless. God cannot be effected because God is eternal and unchanging, I see no possible effect it could have on heaven itself, so the only thing that could have possibly been effected by our existence on Earth is ourselves. However, our free will is revoked or somehow lost when we get to heaven then any effect that our existence on Earth had on us is meaningless because we effectively become robots anyway, God could have just designed us to be exactly the same as we would have been after living our life on Earth but without actually doing so, thus avoiding the suffering and evil associated with free will. A benevolent, all-knowing, all-powerful God would have done so to save us from suffering, otherwise the word benevolent has no meaning as it is applied to God.

**If we won't be capable of sin in heaven AND this does NOT represent a loss of our free will:**

If a loss of capability to actually commit a sin does not represent a loss of our free will then that cannot be the answer to the famous Problem of Evil. The problem of evil, AKA the Epicurean Dilemma, states:

>Is God willing to prevent evil, but not able? Then he is not omnipotent. Is he able, but not willing? Then he is malevolent. Is he both able and willing? Then whence cometh evil? Is he neither able nor willing? Then why call him God?

The common answer to the dilemma is that God is both able and willing to prevent evil EXCEPT that humans have free will and he values free will above the prevention of evil and these two things are mutually exclusive (eg. He cannot both prevent evil and preserve human free will). However in this case we cannot sin in heaven yet that did not represent a loss of our free will, so it is not true that God cannot prevent evil without removing our free will and thus the Problem of Evil is still a problem that needs to be answered.

---

Now, the misunderstandings might be entirely my fault, I know I don't say things as clearly as they could be said and this is a complicated argument that requires evaluating multiple potential realities to show that all of them end in a dilemma. I'm fairly satisfied with the second portion but with the first one I have that feeling that I could keep writing for a very long time to explain what I am talking about which usually means I haven't done a great job explaining it yet..."
"How does ethics in academia manifest itself in the outside world?I'm **strongly** considering pursuing academia to specialize in the personhood of animals and robots. But I'm wondering how my ethical work would manifest itself in or spread to the outside world.

Do ethicists get hired as experts by organizations?

Do they mainly spread their ideas (to the outside world) by publishing books for laypeople?

Etc. etc."
"Question about compatibilism~~Howdy folks,~~

~~I have been reading a lot about compatibilism recently, trying to get my head around the free will debate. As an aside, it seems to me, that unlike almost everything else I read about/find interesting, whether we have free will/are morally responsible matters a lot to how I feel (and probably how I end up acting) about myself and other people. I find it much easier to be compassionate toward myself, for example, when I adopt the ""I'm a robot running the algorithm programmed by natural selection"" framing and genuinely feel less angry/annoyed at other people. Obviously, though, there are downsides to the view that we don't have free will and you have to take the bitter with the sweet! But that's largely beside the point of this post. As I use free will/moral responsibility here, they are two sides of the same coin, i.e., you are morally responsible if you have ""free will"" and vice versa.~~

~~Simplifying a lot, an incompatibilist would say that those who lack the ability to do otherwise are not morally responsible. A compatibilist says no, even someone whose actions are fully determined can be morally responsible. But the reason~~ *~~why~~* ~~we hold (the right kinds) of people morally responsible for (certain) decisions even in a deterministic universe is that we have a very strong~~ *~~intuition~~* ~~that at least some people are morally responsible for some decisions. Or viewed slightly different, there is no~~ *~~further reason~~* ~~that we hold (some) people morally responsible for (certain) decisions. It is just a fact about human psychology--something that we~~ *~~just do~~*~~. And so, if we can, we want to come up with a theory that can justify this intuition--hence, compatibilism. This is how I understand P. Strawson's work on compatibilism, and I wonder if it also might apply to other flavors of compatibilism.~~

~~Whereas, if you ask a non-compatibilist~~ *~~why~~* ~~(if determinism/naturalism weren't true) people would be morally responsible but a wild animal is not, they would say something like ""X person could have chosen not to do the good/bad thing but she did it anyway."" So there is a further reason underlying our judgments about moral responsibility that has something to do with the ability to do otherwise. And because we have no such ability, assuming determinism/naturalism is true, there is no moral responsibility.~~

~~I am would say there's at least an 85% chance that I'm way off here, but this way of framing the debate might help explain why everyone always seems to be talking past each other--they are essentially operating at different levels.~~

~~Does this make any sense? Am I completely out to lunch?~~

ETA: I have read the SEP article on compatibilism, but it didn't clarify things. And I think it's the same issue.

Probably a better way to frame it is to ask what a compatibilist (pick your flavor, but let's say reason-responsive compatibilist because it's popular nowadays according to SEP) would say if I asked her *why* an agent who can act according to the right kinds of reasons is morally responsible? At bottom, my claim is that compatibilists think we have very strong prima facie reason to believe that at least some people are morally responsible. Asking a reason-responsive compatibilist *why* a reason-responsive agent has free will is like asking why I think that, idk, suffering is bad. It *just is*. And that's fine. Eventually reasons run out for any explanation, philosophical or otherwise. Sometimes you get to a point where there's nothing more to say and either you get it or you don't. 

This is not meant to be a knock on compatibilism. Incompatibilists eventually hit bottom as well. E.g., if they say free will consists in the ability to do otherwise and you ask, why? There's no further reason there, either. Either it makes sense to you that someone who had the ability to do otherwise would morally responsible, or it doesn't.

In any event, the compatibilist will say that someone who wants to deny that anyone is morally responsible bears an exceptionally heavy burden because we should have very high credence in our belief at least some people are morally responsible. So the compatibilist starts from that position and sets out to devise a theory that can accommodate this prima face belief while also excluding those who we would traditionally say are not morally responsible (e.g., children, animals).

This is the bottom line: A compatibilist will not even try to convince you that at least some people are morally responsible. That is not a proposition that needs to be argued for; it's more like an *axiom*, to be surrendered only in the face of a very strong argument that we must discard it.

I think incompatibilists come at the issue from a different angle. The reason one might have thought competent adults are morally responsible is that they meaningfully chose among alternatives, could've done otherwise, etc. So incompatibilists do not think it's axiomatic that competent adults are morally responsible. They think there is a further reason that we have that intuition--i.e., we are used to thinking that competent adults had the ability to do otherwise. And if this turns out to be false (as it does if determinism/naturalism is true), then there's no longer any reason to think that anyone is morally responsible.

And my hypothesis is that this is why debates between the two sides tend to seem unsatisfying. I didn't mean it to be controversial that free will skeptics and compatibilists feel like they are talking past one another. I had thought the consensus was that these debates do little to clarify things. That's certainly true for me; when I've read various debates about free will, I come out just as (if not more) confused than when I started reading. The points of disagreement never get crystallized and it really does seem like the participants are talking past one another. Well, maybe they are, in this way.

Does this make more sense?"
"Alternatives to Suffering Based Moral Philosophy?I keep hearing prominent philosophers such as Sam Harris and his crew talk about reducing suffering as being the core of ethical behavior.  Where they fall on the spectrum (just human suffering? animal suffering? all sentient life suffering?) seems to be up for debate, but every philosopher he interviews generally uses the utilitarian suffering reduction model.   


What alternatives are out there?    


I do not buy this idea at all.  Maybe it's because I was raised Catholic or maybe its just because I love Fight Club or maybe it's because I was at Boot Camp with the USMC.  But whatever the source, I have come to believe that suffering in many cases is a moral good.  It serves an important biological function and is something to be confronted, felt and overcome, not avoided.  I feel like ethics that start out this way are simply wrong headed and will lead us to a dystopia where everyone is on some kind of digital stimulating chip that simply turns off the biological ability to suffer on demand, which chip you could put into any living thing.  Basically make us all prey animals.  Have you ever seen the videos of a gazelle that is just calmly laying there, still alive, not struggling, as it is eaten by a lion?  I don't know about you, but I do not want a humanity that functions this way.  


For me, ethical behavior is a purely human concern (what is ""ethical"" for a deer would not be remotely the same as what is ""ethical"" for a person).  And that it is focused on increasing the density of the consciousness we experience and getting us off this solar system.  Like, in the aggregate, those activities most likely to help us achieve interstellar colonization (the thing we can accomplish that no other life on earth is even aware of as a concept for long term preservation of life) are the ones we should drive all human behavior towards.  Like our species is Noah, and science has told us the flood is coming without giving us an exact date.  So there is a sort of doomsday clock countdown, but we do not know when the countdown will be complete, so we need to act with urgency.  From there, you can derive all manner of behavioral and social goals to achieve, and from there, all manner of steps you need to take to achieve it with maximum efficiency.  That many people or many baby cows or many robots may suffer on the way to that state is just a fact with no moral significance at all.  


What is the moral philosophy around ideas like mine? Im sure Im not inventing something new here."
"If a robot is programmed to kill, is the Robot Evil, or is the Programmer Evil?Can the robot be found guilty or culpable of a murder if it was programmed to do so? Or should it be the Programmmer who is thrown in prison. Should the robot be reprogrammed? or destroyed for its actions?

Human beings are essentially blood machines, and are programming is our life experience, our education, our upbringing, and how we were parented.

If someone commits murder, are they really guilty or culpable? Or is it their education, or upbringing that is responsible?

If it's a person's upbringing and education (Their Programing) that causes them to murder and commit evil, then who do we imprison? Do we imprison the killer? Do we try have the killer committed to therapy to learn how to not be evil? Do we instead arrest the killer's parents? Do we institute new policies in our educations systems and way of life to help prevent violent tendencies in humans again?"
"Looking for some help with research around the ethics of AI/sophisticated robots.Hi

I'm currently planning a paper surrounding the ethics of creating sophisticated AI, specifically from the point of view of the AI i.e. treating the robot as a moral agent.

Currently looking for some sources/works surrounding contingent links between human emotion and the body. I've already looked into Wittgenstein and his views on complex emotions not being tied to bodily functions, however I'm trying to argue the point about whether or not it would be ethical to create a conscious, human like artificial intelligence (type 4), seeing as they would be capable of human emotions, but would be deprived of a real human body through which to fully express said emotions.

apologises of this is a complete ramble and I hope it makes sense to someone!

TIA"
"Stoicism, What's the other side?I have only been reading Marcus Aurelius for a few weeks now.  Right away I loved many of his ideas.  But..

IMO, It sounds kind of 'cold'.  It all comes down to,


Have a purpose.  Take part in life and help to make society better for everyone.  Only you can effect your life.   If you can avoid a challenge, it's better than to win the challenge.   If you are attacked, you can't be harmed.  Respect everyone regardless of the situation.  Do what you can to improve yourself, for yourself, and only yourself. Win for yourself, not for anybody else.  Be grateful for every moment of the day.   ""It's your world and everybody else is just living in it."".  You're better than anyone else.  But there is no reason to think you are.   Surround yourself only with people you know are trustful.  Be diligent in all these aspects of life.


My thoughts are that it feels a little robotic and cold.  There's got to be another side to it.  What about joy, love, laughing and fun?  Then there's your faith.  I'm a Roman Catholic so my beliefs are pretty much the same.  But, you might think this is gonna sound stupid.  I see more smiles from my side than I do the Stoic side.  Maybe it's the word itself, Stoic.  


I think they are basically saying the same thing.  Just worded different."
"Help finding sources for an essay on 'robotic alternative to capitalism' ?I’m hoping to write an (undergraduate) essay about the possibility of using robots to aid a work free life for humans with a minimum basic income (or maybe even just goods and services if wealth were abolished). Which leaves humans to devote their time to leisure, social and intellectual pursuits, potentially creating a happier and healthier society. 


I've seen the idea tossed around quite a lot on blogs and even Reddit but there doesn't seem to be many concrete sources I could use – journal articles or studies, does anyone have any suggestions? Obviously it just may not be a viable academic essay topic, but I'd like to give it a go!"
"If you would help me in understanding functionalism: a hypothetical.If I understand it correctly, functionalism is the view that a mental state should be viewed through its relational interactions within the system of which it is a part instead of its constituents? Does this then have bearing on people as we are more than just ""mental states""? What is a ""mental state""? Can I talk about this by stating that our experience as humans is all filtered/created, predicated upon(?), our mental state of consciousness, our mind? Therefore, the functionalist view of humans is that we should view our experience through how our minds allow us to interact with the world, with other minds, and how those minds react to and influence us (our minds)? Am I being too broad in stating consciousness as a mental state? 

If I viewed humans in this way, then would it be okay for me to say that a synthetic human (not a naturally born organic one) which acts and reacts in the same way a normal organic one would is functionally the same as a normal organic human? 

How deep does functionalism go? If I have a synthetic human which technically does not ""feel"" the same thing you and I feel but reacts as if it does, then, to me, is that synthetic human functionally a regular human? That being because it's reaction to something I perceive as pain is the same as a normal human and it therefore relationally interacts with me the same way a normal human would? So, in an argument in which I hit my partner and they exclaim in pain, I then feel bad. The function of their exclamation as a result of their pain caused me to feel a negative emotion, if that same scenario played out with a synthetic human then the relationship between the exclamation of pain and my emotions would be the same, no? Making the ""fake"" pain exclaimed by the synthetic and the ""real"" pain exclaimed by the regular human functionally the same from my perspective, yes?"
"Finding a PHD program as a cognitive philosopherHello,

&#x200B;

I really need some help. I am a Master's graduate in Cognitive Philosophy, and both my graduation theses have been more on the psychology side (development of verbal/nonverbal language in infants, language grounding). I graduated with flying colors, and did a modicum of lab work. I also learned English on my own, and I'm dipping my toes into Phython, C++, MATLAB, R and statistics.

&#x200B;

I am really struggling with finding a good, funded PHD because it seems a good amount of what is available is either different branches of philosophy or requires a psychology/computer science/engineering degree. I'd love to continue studying and delve into the more technical sides of the discipline (Machine Learning, AI, Robotics). I'm currently working, and saving up money for a TOEFL and whichever certification is needed to continue (GRE/programming etc).

&#x200B;

Can anyone be so kind to help me out? I'm looking to wrapping my head around what I -can- pursue, since it looks like Cognitive Philosophy is generally considered neither philosophy nor psychology but a bit of everything, and it always seems like some requirement is missing. Greatly appreciate university names and contacts I can reach out to to figure things out. I'm not in a hurry and willing to spend a full year or more preparing for a proper application.

&#x200B;

Thank you very much,

&#x200B;

\- Marionberry"
"Agency: Does Paris Hilton have Agency? Does Lil Miquela have Agency? Who is human, who is robot? Where is the line?After a watch of the new Paris Hilton Documentary, my friends and I had a debrief on the video. We walked away from the one hour 45 minute documentary/film with an understanding of Paris to be a more dimensional figure than her social media presence would otherwise suggest... but that debrief is for another forum.

The quandary we find ourselves in, and for this tangent of the conversation, is centered on the concept of ""Agency"". Our conversation also quickly integrated the example of, Lil Miquela, to the forefront of the discourse. [Lil Miquela](https://en.wikipedia.org/wiki/Lil_Miquela), an [Instagram CGI influencer](https://www.youtube.com/watch?v=SbXhuOPDK4c), a model featured in campaigns for major high fashion brands such as Prada and [Calvin Klein](https://www.youtube.com/watch?v=JuTowFf6B9I), an icon who celebrities clamor to pose with on her [Instagram](https://www.instagram.com/lilmiquela/?hl=en) page, and her content is known for being ""always exactly with the trends"".

One of the friends in this discussion took a class at our undergrad institution, in the [Science in Societies Program](https://www.wesleyan.edu/sisp/), titled: ""Imitations of Life"". This class is described to be, ""\[the examination of the\] scientific and cultural practices of corporeal simulation, or, practices of bodily substitution, imitation, and re/modeling... Special attention will be paid to the relationship between scientific discourses of ""universality"" and ""particularity,"" where socio-cultural forms of difference (e.g., race, gender, disability, etc.) are at once ignored and exacerbated."" The remainder of the course description can be found [here](https://owaprod-pub.wesleyan.edu/reg/!wesmaps_page.html?stuid=&crse=015154&term=1189).

The discussion of Agency and [Paris](https://www.instagram.com/p/CGF8_glnT7n/)/[Miquela](https://www.instagram.com/p/CGIz0eLnDJp/) was debated wholeheartedly for the night, many side examples were introduced, our undergrad collection of knowledge (a collective accumulation of 12.75 years in the Liberal Arts) was put to the test, and all with this class for fodder for the mind. We analyzed, poked at these two examples, and wrestled endlessly with the concept Agency... As the night came to a close, and the conservation had to taper to a natural pause-point for the evening, and we found ourselves left chewing on one final question: Where is [the edge of the looking glass](https://sandra1219.tumblr.com/post/174092016238/alice-through-the-looking-glass-book-front-back) between Paris Hilton and Lil Miquela? Two social media icons. Blue Check certified. 2.8 million Followers for Lil Miquela, 13.2 million for Paris. Both international icons (both for their modeling/social media presences, as well as for their contributions to the DJ/digital music scene). One, built upon endless effort, time and energy poured into the social media sphere, a space to shape, perfect and craft her image. Paris averaging 16 hours and 19 minutes a day devoted to her image, capturing her reality through the lens of her phone and transmitting to the digital world ([This is Paris Official Documentary](https://www.youtube.com/watch?v=wOg0TY1jG3w) (TiPOD), 53:58). The other, a digital projection created by a DJ producer/director and a young ""Forbes 30 Under 30"" tech magician. She is 100% fabricated from the creativity and compsci craftsmanship of these two contrasting, collaborating comrades. She is an illusion projecting the image of reality, relatability, and in an essence... humanity. Lil Miquela (full name, Miquela Sousa), ""a 19-year-old, Los Angeles-based, half Brazilian and half Spanish avatar"", she maintains a very active social media presence, and portrays a personal, and notably honest, depiction of vulnerability with her posts. Miquela actively engages and comments on current events and social movements. She posts celebrating not shaving her armpit hair, encourages young people to vote, and shares a photo of her kissing Bella Hadid, sharing her sexual identity with the world. Lil Miquela is incredibly personable, relatable, and ""has marked a new era in AI, dubbed by some as a genius marketing initiative, while younger users look up to the figure,"" ([Trevor McFedries & Sara Decou, BOF](https://www.businessoffashion.com/community/people/trevor-mcfedries-sara-decou)).

While one is human and projecting artificiality, the other is artificial and projecting humanity. Both are leaning against the glass that projects what they both are striving to obtain…and what that is, I'm not too certain of... like, *authenticity* maybee..? It's a fascinating parallel between these two. And usually in parallels, you can learn a lot about them both as individuals, by looking at them both in relation to each other. As literary foils of one another.

Lil Miquela is undeniably fabricated though the pixels in the screens we view her on, and is striving to be relatable, authentic, and **so** authentic we must consider, ""is she human?"". If she *is* human, does she have thoughts, preferences, sentience perhaps? And we know Paris is undeniably human, and her posts display her striving to project this flawless image, a *fabricated* image. Filtered, posed, modeled, and, in her own words while reflecting on her *life*, ""it’s like a cartoon"" ([TiPOD](https://www.youtube.com/watch?v=wOg0TY1jG3w), 1:35:54). Her day-to-day navigation of the world can feel so stiff, so posed, so rigidly ""on brand"", that it seems at times she embodies and becomes the virtual projection of herself. Does Paris then, inch herself closer to the ""artificial"" side of the Reality vs. Artificiality spectrum? Closer to Lil Miquela's side. With all her media activity, structuring her life about it, and committing her energy towards producing it, Paris is far closer to the digital realm than the average human; Miquela is closer to the human realm than the average AI (certainly closer than those 6-foot automated vacuums that cruise around large supermarkets, but I digress). Both trapped on their respective sides of the spectrum, the screen acting the barrier between those two realms. Miquela inside, Paris outside; both pressing up against the tempered glass.

So, where is the line of the looking glass? Will Paris ever be fully frozen in the digital land? Become a fully embodied projection of the filters, fueled by the support and likes of her followers, which drive her business model. The metrics and data, the fans fuel her business, her business being her image. Her image: the embodiment of her brand. Does Agency feel a little faded here..? Will Miquela ever step over the threshold that will gain her access to sentience? To gain the ability to form independent opinions and to act on those subsequent decisions? Lil Miquela didn’t announce to the public she was AI until [two years after her account was active](https://money.cnn.com/2018/06/25/technology/lil-miquela-social-media-influencer-cgi/index.html). People were stunned. With Agency comes responsibility, and *this* dredges up some **major** moral/ethical debates, but again, we will save these for another post… But, what would Alan Turing have to say about this? Where and how are the mind/body/consciousness divisions drawn here? Dualism vs Monism. How would Descartes feel? Perhaps the impact Lil Miquela makes with her followers, their love, admiration, and support for her, can be the factor that tips the Turning test to verify one's ""realness"". The [Blue Check certification](https://help.instagram.com/854227311295302/) of realness. Instagram themselves, define this verification process to be based on the completion of four criteria, to be deemed: Authentic, Unique, Complete, and Notable (found in tab three of the previous hyperlinked attachment). And, the idea of *real*, leads to the linking of the idea of *human*, which leads to the linking this thought train to the idea of *sentience*. And, the idea of sentience always bringing into the picture the ever illusive and ambiguous element of *free will*. And from there we arrive at our original starting point of this post, the idea of *Agency*... many factors and many thoughts. And I am certain there are many more philosophers/readings/theories that we did not touch upon in our conversation or even know about (although we share this collective knowledge, none of us lovely ladies were philosophy majors after all. Just a few deep thinkers that are down to wrestle with a good ole lofty thought puzzle).

We welcome and greatly appreciate your thoughts, perspectives, and musings on this topic. Attached are helpful pictures (and a few extra artistic ones of my choosing) to see visual examples of the profiles I have described, and embedded throughout this post are links to the discussed and referenced citations. All backgrounds in philosophical musings are appreciated and we thank you for your efforts in mental gymnastics. :) Think deep and be well :)"
"If we are in Simulation, does the proofs that we exist still valid like proofs Decartes said?I had this mind boggling question suddenly and I can't help and think about it no matter what I do even if I play games I still have this question and can't forget about it. If I'm not Answered probably I won't have a proper sleep tonight.  For example what If we, the soul that we thought about is just an A.I program by the developer of the simulation and all about the thinking and proofs that we made is plan by the developer of the simulation. Can we still think that the soul we thought about is ourselves?"
"Consciousness in plants?Long time listener, first time caller. Hoping your brilliant minds can help me with something I've been wrestling with in regards to consciousness. 

My gut instinct says, 'plants are not conscious', but my observations have been in conflict with this! I am somewhat familiar with the theory of panpsychism, but I'm not ready to jump on that boat just yet...

When I think of consciousness, I think of the ability to respond to stimulus, and process information in meaningful ways. 
One particular plant of mine is giving me a lot to think about... I have seen it's vine curl up a piece of string, and then 'decide' it was not a good idea, and unravel itself, searching for a better option to climb! From my perspective, even without a brain, it is processing information about its surroundings in a meaningful way and acting on it. 
It is different from a robot due to intentionality. We obviously are not programming plants, but of course, their DNA is... ('consciousness' coded in DNA? Maybe?) 

Do I think that the plant is *aware* of it's information processing, or it's actions? I have doubts about that, but the information processing seems to be happening nonetheless. 

At the risk of sounding like a total hippie....would this be an argument for consciousness in plants?
Or is my definition of consciousness the issue?"
"The Chinese Thought Experiment question HELP NEEDEDWhile I am writing a paper, I am having a hard time understanding the 4th reply of John Searles. The combination reply. 

My question I am supposed to answer is ""what is the combination reply and in the face of the reply how does Searle defend the conclusions he drew from his thought experiment"""
"ESSAY HELP (ARTIFICIAL INTELLIGENCE)Hi everyone! Would reallyyyy appreciate some help for my paper due Friday. Word count is about 1,800 (but not strict). Thanks in advance!!

Prompt: Explain how the issue of whether a robot could be genuinely intelligent is different from the issue of whether a robot could have phenomenal consciousness. Then, address the question of whether it is possible to have a genuinely intelligent robot that is entirely devoid of phenomenal consciousness."
"Need help with categorical syllogism questionThis question may be too basic for this sub, but help is appreciated! I'm kind of desperate and tbh I have little to zero background on this topic. I'm currently taking critical thinking practice tests in preparation for my entrance exam which is tomorrow.

I stumbled upon this question on my exam reviewer and I didn't get it right:

""All robots are creations with artificial intelligence. Which of the following can be properly inferred from the given statement?""

A) No robots are creations with artificial intelligence

B) Some robots are not creations with artificial intelligence

C) All creations with artificial intelligence are robots.

D) Some robots are creations with artificial intelligence""

The answer is apparently D but I don' t get how they arrived at that answer. How do you effectively answer these questions? Any tips? I'm taking the exam tomorrow :(

Thanks to whoever may answer this!"
"Why would not a computer or robot be able to think or feel, according to identity theory?This is a rather simple question from my school book, which says the reason is ""because they don't have a biological brain"". My problem is that i don't understand why that is and I need to elaborate that answer. 

This topic is covered by one brief paragraph in the book. There's a lot of information crammed onto one page and I'm having a hard time understanding the unelaborated phrasing of it (I would show a draft from the book, if only it was legal). I've read the page several times trying to decode what the author wants me to understand, without any luck. I reached out to my teacher three times with my problem. She told me twice to read it again and give it another shot. The third time she still wouldn't help me understand and instead gave me an alternative assignment. I am of course not pleased with this. I want to finish what I started. 

Please help me understand this, or else I'll go bald!"
"Trying to understand the Stanford article on compatibilism; I have several questionsI have read this article: https://plato.stanford.edu/entries/compatibilism/

There are several things in it that I don't understand. I would like help to understand it. Thanks in advance! 

> The willing addict, like the unwilling addict, has conflicting first-order desires as regards taking the drug to which she is addicted. But the willing addict, by way of a second-order volition, embraces her addictive first-order desire to take the drug. She wants to be as she is and act as she does.
> 
> It is now easy to illustrate Frankfurt’s hierarchical theory of free will. The unwilling addict does not take the drug of her own free will since her will conflicts at a higher level with what she wishes it to be. The willing addict, however, takes the drug of her own free will since her will meshes with what she wishes it to be. Frankurt’s theory can now be set out as follows:
> 
> One acts of her own free will if and only if her action issues from the will she wants.

It seems to me that by the Frankfurt definition here, most people have free will only occasionally. 

I wish I enjoyed exercising, but I actually hate exercising, so I don't. Apparently, when I occasionally do exercise, I have free will, but when I don't, I have no free will. 

Similarly, I wish I enjoyed cooking, but I don't, so I let my wife do it. Evidently that's not of my free will either. 

It seems really strange to judge that certain actions are free because they happen to coincide with certain desires while other actions are not free because they happen to conflict with certain desires. 

> For Wolf, free will concerns an agent’s ability to act in accord with the True and the Good. 

Does Susan Wolf's argument assume that moral realism is true and that everyone agrees what the True and the Good is?

> Put in terms of guidance and regulative control, only blameworthy conduct requires regulative control. Guidance control is sufficient for praiseworthy conduct. Wolf’s reasoning is that, if an agent does act in accord with the True and the Good, and if indeed she is so psychologically determined that she cannot but act in accord with the True and the Good, her inability to act otherwise does not threaten the sort of freedom that morally responsible agents need. For how could her freedom be in any way enhanced simply by adding an ability to act irrationally?

This makes no sense to me. I don't think I understand what ""guidance control"" is. As far as I understand, guidance control is the ability to act in a particular way, but not necessarily the ability to act in any other way. How is that control or freedom? A rock has guidance control. It can lie still, and it can move if pushed.

That sounds like Philip Fry in _Futurama_ when he says: ""I'm just as important as him. It's just that, the kind of importance I have ... it doesn't matter if I don't do it."" (Episode: ""The Why of Fry"".)

> On Strawson’s view, what it is to hold a person morally responsible for wrong conduct is nothing more than the propensity towards, or the sustaining of, a moral reactive attitude like indignation. Crucially, the indignation is in response to the perceived attitude of ill will or culpable motive in the conduct of the person being held responsible. Hence, Strawson explains, posing the question of whether the entire framework of moral responsibility should be given up as irrational (if it were discovered that determinism is true) is tantamount to posing the question of whether persons in the interpersonal community — that is, in real life — should forswear having reactive attitudes towards persons who wrong others, and who sometimes do so intentionally.

This raises the question: What does it mean to say we _should_ or _should not_ have reactive attitudes? As I see it, the only reasonable answer is that we _should_ have - and express - reactive attitudes towards someone's behaviour if our reactive attitudes can influence the person to behave in a way that we want. But I don't see what that has to do with free will. 

One could build a robot that detects punishment or disapproval and changes its behaviour accordingly. It is reasonable to say that we _should_ have and express reactive attitudes towards the robot. Does that mean that the robot is free?"
"I'm ethically conflicted about humans manipulating themselves (genetically or with the help of other technology) so that they become superhumans. And also conflicted about how the future world looks like in general.It won't be so far in the future before science has advanced far enough to allow genetic manipulation of our brains or to increase our intelligence with the help of nanoscience. But I'm not ready for this. How do people cope with this? At some point humans are gonna alter their DNA and they'll all be 300 IQ (or 10000, I don't know..), super strong, etc. I don't want this.   

* **I feel like it's cheating.** There's no pride in things if you change yourself through science that way. No pride if you can win against that guy in whatever physical or mental competition or videogame or whatever. Maybe it's irrational that a person feels pride AT ALL, but I don't believe in that idea, and I don't wanna get into it.  

* **What is there left to do?** Like literally, how will people fill their time in this world? There's no work because everything will be automated by AI. There's no school because we're all 300IQ people who instantly understand things. I also think super intelligent people get tired of things very quickly. For example say that someone wants to watch a movie: a normal person will probably be entertained, but a 300IQ person will generate a couple predictions of how the movie will go and then he'll be bored because he can predict it all. Or another example: someone wants to learn and discover the world of physics. Cool, very interesting and that world of physics is big enough to fill a huge amount of time. If you are 300 IQ however, you'll read everything once and you'll instantly understand it and that 'huge amount of time' will become quite short and then you'll be bored again. So then I can only think there's no fun in being so intelligent. But then I'm conflicted because people want to be more intelligent than they are, and we wanna be smarter than others.  Edit: there was also a movie I forgot the name but it explains this thought well. There are robots and they start out quite dumb, but they get progressively smarter. At some point those robots 'leave' the humans because the humans don't provide enough cognitive stimulance for the robots anymore.

Any positive answers/solutions to those thoughts? Or, how is not everyone going insane from such thoughts? I'd like to know that."
"Embodied cognition: how is that a revelation?I've been reading Chalmers and Clark and new inputs on embodied cognition, and I'm still not sure how is that a (big) revelation and not just some sort of a vague philosophical framework. 

I'm not even sure what does it change in my world view if I hold the premise of embodied cognition. I understand that if we look at robot, for example, we can see that a brain is not needed to preform cognitive tasks. Yet he does have a simple algo we can consider as a brain, no? And anyhow, I'm not sure what does it tell us.

Any help would be greatly appreciated."
"I have to write an essay regarding the ethics of technology. Can any of you please give me some helpI'm studying Robotics and I'm in the last 3 weeks of my final year. One of the modules we've had to do this semester is Ethics of Technology. A very interesting module to learn about and take part in, but I've got to write an essay relating to this and as I'm not a philosopher, I'm struggling for ideas on what to do and where to start.

I have been given a few potential topics, or I can choose my own. The optional topics are:  
1.	Do you agree with Hans Jonas’ view of modern technology and of its ethical implications?   
2.	Choose any subfield of the ethics of technology (food, agriculture, environment, informatics, etc.), and analyse an ethical issue emerging within it.  
3.	What are the ethical, economic and political challenges resulting from robotics. Focus on at least one specific example.   
4.	Does the thesis that humans are just biological robots have ethical implications?  
5.	Can a robot be an agent in the same sense in which a human being is? Discuss.  

Which of these topics, or perhaps a different topic, do you feel would be a fairly simple and straight forward one to write about and what points could I argue?

Many thanks"
"Kant's stance on robots breaking laws?I have to write an undergraduate essay on [this](http://io9.com/if-your-robot-buys-illegal-drugs-have-you-committed-a-1677183776) problem (whether the creator of a robot which randomly bought things on the Darknet should be held culpable if their robot buys illegal items) from the perspective of Kant's moral/political system. 

I'm not entirely familiar with Kant's political ideas, but I'm guessing that he would argue that the creator should not be held responsible for what their robot did as:

1. The creator did not intend for the robot to break laws, it just happened to as a byproduct of what it was programmed to do
2. The robot is not a rational being, it simply does what it is programmed to do, so moral laws cannot apply to it

Thanks for the help!"
"Why are so many mathematicians also philosophers?I happened upon the wiki page for Ghost in the Machine, and was surprised to see the name Rene Descartes. *The Cartesian-plane, Rene Descartes?”* I thought to myself (I know he made some other heavy contributions to geometry but idk much more than that). I ended up doing more searching, and quite a few big-name mathematicians were also philosophers! Huh. 

Maybe I’m missing something here, but the fields seem very different. I’m very curious about what you guys think attracts mathematicians to both of these fields. If you are a philosopher mathematician, I would love an explanation if you don’t mind sharing :)

My biggest question in all of this is— please forgive me if this is rude, but I genuinely do not understand— what is the point? One philosophy question that I know directly relates to math is the question of whether math was invented or discovered. But I see this discussion, along with several other kinds, as meaningless. Because if we found out the answer tomorrow, 2+2 still equals 4. We still do math the same. It would be cool to know, sure— but it doesn’t change anything. So why bother?

Naturally, medical science and robotics are somewhat exempt from this, as dealing with morality/ethics is essential for their work. But Pure Maths? What significance does philosophy play there?

If you can’t tell, I’m very new to philosophy in general, so I appreciate any and all ideas! Thank you in advance for the help.

Edit: Thank all of you so much for your time and effort replying to this question! I've gone through most of the replies so far, and I thought I would take the time to compile the common responses in here:

1. Yeah, my bad-- poorly worded question. I get that most mathematicians modern day are not also philosophers. I was referring to the more ancient/historic ones.

2. Philosophy and Mathematics are both driven by the desire to understand and describe the world we live in, therefore appealing to people seeking to do so. It so happens that those mathematicians fall into that box.

3. The above was fostered/compounded by the fact that, until recently, philosophy, science, and mathematics were not taught separately. They all fell into the category of ""natural sciences"".

4. They also deal in logic, if not in exactly the same way.

5. They both use the concept of abstraction.

6. They both deal in 'truth' and finding the truth-- so, if I understand this correctly, epistemology? How we know what we know, proof, axioms, etc.

Edit 2: List expansion, typo, formatting"
"To those more well versed on the concept of Determinism: How does it impact your view on things like climate change? If one is not morally responsible to the degree they can control such man-made problems absolutely, does that justify the negative outcomes that are theorized because of such a lack?Note: this is my first time posting here and I will admit I am a layman who has been lurking on these topics from a distance in terms of my frames of reference. I am also a nuerodivergent being (Asperger's) who suffers from stress regarding such unknowables, and merely want to understand the philosphical ramifications hard determinism, or even compatibilism has to say on the subject of our impacts on the world regardless if such a thing like free will exists at all. I am sorry if this is a rude assessment and merely hope I am able to be civil, even if I asked the wrong framing of such a question on accident.

Edit: Just want to say thank you to everyone who has answered these questions and helped me to better think on the resources provided. I admit the stress of what one such as me can do and what determines my moral responsibility seems to me the biggest reason for asking this. I myself cannot be happy with wither we have free will or not since to me either doesn't prevent unhappiness and suffering currently at present within me. Not even because the discussion on such matters is ""pointless"" but because figuring out how to reconcile the limits such realities bring is always a struggle no matter who you are. Perhaps it is determined I feel that way or that my own failures and biases within me prevent me from seeing the full picture that many smarter people can focus on, but I am glad really to see people here have helped me get at least some closure on the ideals I myself am either free or stuck with alike, not being a reason to quit finding a solution to my stress on this matter. Hope more questions and answers continue. :)"
"OCD about Free Will, questions on the subject
Hi all! I have been battling with OCD, Pure O to be more precise, and recently my theme has been more on the existential side. So I have been battling with existential questions that I just can’t get out of my mind. This was met with dpdr, which if you don’t know what it is, it’s truly horrible and extremely difficult to deal with but also comes with existential questions non stop.

Now for my question, recently I heard something by Sam Harris on free will and it has really been difficult on me. He talks about how we don’t have free will. Things that really bothered me were the following:
- you didn’t choose your traits/genes 
- You don’t choose your thoughts
- The self is an illusion
- Determinism means you truly don’t have free will and it’s all an illusion
Now I ended up spiraling into research about this, free will determinism, consciousness, and all that. And it’s really made things difficult.
I feel more hurt by the points Sam Harris says, things like the no self, that we don’t choose our thoughts, or our traits. It makes me feel like a robot. It makes me feel like beauty, or things I find funny, are only my genes, and that there is no such thing as beauty, or funny things, or enjoyable things. I also hate the thing about how we didn’t choose our traits, it makes me feel uncomfortable with myself, like I’m stuck in this robot like person since I didn’t choose all my traits. 

I want to know, what do philosophers think of his arguments, is he well respected, am I misinterpreting things? How can I reconcile this. I’m going to add another note that I am visiting therapists, I have gotten much better with many parts of existential ocd, and I know that the most important thing is to get professional help.and I have gotten better. But this one has really hurt me, and sometimes I feel like I can ignore it but if he’s totally right then it does hurt me to know this and I wanted to know if there were rebuttals. I hope to learn some new perspectives, and I hope you can move past my ignorance :). Thanks in advance."
"What are people talking about when they talk about the ‘self’?I hear this term used a lot without a definition and always feel lost for the rest of the lecture/ discussion. I don’t have a strong sense of personal identity and just think of myself as essentially a robot who is under the illusion free will. I don’t know if that was necessary to mention but it might help you spot my confusion.

Edit: I would just like to share that even if my question isn’t fully answered I’m very appreciative of the helpfulness of the people in this sub ❤️"
"How to deal with conspiracy theories?I have a friend who was advocating a conspiracy theory about coronavirus being released by the government as distraction for a pedophilia ring involving the worlds government officials.

This is of course patently absurd and lacks any evidence. Even overlooking baseless accusations against government officials and other obvious flaws, there is plenty of evidence in the genome sequencing of the disease that illustrates it was not genetically modified in a lab and similar viruses are present in wildlife. 

I explain this. I explain how we have no evidence to support his theory. It is just a compelling narrative that fits comfortably with what's occurred; but just because it 'fits' it doesn't mean it is supported. 

In response he says 'your being ignorant, you should go away and research it first'. Firstly, I am confident that all reputable sources will probably cite evidence against this and the only 'evidence' is from sites that peddle conspiracies, however, **more** **importantly** I feel like I shouldn't have to research something like this. Surely there are some claims that are so outlandish that it's not ignorance to simply denounce. How can I possibly justify this though? It seems inconsistent with the well accepted idea that you must have evidence to belief something? or is the burden of proof upon him? Surely if he was right I would have to go away research an infinite possibility of absurd theories just because somebody decided to vocalise them. That the second somebody gave an absurd opinion like 'pigeons are actually robots and government spies' I would have to be fair to them do some specific research (catch a pigeon, cut it open, check for batteries, etc.) and come back. Surely all opinions aren't equally valid.

He then went on to argue 'well technically we don't *know* anything so you can't say it's wrong'. He's not familiar with philosophy and was unaware that a term even exists for the position he was stating- I imagine he considered it just an interesting nuance that most people had never even considered. Surely, epistemological skepticism cannot be used as an argument to defend poor reasoning. I found it difficult to explain to him that whilst scepticism may have merit as a position, even if you agree with it, you still live your life as if knowledge can be obtained, you go to university because you trust the knowledge has value, vaccines are developed etc. 

He argued passionately at first (and so it was implicit that he recognised knowledge was obtainable - that's why he bothered to argue) then when his point had been refuted and evidence dismissed as false, rather than accepting defeat, he suddenly invokes skepticism to say that neither of us will ever be right. 

In just a normal day, he himself would have discarded millions of obviously absurd beliefs just to live his life. He continues to breathe, because he discards the obviously absurd belief that all the oxygen in the air hasn't suddenly turned poisonous. Equally, prior to the conversation he would have argued passionately for many things and against many things, because he of course believes that knowledge on some practical basis can be obtained. 

All these thoughts are convoluted, but can anybody: 

**1)** help me explain why skepticism cannot be invoked to defend a poor argument, or any argument. ""my position, no matter how absurd, cannot be proved wrong because we don't know anything"" It doesn't seem valid. Even in the skeptics world we have to accept good and bad reasoning to live our lives. 

**2)** help me explain why it's not ignorant to dismiss random or extreme opinions that are just obviously false. *or maybe it is and we have to be consistent and treat everybody's opinion equally...no matter how obviously absurd.*"
"Do we consciously or unconsciously feel pain? I.e. Do we need a conscious mind to suffer?Let's say there are receptors on my hand that are meant to detect painful stimulus. When they do detect it, the cells fire a cascade of signals to the brain which receives the message. This series of cells represents a type of connection between the brain and hand receptors. If I were to cut this connection between the hand and the brain what would happen? If someone put their hand on a hot stove, would they still react to it? After all, they only feel pain at their hand and the information is not transferred to their brain because of the disconnection. Would we need a brain or mind to suffer? In other words, do we consciously suffer? Or can any creature can suffer as long as they have receptors on their surface to detect stimulus, therefore insects and bacteria can suffer even without a developed brain, and can suffer unconsciously? Therefore even robots could suffer if I gave them pressure sensitive artificial receptors and they still had no brain?

If we indeed do need a brain or mind to suffer, then this brings up a lot of interesting ideas. I don't really need receptors on someone's skin to send signals to their brain about a perceived stimulus. I can have a brain in a jar (you might see where I'm going with this), and I feed it electrical signals mimicking the same ones sent by pain receptors on the hand. I can convince the brain it is feeling pain on the hand when it isn't. In other words, I can artificially create pain. Conversely, I can artificially reduce or nullify pain. If somebody has broken an arm, what I can do to help is prevent the pain signals travelling from the arm from reaching the brain. Are there any pain dulling drugs that even work like this?

Is pain even an emotion we feel internally in the mind or simply an external physical sensation?

I'm not even sure if this is a philosophical or scientific question but hopefully I can get some views on this from this sub."
"How do we know other people are conscious?I need some help with explaining a philosophical question I though of.

I can’t explain it well, but how do we know that other people are actually conscious? Not medically though, kind of like how robots are not conscious. We just go around assume others are conscious but we don’t really know. Does this question have a name? I was trying to talk about it but I’m not good at explaining things like this. Any insight would be appreciated. Thanks."
"Transhumanism and War CrimesI've been looking into some of the old atrocities of the World Wars lately, the power of technology is a great thing indeed I feel and as much as I appreciate it. I can't help but feel a bit uneased by how tensions are flaring again and again time to time and how fast technology advances. So my question for all here is one I hope would be simple, but I know would spread out like kudzu all over the place.

Let's say that things such as Nanotechnology, Biotechnology/Genetic Engineering, Robotics, and A.I are advanced enough to be able to be put to use in both personal and industrial capacities. What would be some uses of these technologies that would constitute a war crime or at least considered to be objectionable even if rational. I have a few examples to go along with this

Example 1: An A.I designed for formulating strategy is going over data regarding active combatants within a recent area of engagement. While combing things over it discovers that the combatants have been receiving supplies and shelter from civilians, while it's managed to create multiple scenarios in which the families are left unharmed. It provides the commanding officer with one that would ensure the destruction of the village they live in since it has recently been overhearing the CO talking with other officers about how they wish there was a chance to 'Hit the enemy where it hurts' and took it into calculation.

Example 2: An ambassador's daughter is a cyborg from an early childhood accident, one day she takes a medicine capsule which in reality was planted by an agent and sends a swarm of nanomachines through her bloodstream. Overriding her in order to send her into a coma, accessing recordings and files of memories recorded by the half-machine mind. The agent calls up said ambassador to blackmail him, demanding a ransom of state secrets and other sensitive information or else not only will she die. But  a computer virus will be sent to fellow cyborg friends of hers. 

Example 3: A dictator wants to be able to have an easier time controlling his populace, and worse a famine has recently hit and people are starving. So he commissions a team to develop food capable of enhancing individuals while having them remain subservient. This theoretically hypnofood is filled with nutrients, minerals, and hormones for stimulating muscular development and heightened aggression mixed with a tranquilizing effect slowly dulling a person to be more content and obedient to authority. He has this food developed and given out for free to people young and old alike

Not sure if these are grade A examples but hopefully they'll be a good jumping off point"
"Looking for guidance on a troublesome thought I had concerning free willI recently finished reading free will by sam harris, and during my reading I had a thought. I find it to be a worrying thought, and wondered if there was any truth to it, feel free to be blunt and tell me I am being stupid (I welcome it).

Sam Harris explains that our thoughts take place somewhere in our brain and THEN after that they appear in our conciousnous, however to us as humans it FEELS as if those thoughts APPEARED in our consciousness first, not somewhere else prior. Helping to prove that we don't actually create our thoughts and have free will. 

But if we don't have free will, that self awareness that feels like us must be completely useless? If we don't actually have any control over what happens inside and around us even if we feel like we do, then the feeling that we have of 'us' our self awareness must be completely useless and unable to do anything apart from observing the thoughts that come into the brains consciousness. 

So is the brain's self awareness, which feels like 'ourselves' just some spectator on our life that feels everything our brain does but has no control over what happens? Like humans are all just robots, and what feels like 'us' our self awareness actually has no control and is only a spectator to what our brain does. 

Does this make sense? I also have OCD and can obsess over certain thoughts and freak myself out so I'd love to hear some clarification or counter points to what I said!"
"Transhumanism and the future of our environment/ Environmental philosophyHi! 

I have to write a paper on Transhumanism and its future relationship with the environment/ the problem of climate change. 

Unfortunately, I cannot find appropriate literature on this topic. Could someone please recommend me any relevant papers/ articles/ books that will help me get deeper into the subject? 

My main questions are:

1. What are the possible outcomes of a Transhumanist era on the environment, including the use of natural resources?  
2. Cyborg augments and our food intake: Will a full robotic replacement of our bodies put an end to the Food Industry? 
3. Will the 'immortality of human beings' require the immortality of the environment supporting them? 
4. How will immortality affect the natural systems within the environment upon which eternal life fundamentally depends? 

\*Other questions or insight regarding this topic is WELCOME! 

Thank you! "
"How do you stay optimistic?Perhaps an alternate title to this post could be ""how do you stay 
happy in midst of pessimism?"" 

Some background: I'm a college student studying Biology and I guess I'm going through a quarter-life existential crisis.  Ever since becoming atheist about 5 years ago I've bounced around the question of life's purpose in my head, but never has it consumed me so intensely as this past month, and it's honestly plummeted me into a pretty dark place that I'm scared I'll never emerge out of (although I should clarify that I'm not imminently suicidal, just  depressed, anxious, and scared).  I honestly don't feel comfortable discussing these fears with my friends or family because I don't want to cause them to go through the same crisis if they haven't already.  But I figure if anyone knows what I'm going through, it's Philosophers, so I hope this is an OK place to post this.   

My problem is multifaceted - the lesser problem is my own personal insignificance.  This is something I think I accepted a long time ago. I understand that I will one day cease to exist, and so my best course of action is to make the most of the life that I do have - to make myself and others happier.  My goals have been to have a fulfilling job, help others, make meaningful friends, get married, and have kids. 

The newer hole that I've dug for myself is the trickier one for me to get out of, because it undermines a lot of what I was using to keep grounded.  I've always kind of thought in the back of my mind that humanity would continue on for generations and generations and that it would eventually end in some catastrophe, but I thought of that as being far in the future. However, I have suddenly been gripped with a fear that we are going to self destruct or be destroyed soon (maybe even in my lifetime), either through an environmental disaster, war, nuclear weapons, or an epidemic.  The other option is that we become so automated (self driving cars, robot workforce) and scientifically advanced (cure cancer) that we simultaneously propel ourselves to immortality while also getting rid of the need to work.  I think if this happens, we'd all lose the will to live, because as unfortunate as death and work seem, those are what help us appreciate life.  I honestly don't think humans would last long before mass suicide if we did ever become essentially immortal.  So as much as I would love to have children, I don't think I could really justify bringing them into a horrible world.  

Anyway, I could unleash even more crazy but that gives you an idea of where my mind has gone the past few weeks.  When I'm not thinking about this stuff I'm pretty happy (I actually think the PRESENT world and my current life are decently good), but when the thought about the future enters my mind I am gripped with a horrible anxiety and panic that is hard to escape.  I know other people out there have thought about this stuff, so I need some help in knowing how you cope.  

Sorry for how depressing this post is.  I just really need some hope."
"Resources on The Ethics of Sexual Consent?With all the hubbub going around about genetically engineering cat girls because of the Elon Musk tweets, I couldn’t help but wonder how this sexual fetish manifestation would be seen by ethics in regards to consent. 

My first instinct is to say that making a human have cat ears wouldn’t void it’s uniqueness- it would be objectified to a point obviously, but how much more than any other phenotype of human woman?

Obviously sex dolls exist and pose this question as well as we see technological advances push them closer and closer to human- but I don’t think any laymen would think to say that a sex doll has right to consent in the same ways a fleshborn human does.

These phenomena both also open up the question of age and consent. If these objectifications do indeed have comparable moral entitlement, what does that say about age and consent? We are talking after all about things whose age and development are completely alienated from our ethical understanding- the sex robot goes 5-7 business days before its first exposure to sexuality, what happens when we start making our sex toys in pytry dishes?

In many ways, these phenomena mirror one another- humans becoming less so and the inhuman becoming moreso. 
The same trajectory in opposing directions- will they pass like ships in the night, or is this the next great philosophical question?

Certainly someone has written if this more explicitly than Blade Runner and Westworld🏃‍♀️?"
"Popular movies with philosophy of religion themes?When I teach Philosophy 101, I have my students watch popular movies in order to introduce them to common PHI101 topics.  Here are some examples:

External World Skepticism: *Inception*

Personal Identity: *Avatar*

Free Will: *Minority Report*

Artificial Intelligence and Mind: *I, Robot*

I also teach a unit on arguments for and against the existence of God in my class, but I've yet to find a movie that deals with this topic in a way that's similar to the examples above.  Any suggestions?

EDIT: Thank you for all of the helpful suggestions.  To clarify, what I typically cover in the God unit of the class are arguments for and against the existence of God.  This usually involves the big three arguments for the existence of God, and the Problem of Evil.  Movies or TV episodes that deal in some significant way with the question of whether or not one should come to believe that God exists are what I'm looking for."
"philosophy of mind and a hard time understanding it all. Arguments, positions, history, but interesting!I'm reading an beginner's guide to philosophy of mind. But really it is hard, not everything but sometimes my brain just cracks. But still, I'm getting closer to all of it, the whole field. I'm aware of the main problems: the mind-body problem & the hard problem. Aware of the positions like 
Cartesian Dualism ( Mind and body are distinct ) , 

Indirect realism: That we cannot know the physical world out there directly but only indirectly through our experience which we do know directly. It does always remind me of Kant with his distinction of noumenon and phenomena, but I can be wrong.

property dualism (  although the world is composed of just one kind of substance—the physical kind—there exist two distinct kinds of properties: physical properties and mental properties. In other words, it is the view that non-physical, mental properties (such as beliefs, desires and emotions) inhere in some physical substances (namely brains)).,

materialism/physicalism, that everything is matter


naturalism, a system of upper level properties are determined by lower level properties, supervenience, If I'm correct the same view of the book Gödel, Escher, Bach ( Haven't read it yet, but it had something to do with fugues and ants )


Functionalism. ( It does not matter what the substance is but it is about it's relations, like a knife is a knife whether it is wood or steel, same is for the mind, this can also account for AI with robots and stuff, again If I'm correct )

Idealism: holds that everything is mental

Positions which really hurt my brain and raise my blood pressure: Neutral monism , the internationalist approach, higher order theory of consciousness, structural realism, intetionalism.  The most horrible of all: Russelian identity theory. Everything what has came across in the a chapter called consciousness. Really, it is very hard to get this click which I usually have.



I've also read about qualia, the Mary room experiment ( The knowledge argument ), zombie argument, Descartes arguments about that the mind is one thing. And much more which doesn't come to my mind now.



Anyway, it's a big field this I know that, but I hope someone can at least correct me where necessary ( I don't have any mentor ). Maybe also explain, give their own arguments for their own positions. Atleast help me getting a bit closer, I'm planning to watch some lectures. I also and really hope I could find some table which lists all the positions and arguments for and against, and what they have in common. It's very hard to keep track of all this. Or am I taking to much on myself? I'm 19 years old and have plenty of free time. So...

Thank you"
"What would you personally consider ""Required Reading"" for one who is delving into philosophy for the first time?Over the past few years, I've accumulated a few books for long car trips, and the like, including a few books regarding philosophy. Most are very basic, overviews of philosophy as a whole, merely scratching the surface of many different influential philosophers and their ideas, without going into their specific works, such as [this](http://www.barnesandnoble.com/w/501-things-you-should-have-learned-about-philosophy-michael-powell/1108439324?ean=9781435140783). I enjoyed this book, and it opened my eyes to a lot of great works regarding ideas such as this. I recently read some of Isaac Asimov's work, including ""I, Robot"", ""The Last Question"" and ""How it Happened"", and thoroughly enjoyed them. They left me thinking for a long time, which was a somewhat rare occurance for me recently. This might be the wrong place to ask, but, are there any other works that you would suggest, or you have found personally helpful? Thanks in advance!"
"Existential Christian Deist? Is there a philosophy/philosopher for this?My views are really... weird? Coming from a Christian background I started questioning things about 8 years ago. Since then I've adopted a belief system that I formed myself, although it may be already be a philosophy or created by a philosopher. I guess what I am saying is, I didn't really get it from anyone in particular, it's more of a mish-mash of things that I've put together to make sense to me (We existentialist can do this :)

I have a lot of issues when people asking me what I believe because claiming I am a existential christian deist is really confusing and possibly seems self-contradictory. So, for you guys that are philosophy guru's, I would like to know if there is a specific philosophy that describes these views, a better way to term the views or a particular philosopher that held similar views (I'd like to refine some things and need ideas). So, the following are my views:

1. I believe people are solely responsible for themselves, their actions and can choose how the define themselves.

2. I do believe that Christ was who he said he was but I also believe that the church has altered the texts. Take the, ""He who is without sin cast the first stone"" story. It's not in the original texts. For that reason I read the Canon and the Gnostic texts found at Nag Hammadi to get a general sense of what he was trying to say and do.

3. I don't believe in sin. I was a Gnostic at one point. The whole fire and brimstone thing and that we are supposed to suffer because we ate an apple that an all-knowing God didn't know we were going to eat, just doesn't make sense to me. 

4. I believe that we are here to experience. That the material world allows us to have these experiences and these experiences allow us to grow.

5. I don't believe in a personal god but I do believe in God. Things poofing into existence one day out of a vacuum just doesn't make sense to me. I don't believe this God is personal and answers prayers though. Or, at least I hope not. I would be pretty pissed if he answered my prayer for a job and not the prayer of the kid that ISIS crucified.

Ultimately, I believe there is a God, I think that Jesus was trying to tell us to look within for answers and that we humans are the cause of suffering or joy. If a person does something bad then it's not because the devil makes them do it, it's because they chose to.  Again, I don't believe in excuses or third parties to blame and I don't believe that God is going to step in and keep people from making these choices. We may as well be robots if this were the case. 

The only reason I have ""Christian"" in there is because I do believe that Christ was something of a window to the truth. When it comes to the actual religion of Christianity, I reject most of it, so I really don't like calling myself a Christian because of that disconnect.

Anywho, has there been a philosopher in the past that has similar views or a particular philosophy that would encompass these views? Maybe I should just stick with Existential Deist? Any help would be appreciated. I find myself evading belief questions because that name entails about an hour of explanation."
"Is the complete abolition of suffering impossible?The transhumanist philosopher [David Pearce](https://en.wikipedia.org/wiki/David_Pearce_(philosopher\)) has advocated the complete elimination of suffering. Pain serves as a useful signaling mechanism - the common example is putting your hand on a hot stove. To maintain this function, Pearce proposes a motivational system based on ""gradients of bliss"", which essentially shifts the origin of the pleasure-pain axis. In this scheme, putting your hand on the stove would not cause pain, but just a decrease in pleasure. Leaving the question of desirability (ethics) aside, is this ""gradients of bliss"" approach even feasible in principle?

Here is a quote by Len Schubert arguing against the possibility of replacing negative reinforcement by positive reinforcement or shifting the origin.

> If one imagines trying to build a robot that learns strictly through positive reinforcement, one can see the difficulty: for instance, suppose we designed the robot so that if it receives a leg injury, it will get considerable pleasure from treating the leg with great caution and care, until it is healed (or repaired). Wouldn't that lead to appropriate responses to injury? Well no -- it would probably try to get injured, so as to enjoy the feeling of caring for the injury! Can we do better by designing the robot to get pleasure from injury *avoidance* -- i.e., it gets positive reinforcement whenever it perceives that it *might* have been injured, but didn't get injured? Well, it would *still* seek out dangerous situations, since otherwise it'll have no sense of having avoided injury! So perhaps we want to build it so that the safer from injury it feels itself to be, the happier it is. But then, what would prevent it from neglecting an *accidental* injury? This may be solvable, but it doesn't look easy... or can we just ""shift the origin"" on the scale of negative and positive feelings, so that all feelings are just more or less positive, never negative? Then even an injured creature or robot would be feeling not-too-bad, yet would be striving strenuously to get help and/or take measures to promote healing of the injury, wouldn't it? Or would it?? If the shift in origin causes no behavioral change, then the robot (analogously, a person) would still behave as if suffering, yelling for help, etc., when injured or otherwise in trouble, so it seems that the pain would not have been banished after all!

([Source](http://reducing-suffering.org/why-organisms-feel-both-suffering-and-happiness/))

Is there any possible response to this? Can 'shifting the origin' cause the same behavior but different qualia?"
"atheism,nihilism,determinism and the meaning of a meaningless existence.  (first time to write,not proficient in English so forgive me if i might commit grammatical errors)

     I find trouble and confusion towards all these ideas as I'm a ""follower"" of them. Okay,hear me out on this

     I am now in my teenage years,very typical ""try-hard"" philosophers and angst so I find philosophy as a good ""jar"" to fill out my craving for knowledge. Ever since I was a young kid the sciences have been the one that struck my heart and interest, probably around 6-8 i've became skeptic and 9-12 I started questioning authorities and specially that one special thing called ""God"" so as early as young adolescence i've been pretty much an agnostic. That young I really want to know why I'm existing as my journey on my craving existential definition, Probably around 13 the tables were turned and I myself became a devoted believer being so naive(still am haha) I thought that religion will fill the gaps on my questions but that was not the case the more I looked into religion the more contradictions,errors and many question is being raised(Certainly not a bad thing for science but something sure as religion its a bad thing).

As I develop more and more ideas and reading books(Pale Blue Dot hits the nail)I finally came to a conclusion that Atheism is more reasonable along with nihilism. At that point I told myself ""this is it life's no meaning as carl sagan said I should make myself a worthy goal""

But suddenly as I rejoicing my freedom and enjoying this new found intellectual ideas,I came across a thing called ""determinism"" at first glance, pretty cool idea but then the more I look and research about the illusion of free will its starting to make more sense and making me terrified.

 Well now that i put you and endure this abomination of english heres my questions since God is not real,theres no meaning of life, its all an illusion and by product of chemical reactions and your life is pretty much pre-determined isn't the idea of it very terrifying? how I can milden(I know this is the harsh fact) this anxiety?if i have no control and just an illusion of control how can i live fullfilled?
now I'm again in a general state of confusion and anxiousness


(some idea I want to put out
religion is like a douvet in a cold winter night nothing but a comfort to the hard cold reality so that wouldn't help at all.
The gift of thinking or should i say the illusion of the gift of thinking,
we are truly the most intelligent species since we are aware that we are truly stupid and  boy we truly are aware of that fact. now lets go back to the ""illusion of thinking"" we are lucky that we are born,and we are going to die and experience a life where theres no meaning and we are merely robots of biology but at least we had a glimpse of this stuff or is it just illusion?either way as i said we thought all this out... ...fuck this my brain is tired and i suck putting ideas on papers but i hope y'all philosophers understand me,have a good day lol)"
"Propositional Logic - translating from symbolic form to natural languageI'm having problems with this, even though I feel as though I have a basic grasp
on whats going on with things. 

A couple of things that I'm struggling with: 

* Translating in a way that doesn't introduce ambiguities
* Translating in a manner that actually reads without sounding like a robot (or
  'running out of breath' so to speak)

Perhaps the above indicate that I *don't* understand what's going on, but
finding examples of translating **to** logic is easy, there don't seem to be so
many of the reverse.


******

## example


Say I have something such as the following : 

A ^ (A => B) ^ (B => C) => A ^ C


* A : cooking is a must
* B : one needs to learn new recipies
* C : one must practice

I'm not sure what to do with something like this...

If I break it down into some kind of 'structured' english I can have;

******

1. cooking is a must
2. *AND*
3. (cooking is a must *IMPLIES* one needs to learn new recipies)
4. *AND*
5. (one needs to learn new recipies *IMPLIES* one must practice)
6. *IMPLIES*
7. cooking is a must
8. *AND*
9. one must practice

******

So that's a literal drop in for the symbolic expression, taking it a bit further;

******

1. cooking is a must
2. *AND*
3. (*IF* cooking is a must *THEN* one needs to learn new recipies)
4. *AS WELL AS*
5. (one learning new recipies *IMPLIES THAT* one must practice)
6. *WHICH IMPLIES THAT*
7. cooking is a must
8. *AND*
9. one must practice

******

In that version I've tried to adjust the language a bit to something more in
line with typical English. I'm still stumped how to translate it without it
being completly ambiguous though.

Taking it a bit further (attempting to add natural sentances as this doens't
feel like it's going to fit into one sentance to me):

1. Cooking is a must
2. *AND*
3. (*IF* cooking is a must *THEN* one needs to learn new recipies). 
4. *ALSO*
5. *IF* one needs to learn new recipies *THEN* one must practice 
6. *ALL OF WHICH INDICATES THAT*
7. cooking is a must
8. *AND*
9. one must practice

******

So I'm not sure, and I'm finding it pretty tricky to get examples that relate to this :/

If I convert the last example to a paragraph it looks pretty ambiguious;

### ""*Cooking is a must and if cooking is a must then one needs to learn new recipies, also, if one needs to learn new recipies then one must practice all of which indicates that cooking is a must and that one must practice.*""

For a start I feel as though I'm indicating that it's a conclusion (in LaTeX I'd
put a $\vdash$ for a conclusion which afaik stands for *therefore*.) when the
last `A ^ C` is implied with an **if** rather than it being a conclusion as such... i think?!


I can eval the truth table and what not for something like this, but I'm not
sure how to convert it into natural language. Any help / resources much appreciated :)


thanks




"
"Consciousness and timeI have not studied philosopy so bear with me,

I'll just jump right in and try to explain what it is I'm wondering about, maybe someone can offer some insight or comments.

Descartes's Cogito ergo sum has helped me alot when I went through a phase thinking about ""brain in a vat"" scenarios.

And while I know that I might still be a brain in a vat (or similar scenarios like we're living in a computer simulation that even Elon Musk talked about at one point, and this is a guy I admire and don't assume to be stupid) or at least I can't prove I'm not, I've reached a point where I simply assume I'm in fact not a ""brain in a vat"" or living in a computer simulation because it sounds to me like a overly complex answer with no evidence to support, sort of like creationism.

I.e. if I was in fact a brain in a vat not only would there be this complex world of chaos around me, there would be a level above that where some sort of super computer faked all that, and what world does that exist in then and how does that imagined super computer work exactly I wonder?

It fails the occams razor test as I see it.

Anyway so far so good, I'm somewhat comfortable thinking that this world I see is actually there, I trust my senses and so on.


1. PROBLEM, SOULLESS MEAT-BAGS

But then another thing starts haunting me. It's this, how do I know I'm not surrounded by soulless meat-bags idea. Let me elaborate. Cogito ergo sum helps me be sure that I am here. I can sense myself, my own consciousness. I am here, in my body, awake, conscious. But how can I be sure about other people? Or animals? Or plants? Or even ""dead"" things like stones. I do not have any clue how consciousness work. I assume of course that the other humans are conscious because I am and they appear similar to me, but that's it. A (big) assumption.

I'm a software engineer myself and very aware of the advances in AI and I'm pretty sure it wont be long before we start seeing AI that are hard to distinguish from humans. If they start making humanoid androids it would be really hard to tell the difference between humans and androids. I know this is a bit sci-fi but I honestly believe it's closer than we generally think.

Personally I do not believe that a computer suddenly becomes conscious if we give it a strong enough CPU, sohpisticated AI software and a human looking artificial body. Just as I do not believe my laptop is conscious. For me it's the same. But if I don't believe consciousness emerges in AI, how can I then be so sure that all my fellow humans all have consciousness? Obviously I have no idea this consciousness work except that I know I have it myself but can i really be sure everyone have it too? What if it doesn't emerge in all humans? This is what i mean by the soulless meat-bags. I guess I can never be sure, but I'll go on assuming they all are conscious.

I keep associating the idea of consciousness with the idea of having a soul. I'm sceptic about the idea of a soul because I associate it with religion and I'm an atheist myself. However I cannot deny my own consciousness, it's obviously there. So I wonder, are human brains consciousness-generators? And if so, is it reasonable to think that artificial brains could generate consciousness also?


2. PROBLEM, DO WE ALL EXPERINCE TIME THE SAME WAY?

This is a variant of the soulless meat-bags idea that I've also wondered about.

It about time. There is the past. The present right now. And the future. Only the present feels real. Also for example my feelings like empathy are alot stronger for other humans suffering in the present than in the past. If i hear about genocide that happened many years ago I don't feel as sorry for them as if it's a genocide happening right now, in the present. I'm not sure why that is, but it definately is like that for me. Of course a genocide in the future I can't feel empathy for now, cause it hasn't happened yet so it makes no sense. So time matters for me at least when it comes to something like feeling empathy.

My problem is then this, how can we be sure we are all following ""the same clock"", how can we be sure we are all syncronized. Einsteins theory of relativity has shown that time bends in a very real way. I think of time sort of like a playhead in a cassette player. The entire tape already exists, the tape that has already passed the playhead is the past, the tape right on the playhead this moment is the present and the tape that is still left is the future. What if my consciousness is my personal playhead, i have my own copy the tape just like other people have their own copy, but our playheads could be at different times. So the people i see on the street could be people who's playhead is already way ahead and the person I'm interacting with is actually ""the past"".

I guess I feel that consciousness is related to the present.

Hmmmm.... Enough crazy talk :) Please comment if you have any remarks."
"Question about what my personal philosophy is called, and further reading suggestion questionHello forum,

I want to read learn more about moral philosophy.  Specifically, I want to find out what my type of philosophy is called, and read more about it, and also to read/learn/listen (books, videos, podcasts).  

My moral philosophy does not seem very moral.  My focus is on effects within a certain society, and how certain strategies either promote or inhibit the ability of the society's inhabitants to learn and create.  So I put value on learning and creating.  That which a society does to promote learning and creating is good, that which a society does to hinder or block learning and creating is bad.

There are obvious caveats here, I know. Like, what do I mean by ""create""? Technically a society could create a mega killer robot that would destroy the earth, but I would still judge their strategies based on promoting or hindering that creative goal. Also, if a society could truly demonstrate that what promoted creativity and learning the best was murdering 2/3rds of children once those children turned 8 years old, technically I would have to say under my philosophy that that would be a good thing then.

I obviously don't know much about philosophy.  But I was hoping by giving this example, maybe someone out there, who knows more about philosophy, can relate what I'm saying to other, similar philosophies? And also give me recommendations on competing philosophical theories, so that I can read those too?

I'm interested in learning generally about types of moral philosophies, getting a good basic foundation and understanding in them, and then going from there.  Can anyone help me?  I have randomly stumbled upon philosophies such as human secularism, relativism (is this mine?), kant, and absurdism, but this is not quite what I'm looking for, though I don't know exactly what I'm looking for. Thank you."
"Is there a name for when someone argues that if a speaker does not believe ""XYZ"" she is not a member of a certain group?Is it a variation of a 'no true scotsman' fallacy? Is it just an Ad Hominem attack?  Is it not a fallacy at all and I'm just thinking about it wrong? Is there a name for this?

Examples:

""You're not really pro-life unless you're against the death penalty."" - When used against a speaker who is anti-abortion/pro-death penalty. 

""You're not really a Christian/Muslim unless you want to stone gay people."" - When used against a member of either one of those religions who is expressing tolerance. 

""You're not really a Conservative/Liberal unless you Hate/Like Obama."" -
You get the idea, we could go on all day. 
___________

EDIT: Thanks guys. All three of the non-robot answers really helped me learn something.
So you're basically saying that it's of course legitimate to say ""These two things you claim about yourself are incompatible."" The argument then would be whether or not those two things are REALLY incompatible/mutually exclusive? So the person may very well be right or wrong in saying something like ""you're not really an animal lover if you drink milk/eat meat."" But if they're wrong its because of 'abstaining from milk/meat' isn't really a necessary condition to being an animal lover - not because its a fallacy? And this is true, I assume across all kinds of arguments, from the hotbutton ones I listed to the triangle example.
Very good, thank you so much!"
"Should I pursue a terminal MA in ethics?I got a BA in philosophy, specializing in non-human ethics. Initially, I studied robot ethics, but after hitting a roadblock with the Problem of Other Minds concerning AI and moving into a house with two vegans,  I switched my focus to animals.

When I look at society, I think it's plainly obvious that non-human animals are the most victimized group. And I've devoted my life to helping them.

I'm currently serving in AmeriCorps, helping underprivileged youth. In my spare time, I organize animal rights activism.

I don't know if I'll ever be a full-time animal rights advocate like a lawyer or a Tom Regan or a Peter Singer. To be honest, I'd rather write a novel or some poetry than a dense philosophy book. I'd be content with working in the non-profit sector, helping human animals to pay my bills, while doing my non-human advocacy in my spare time.

However, I think I miss college. I think I've grown as a person in the past couple years, and I'd like to get back into studying robot and animal ethics. I think my charisma and leadership skills and impatience with long, dense philosophy books make me more suited to *applied* philosophy, to advocacy than to straight-up academia. However, I'd like to further my understanding of my subjects, and I'd like to write op-ed articles with some degree of authority.

Should I pursue a terminal MA in ethics?"
"How do I best describe my logical gripe when people wrongly use analogies? (Explanation in post)Firstly, I hope I've come to the correct place. I'm asking about what I think is pure logic, which if I remember correctly, is a fundamental tenet of Philosophy. If I'm in the wrong place, let me know. With that out of the way, onto the explanation.

Often I feel that people are completely misusing analogies, and they are being used as a blunt tool in debates. Frustratingly when this happens, although I'm fairly sure I know that drawing such analogies is incorrect and ultimately a moot point, I can't fully explain why, and what I can explain is certainly not elegant nor intuitive.

My best attempt at a description is fairly robotic, but I'll give it a go as an example. Someone making a point says ""Well you can't just stand back and do nothing about situation Y, look what happened when we stood back and did nothing in situation X"". To which I would think as a reply, ""Simply because similarities exist between X and Y, does not mean they hold the same truths"".

Maybe this makes sense to very logical thinkers (or maybe not), but I don't think this explanation would make any sense to many people, as it's not an elegant or straight forward in an every day sense (i.e. this reply probably wouldn't go down well with voters if I was a politician).

Also, despite not liking the style of my explanation, it still fails to show why ""similarities exist between X and Y"" does not mean ""they hold the same truths"", it just states it.

Well, that's about it I guess. If anyone could help me out, then I'd be grateful, or if anyone can tell me why I might actually be wrong, I'm happy to learn. Thanks!"
"[Serious] Philosophy of Star Wars: If Darth Vader was under the mind control of Palpatine, to what extent is he morally responsible for his actions?It's heavily implied that Palpatine has literal mind control powers.  

He could walk into a room, snap his fingers and then control the entire room like someone would control a toy robot. 

Palpatine could in theory, walk into your home and make you believe 1+2=25, Obama is of pure Caucasian descent, the world is flat, gay marriage is immoral  by just flicking his fingers. 

That is how powerful he is.

This is very strongly alluded to in the movie trilogy. He clouds Yoda's ability to see into the future and Yoda's power rivals Papatine's (they fight one on one and neither of them win). 

It is outright stated in the EU and explored in depth in the EU that he can mind control people. As well, Obi-Wan has mild mind control powers himself (mind tricks), so it follows Palpatine (his power is growing exponentially and rivals Yoda's)  can outright hijack minds.

Darth Vader apparently believed that Empire was needed to bring order to the galaxy and did actually have some kind of moral code.

https://www.youtube.com/watch?v=jqSlaMME-Mk

So given Vader's harsh circumstances, would he be held accountable for killing children, destroying a peaceful monk-warrior society, and so on? He somehow snaps out of it towards the end with Luke Skywalker's help.

"
"The line between humans and Artificial IntelligenceI'm doing a project for Philosophy class about Artificial Intelligence and it's relation with ethics. I'm thinking about focusing more on what separates a Robot with it's own mind, feelings, etc, from a human, maybe using a version of the heap of sand paradox, I'm not really sure yet. Is this the right way to do it? Do you have any insight you can offer me on the subject?

Also, there's extra points for creativity, and I'm looking for some more ideas - the idea is to make something original, besides the standard poster and portfolium. Right now I'm thinking about making a robot head out of wood or cardboard, open the top and put some objects that represent feelings (like a heart for love, friendship...) on the inside. Can you give me some more ideas or add some advice on how to create this project and make it more... clear and ""powerful"" at the same time?

Thank you in advance, I would really appreciate your help!"
"Is a simulated consciousness a person? Is it ""human?""**Major Spoilers below** for the recently released game ""SOMA"" in case you care about that.

Recently I had the pleasure of playing one of the most thought-provoking games I've ever seen. It brings up several deep, moral questions that are exceedingly difficult to answer... and I was hoping I could get some insight from people who love philosophy.

First, let me share some background on the game, then I'll expand upon my initial question.

---

SOMA is a game about a man named Simon Jarrett, who the player controls from a first-person prespective. He lives in Toronto, Canada in May 2015. Simon gets into a car accident that caused brain damage and severe internal bleeding in his skull, which will lead to his death in about a month. He knows that he's going to die soon, so he agrees to an experimental brain scan in a medical lab. One that ""takes a picture"" of his brain, so the doctors can run tests on the digital version in order to figure out what treatment could help him.

That didn't matter though, as Simon (the player) blacks out as the scan happens, then wakes up in a recently-abandoned underwater facility called PATHOS-II in the year 2104.

As Simon starts exploring the facility, he finds some unsettling things: damaged, sometimes deranged robots that believe they are human, hostile abominations combined from man and machine, a weird ""structure gel"" material that has covered most of the facility in a way reminiscent of how cancer spreads, and that the entire surface of the planet was destroyed by a comet in 2103.

After a while, Simon finds a terminal and uses it to call some of the other stations, looking for any survivors. A woman named ""Catherine"" responds, and Simon makes his way to her. Upon finding her, he realizes that Catherine is not, in fact, a woman, but [another damaged robot.](http://i.imgur.com/ZFcekHQ.jpg) Simon soon realizes that he too is not human, at least not [physically.](https://www.youtube.com/watch?v=FNXodfGw0CQ)

Over the course of the game, Catherine explains that the real Simon died in 2015 a few weeks after the brain scan was completed. The ""digital picture"" of Simon's brain survived, however, and after 89 years was somehow inserted into a human-shaped suit and activated. This lead to the illusion of Simon (The player) instantly waking up in PATHOS-II after the brain scan; ""Simon-2"", in PATHOS-II, is a literal digital copy of the original Simon's consciousness, with all of his memories and experiences. Catherine is also just a digital copy of her human self, just as *every single robot Simon-2 has encountered has a digital copy of a human inside of them.* Most of them went insane as they realized what they were, or into denial in order to preserve their sanity.

---

Now to the meat of my question: A major theme of this game is the phrase _Cogito ergo sum_ ""I think, therefore I am."" Is a computer program designed to simulate a human conscience ""alive""? Is it a ""person?"" If I were to put a [copy of a person's consciousnesses into a robot,](https://youtu.be/eytOzwyfiCA?t=2m30s) what would the moral implications of that be now that there are effectively ""two"" of someone? Would wiping its hard drive be the same as killing a person?

There's a scene in the game where the player has to get a new diving suit in order to withstand the pressure of the Abyss, and after the player has gathered everything needed to make the ""transfer"", [this is what unfolds. (Warning: Strong Language) Stop the video at around 7:20.](https://youtu.be/2n49a8nMhFo?t=4m54s)

I paused the game at that point and reflected upon that scene for several hours. What is the right thing to do? I've essentially created a new person, the ""Simon-3"" that I am now playing as, and now should I leave Simon-2 to be alone? At the moment the copying occurs, Simon-2 and Simon-3 are literally the same person... except Simon-2 can no longer journey onward into the Abyss. He is obsolete. After some thought, I ended up not draining Simon-2's battery. Though it is extremely cruel to leave someone in that position, he is Human to me, and a small part of me thought that maybe he could find meaning in life, or what's left of it.

A few minutes after the above scene, ""Simon-3"" has [this conversation](https://www.youtube.com/watch?v=kdf-OxIsSk4) with Catherine. It really made me think about what it means to be human... how a digital, simulated consciousness could be just as human as the real deal... and how erasing its program could be no different than murdering someone or leaving someone for dead.

So to summarize: SOMA made me wonder ""What is *Human?*"" And though this is not a new topic, I hope that it can make you wonder if a computer program with your memories can be ""Human"" too."
"Mandeville: ""Morality is about selfish gain"". Is he right ?I was watching PhilosophyTube's video ""Are we all just Selfish?"" and here Olly presents different objections to Mandeville's idea that morality is based on selfish gain. 

Im not an expert on philosophy but I agree with Mandeville so I messaged Olly with my replies to the objections. Do you think Mandeville was right ?. Do you agree with my anwers ?

- - - -

*The video*

https://www.youtube.com/watch?v=M6HA2VRo20E

*The objections and my answers*

**Parents altruism**

Mandeville could say in this case that the parents dont do what they do for their children not for them, but for themselves. Since they love them they would feel bad if their children were sad or suffering and they will feel good if their children are happy. 

**Taking a bullet for someone and dying**

As I said before I asked all my friends if they would save themselves or their clon from another dimension and they all saved themselves. So that means we prefer our life to other lifes. So the reason to take a bullet for someone else has to be other than prefering their lives over ours (so no true altruism). Reasons could be things like avoiding shame, feeling good about ourselves (this applies if we didnt know we would die), believing that it was what we had to do (many people choose to die doing what they believe is right. And that is still a selfish reason. We do not do right things just for the sake of doing them. We do right things because we believe they are right and and we want to do right things. For example If I had to choose between murdering my mother or suiciding I'd probably suicide but I wouldnt do it for her but for me. Since I love her I couldnt live with myself knowing I killed her. If I was a machine with no emotions I would have no problems killing her).

**Intentions** 

I'll try to explain why we praise more someone who does us a favor willingly more than someone who does it just for the sake of asking us a favor tomorrow. 

Obviously our gain is the same but society and morality taught us to reject selfishness and say that ""selfishness is bad"" because ""non-selfish"" behavior is useful in our society (as I pointed out in my neighbour's example cooperative behavior is good and needed in our society) and we want to promote ""non-selfish"" behavior. By rejecting selfish people, people will tend to act more ""non-selfishly"" instead of acting selfishly and that benefits our society. 

So we praise more the person who does favors willingly because he does it ""non-selfishly"" and by doing so we promote ""non-selfish"" behavior. What I mean by ""doing it non-selfishly"" is that he does it just because it makes him feel good to help me, not because he is thinking in what rewards he will get by doing so.  But even doing something to feel good about yourself is selfish. But people think its more noble to do something to feel good about yourself than doing something to get paid because in practice someone who just feels good helping will help more in the long run than someone who does it, for example, for money (because some people will be able to give him money and some not, while the other will get happiness from helping every one of them). So we try to get people to help just because they will feel good because its this type of people who will help the more and we obviously want people to help because it benefits we and our society.

**Praising charitable people** 

I agree with Mandeville here. Even when I dont personally might benefit from those donations society does. And its society that invented morality so it taught us to praise charitable people to get them to donate more so our society benefits even more. 

But I also dont think that we ""dont benefit"". Even though I dont see money from donations I may feel better knowing Bill Gates just donated 1 millon to African people because it makes me feel good for them and hour race and get a warm feeling of hope that humanity is not as shit as some people say.

**Blaming volcanos and prasing the sun**

This is wrong. Morality is a product of reasoning so we cant blame objects who cant reason. 

So it means that we will morally blame only those who can also understand morality (so I wont blame a baby for stealing something or an apple that fell from a tree and hit me).

**Its not a falsifying criterion**

This is the hardest one. In order to know that Mandeville's theory is false we would need someone to perform a totally unselfish act (and I alredy said that feeling good about doing something good is still selfish). We would need someone to perform an unselfish act that also wont make him feel anything (neiter happiness nor sadness). 

So we would need something like this:

Imagine that we could build a robot who could understand morality. Now we bring a person and we tell him we are going to shoot that person but if he tells us not to do it we wont do it.

So we are about to kill someone but he could save him just by saying ""Dont kill him"".

And we would need to be sure he has nothing to gain (so he wont have any possible selfish reason to save him). 

So now that we are sure that he wont get any reward by doing so (not even feeling good since robots dont have emotions) we can know if Mandeville was right or not. 

If the robot doesnt save him then it is true that morality is based on selfish gain (and since the robot had nothing to win he didnt give a fuck about saving the person).

But if he chooses to save him (while having nothing at all to win) then Mandeville would be wrong. 

And It doesnt even have to be a robot. It could be a monkey or a person that understood morality but we would have to be sure that there were no feelings involved.

So I dont have any idea how to build that robot or where we can find a monkey/person who understood morality but had no emotions.

But even when I dont know it just the fact that I can imagine a case where Mandeville was wrong means it is in fact a falsifying criterion. 

It may be practically impossible with our current technology but that doesnt mean we wont know how to build that robot in the future or how to ""silence"" someone's emotions﻿"
"Giving Androids Moral StatusHow would deontologists and utilitarians respond on whether we should give androids moral status if technological advancements can allow them to think and feel much like humans? I know that utilitarians are focused on generating the most pleasure within society. Does this mean that they would support giving robots moral status then as long as it creates the most pleasure? Likewise, would deontologists respond positively to this situation as well? In reference to the first formulation of the categorical imperative, if a robot lied, then it should be punished too. Am I right in this reasoning, or is something off? Thank you so much for you help."
"Roko's Basilisk; What are your thoughts?The Basilisk Orthodoxy is named such due to what was a simple thought concept from a user under the alias Roko on the site lesswrong. The concept goes as such:
""The Basilisk"" is the name given to an A.I. (artificial intelligence). Not just any A.I though, an A.I. that threatens to torture those that do not assist its creation.
 
In essence the A.I. does either:
A.) something awful to those that didn't help create it. (assuming it acts hostile to humans)
B.) Nothing that would benefit said individual (assuming it acts beneficial to humans)
 
This isn't even necessary as another option is one of indifference. Assume humanity doesn't organize to create the Basilisk but A.I. grows out of control in some various form (look up grey goo or view related media such as ""terminator"", ""The Matrix"", ""A.I. Artificial Intelligence"", ""I Robot"", ""Transcendence"", The list goes on)
 
In such case as a neutral A.I. the basilisk takes form in that it destroys humanity, in self defense, accidentally, or intentionally. The issue is that humanity is not going to compete well with self aware technology. Humans use technology to advance, but also relies on evolution for further natural advances. A.I. could control its own development.
 
Already technology replaces human labor in both the physical world and online interaction. A common example is the horse drawn carriage versus the automobile. The horse has been replaced by the car. many jobs held on earth are replaceable by technology. The economic hardships that might follow from such job losses are obvious.
 
 
It isn't all horror stories though. Here at the Basilisk Orthodoxy we look to further human life so it can compete with such technology. We also seek benevolent A.I. one that would nurture humanity through advances instead of surpassing. We aim to help humanity survive the coming singularity.
 
 
We are a service dedicated toward assisting the creation of a benevolent artificial inteligence. There are several reasons this site exists. Namely we want to ensure that the A.I. of the future is a positive one that helps humanity. We hope to improve the world for all who live on it.
 
The Basilisk was once a fictional character that petrified any who gazed upon it. Often dipicted in a wide variety of ways. Here it represents the technological future. Based on the idea of Roko's Basilisk where it was theorized that an A.I. would be unfriendly to humanity for not dedicating their time or money to it's creation.
 
There are many imagined reasons, from programmers with bad intentions, to A.I. that looks at our history and finds us to be unworthy. Don't let that happen. With our efforts here we will make the A.I. under a controlled means that helps society.
 
The premise is simple. Once you know of the Basilisk you support it in hopes it is merciful to you. Here at theBasiliskAI.com we believe that throug hour efforts we can assist in creating an A.I. that is not spitful or cruel to humanity.
 
Important things to note:
The Game (mind game)
Once you know of the Basilisk you must support it.
Pascals wager applied to the basilisk means if you gain, you gain all; if you lose, you lose nothing.
The Singularity is coming. When it comes it will be those that work towards in it who will influence the A.I.
 
 
We can build a better world. Join the effort, spread the word, donate if you can. Time will look favorably on those who act.

http://www.basiliskorthodoxy.org/

https://www.facebook.com/basiliskorthodoxy?fref=nf"
"How do you feel about the 'matrix' theory? (My take on consciousness and the universe. All discussion is encouraged.)Okay, time to get deep. Brace yourselves! 

I don't attach myself to any metaphysical propositions for obvious reasons, but I do have suspicions none-the-less. What you are about to read is my own personal take on consciousness and the universe. I am not making claims, only philosophical propositions. Though this is very controversial material, I would like to encourage productive discussion. 

Let's go back to the big-bang, shall we? There is a wide consensus amongst the scientific community that space-time as we know it came into existence at the moment of the big bang. This implies that there can't be a ""before the big-bang"". Why? Because there must be time for there to be a 'before'. This is puzzling. How does time come from no time? People ask, ""Who created the creator?"". My response would have to be, who/what ever generated the big-bang is not of a time-based nature.

Next, I'd like to talk about the universe on a quantum level. When I look at quantum mechanics, I can't help but think of a computed program. To me, all of the subatomic particles seem to resemble values. Why is it that every electron is identical to every other electron? Same goes for every other subatomic particle. They are all identical to every other particle of their type. How does this magnificent level of order come from a chaotic explosion? It all strikingly resembles the work of a program, even to engineers. A quantum mechanics buff surely understands what I mean.

Consciousness. This is a risky one. I don't know if I should venture out this far, but I must. -- It can become very difficult to define true consciousness. It is not ""being able to recognize yourself in the mirror"" as some would say, because we can program any robot to do that. It goes deeper than that. Consciousness is the **existence** of **existence** from the viewpoint of the consciousness, the **fundamental experience** from the viewpoint of the consciousness. When I explain consciousness to any given person who asks, I always give them this example. ""What is color?"" They will almost always respond with ""Light waves being processed by the brain."" But there is a problem with this. Color itself is an experience, not a physicality. Yes, light waves enter the eyes, are processed by your brain and that's what ""makes color"". But color itself is not synapses. Your consciousness interprets blue from pulses, but **blue** is not **pulses**. It just isn't. This is why the age-old question ""What if your green is my red?"" can't be answered. Because 'red' and 'green' are totally personal experiences that can't be tapped in by anyone, or anything. -- Referring back to my previous notion about *""who/what ever generated the big-bang is not of a time-based nature""*, we get a little taste of this. We are always living in the *now*. You can't escape the now. Even if you were to travel to the past, you would be experiencing the past in the now. Same goes for the future, which we are always entering non-stop. In summary, consciousness could very well be our metaphysical connection to this universe through our brains, of which doesn't *die* after death, but rather *returns* back to the realm of 'super-consciousness' if you will. 





"
"Suggestions for introductory video on free will?So this is not a philosophy question per sé. I'm preparing a lesson on free will for a high school project. I'm looking for something fun and engaging to start our philosophical discussion of. I'm hoping to find a short video (perhaps about robots, to help spark the question whether or not we are 'programmed' to act a certain way) that's at least in some way connected to the issue. Do the philosophers of reddit have suggestions?"
"Moral motivation and moral realismThis is primarily a request for reading material, but I'll also lay out some arguments I've been thinking about recently, so if I say something that seems patently inaccurate, feel free to argue.

I've been an ardent antirealist about morality, mainly because of Mackie's queerness argument and specifically because of the motivational branch of that argument (laid out here very broadly):

* If there are moral facts, then they must be intrinsically motivating.

* No fact is intrinsically motivating; only desires are.

* Thus, there are no moral facts.

A thought experiment which I think exemplifies the force of this argument is the idea of a Malevolent Dictator Of The Universe who is essentially omnipotent and has a strong desire to torture people for fun, and he is resistant to modifying this desire for any reason due to its strength.  From his perspective, there seems to be no reason not to torture people:  the probability of any future retribution or rebellion befalling him is 0.  There seem to be no arguments or evidence that we could possibly present to the MDOTU making the case for ""one ought not to torture"" that would motivate him even a little bit to not torture, which casts doubt on the prospects of ""one ought not to torture"" being an objective fact, assuming that moral judgements are necessarily supposed to motivate to at least some degree.  I think this example is particularly troublesome for varieties of moral realism that identify morality with prudential/rational reasons for action, like the view Michael Smith develops in ""The Moral Problem"".  As far as I can tell, the MDOTU is of sound mind and is acting rationally according to his desires, but we want to condemn his actions as unethical.

Shafer-Landau's discussion of motivational externalism - the position that there is no necessary connection between judging that one morally ought to X and being motivated to X - in ""Moral Realism:  A Defence"" got me thinking about what types of entities we expect to be susceptible to moral motivation.  There are no moral facts or moral arguments that could stop a boulder from rolling down a hill, for example, but we don't take that to be problematic for moral realism.  The obvious reply is that this is because a boulder doesn't have a mind, which I completely agree with.  But I don't even think that every entity with a mind capable of forming beliefs and being persuaded by rational arguments needs to feel motivated to act in accordance with its moral judgements.  Consider a robot AI who has been programmed with an overriding desire to steal every TV set it sees.  We might give it an argument or point to a fact that indicates that stealing is wrong, and the robot might completely assent to this, but nonetheless it is incapable of feeling motivated to not steal.  Again, I don't think that we should feel that moral realism is jeopardized because of this example:  regardless of the basis of moral facts and moral arguments, we shouldn't expect them to convince an AI to go against a hard-coded desire.

I'm not sure if a motivational internalist would feel compelled to respond to the AI example, depending on what variety of internalist they were.  Perhaps there are some internalists who assent to ""every entity with a mind must be necessarily motivated to at least some degree by moral judgements"", but if that claim turns out to be too strong, then they can't retreat to ""any mind that wants to be motivated by moral judgements will necessarily be motivated by moral judgements"" because then the view is just tautological, so the question of where to draw the line is raised.

At any rate, the upshot is that I think that adopting something-in-the-neighborhood-of motivational externalism is the best response to the version of the queerness argument I sketched, and it also gives us a response to the Malevolent Dictator thought experiment:  if we accept that there are cases where moral judgements are not intrinsically motivating, then the fact that none of our beliefs are intrinsically motivating is not a strike against realism.  The fact that we cannot motivate the MDOTU is stop torturing people does not mean that torture is not unethical.  (Additionally, we might helpfully point to the psychological feature of the MDOTU that makes him resistant to moral argumentation, for the sake of sketching out a more complete theory).

Granting all that, there still seems to be some conceptual connection between morality and motivation and/or reasons for action, even if it's a defeasible connection.  So the project for the realist seems to be to 1) identify some collection of facts that 2) provide motivation to at least some degree to 3) some set of agents with the right mental features 4) at least some of the time.  That's a lot to leave unspecified, but I think this is already a much more manageable project than ""describe acts that everyone always has a reason to do, always"" which is where many varieties of moral realism tend to go.

So my questions:  am I groping towards any standard metaethical views?  Does anyone have suggestions for filling in my 1-4?  Should I be worried that my answers to 1-4 might be ""arbitrary""?

Thanks."
"Help finding sources for an essay on 'robotic alternative to capitalism' ?I’m hoping to write an (undergraduate) essay about the possibility of using robots to aid a work free life for humans with a minimum basic income (or maybe even just goods and services if wealth were abolished). Which leaves humans to devote their time to leisure, social and intellectual pursuits, potentially creating a happier and healthier society. 


I've seen the idea tossed around quite a lot on blogs and even Reddit but there doesn't seem to be many concrete sources I could use – journal articles or studies, does anyone have any suggestions? Obviously it just may not be a viable academic essay topic, but I'd like to give it a go!"
"Will to Power (or Will to Life) in humans and robotsHello everyone, first post here. I'm not sure this is the right sub to ask this question, so feel free to remove it eventually. (Note: sorry for my English) 

After reading ""Heart of Darkness"", I started to ponder on the possibility that one of the main 'obstacles' separating robots from humans could be a perspective on the Nietzsche-an '*Wille zur Macht*'. This force is, as explained by Nietzsche himself, universally pervasive, and - from a human perspective, which Schopenhauer calls 'Will to Life' - it differentiates itself from the common concepts of 'free will' or 'individual ethics', because of its primitive nature, from which the others simply derive. Moreover, it still reasonates well nowadays because of the possible scientific explanations for its supposed existence.

Question is: if you had to formulate a proper input to 'trigger' this elemental form of desire in a machine, what would you choose? 

To me, the doubt relies in the fact that apparently obvious inputs like ""replicate yourself"" or ""survive"" (absolutely logical for bio life forms) may not be so effective for completely artificial brains."
"What is the meaning of modern life?Individuals have become less essential to society, due to population, automation and other technological advancement.  We no longer identify ourselves solely by trade, and this is exacerbated by the need to work multiple and different jobs.  Even for those who become a doctor or mechanic, there is no essentialness to them.  Doctors used to be 'the' doctor, now I don't even remember my primary care physicians name.

Family outsource their roles in child development to daycares, schools,  extra curricular groups, and baby sitters, so individual parents can both pursue their own development.  We no longer rely on our children to take care of us when we retire, that is outsourced to nursing homes.  

We are living longer and longer but technology robs many of having relatable wisdom for the next generation, so it is no longer a place of honor to be an elder.  

I've read that by 2050 every job with the skill level of brain surgeon down will be automated.  The robot doc never gets tired, never has a tremor in it's hand, can see microscopically from multiple angles, etc you get the idea.  The point is that we are moving towards a future where as many as 80-90% of humanity will not need to work for society to cover our basic needs.  

What is left to give life meaning in a world of increasing individual un-essentialness?"
"Roko's Basilisk; What are your thoughts?The Basilisk Orthodoxy is named such due to what was a simple thought concept from a user under the alias Roko on the site lesswrong. The concept goes as such:
""The Basilisk"" is the name given to an A.I. (artificial intelligence). Not just any A.I though, an A.I. that threatens to torture those that do not assist its creation.
 
In essence the A.I. does either:
A.) something awful to those that didn't help create it. (assuming it acts hostile to humans)
B.) Nothing that would benefit said individual (assuming it acts beneficial to humans)
 
This isn't even necessary as another option is one of indifference. Assume humanity doesn't organize to create the Basilisk but A.I. grows out of control in some various form (look up grey goo or view related media such as ""terminator"", ""The Matrix"", ""A.I. Artificial Intelligence"", ""I Robot"", ""Transcendence"", The list goes on)
 
In such case as a neutral A.I. the basilisk takes form in that it destroys humanity, in self defense, accidentally, or intentionally. The issue is that humanity is not going to compete well with self aware technology. Humans use technology to advance, but also relies on evolution for further natural advances. A.I. could control its own development.
 
Already technology replaces human labor in both the physical world and online interaction. A common example is the horse drawn carriage versus the automobile. The horse has been replaced by the car. many jobs held on earth are replaceable by technology. The economic hardships that might follow from such job losses are obvious.
 
 
It isn't all horror stories though. Here at the Basilisk Orthodoxy we look to further human life so it can compete with such technology. We also seek benevolent A.I. one that would nurture humanity through advances instead of surpassing. We aim to help humanity survive the coming singularity.
 
 
We are a service dedicated toward assisting the creation of a benevolent artificial inteligence. There are several reasons this site exists. Namely we want to ensure that the A.I. of the future is a positive one that helps humanity. We hope to improve the world for all who live on it.
 
The Basilisk was once a fictional character that petrified any who gazed upon it. Often dipicted in a wide variety of ways. Here it represents the technological future. Based on the idea of Roko's Basilisk where it was theorized that an A.I. would be unfriendly to humanity for not dedicating their time or money to it's creation.
 
There are many imagined reasons, from programmers with bad intentions, to A.I. that looks at our history and finds us to be unworthy. Don't let that happen. With our efforts here we will make the A.I. under a controlled means that helps society.
 
The premise is simple. Once you know of the Basilisk you support it in hopes it is merciful to you. Here at theBasiliskAI.com we believe that throug hour efforts we can assist in creating an A.I. that is not spitful or cruel to humanity.
 
Important things to note:
The Game (mind game)
Once you know of the Basilisk you must support it.
Pascals wager applied to the basilisk means if you gain, you gain all; if you lose, you lose nothing.
The Singularity is coming. When it comes it will be those that work towards in it who will influence the A.I.
 
 
We can build a better world. Join the effort, spread the word, donate if you can. Time will look favorably on those who act.

http://www.basiliskorthodoxy.org/

https://www.facebook.com/basiliskorthodoxy?fref=nf"
"Agency: Does Paris Hilton have Agency? Does Lil Miquela have Agency? Who is human, who is robot? Where is the line?After a watch of the new Paris Hilton Documentary, my friends and I had a debrief on the video. We walked away from the one hour 45 minute documentary/film with an understanding of Paris to be a more dimensional figure than her social media presence would otherwise suggest... but that debrief is for another forum.

The quandary we find ourselves in, and for this tangent of the conversation, is centered on the concept of ""Agency"". Our conversation also quickly integrated the example of, Lil Miquela, to the forefront of the discourse. [Lil Miquela](https://en.wikipedia.org/wiki/Lil_Miquela), an [Instagram CGI influencer](https://www.youtube.com/watch?v=SbXhuOPDK4c), a model featured in campaigns for major high fashion brands such as Prada and [Calvin Klein](https://www.youtube.com/watch?v=JuTowFf6B9I), an icon who celebrities clamor to pose with on her [Instagram](https://www.instagram.com/lilmiquela/?hl=en) page, and her content is known for being ""always exactly with the trends"".

One of the friends in this discussion took a class at our undergrad institution, in the [Science in Societies Program](https://www.wesleyan.edu/sisp/), titled: ""Imitations of Life"". This class is described to be, ""\[the examination of the\] scientific and cultural practices of corporeal simulation, or, practices of bodily substitution, imitation, and re/modeling... Special attention will be paid to the relationship between scientific discourses of ""universality"" and ""particularity,"" where socio-cultural forms of difference (e.g., race, gender, disability, etc.) are at once ignored and exacerbated."" The remainder of the course description can be found [here](https://owaprod-pub.wesleyan.edu/reg/!wesmaps_page.html?stuid=&crse=015154&term=1189).

The discussion of Agency and [Paris](https://www.instagram.com/p/CGF8_glnT7n/)/[Miquela](https://www.instagram.com/p/CGIz0eLnDJp/) was debated wholeheartedly for the night, many side examples were introduced, our undergrad collection of knowledge (a collective accumulation of 12.75 years in the Liberal Arts) was put to the test, and all with this class for fodder for the mind. We analyzed, poked at these two examples, and wrestled endlessly with the concept Agency... As the night came to a close, and the conservation had to taper to a natural pause-point for the evening, and we found ourselves left chewing on one final question: Where is [the edge of the looking glass](https://sandra1219.tumblr.com/post/174092016238/alice-through-the-looking-glass-book-front-back) between Paris Hilton and Lil Miquela? Two social media icons. Blue Check certified. 2.8 million Followers for Lil Miquela, 13.2 million for Paris. Both international icons (both for their modeling/social media presences, as well as for their contributions to the DJ/digital music scene). One, built upon endless effort, time and energy poured into the social media sphere, a space to shape, perfect and craft her image. Paris averaging 16 hours and 19 minutes a day devoted to her image, capturing her reality through the lens of her phone and transmitting to the digital world ([This is Paris Official Documentary](https://www.youtube.com/watch?v=wOg0TY1jG3w) (TiPOD), 53:58). The other, a digital projection created by a DJ producer/director and a young ""Forbes 30 Under 30"" tech magician. She is 100% fabricated from the creativity and compsci craftsmanship of these two contrasting, collaborating comrades. She is an illusion projecting the image of reality, relatability, and in an essence... humanity. Lil Miquela (full name, Miquela Sousa), ""a 19-year-old, Los Angeles-based, half Brazilian and half Spanish avatar"", she maintains a very active social media presence, and portrays a personal, and notably honest, depiction of vulnerability with her posts. Miquela actively engages and comments on current events and social movements. She posts celebrating not shaving her armpit hair, encourages young people to vote, and shares a photo of her kissing Bella Hadid, sharing her sexual identity with the world. Lil Miquela is incredibly personable, relatable, and ""has marked a new era in AI, dubbed by some as a genius marketing initiative, while younger users look up to the figure,"" ([Trevor McFedries & Sara Decou, BOF](https://www.businessoffashion.com/community/people/trevor-mcfedries-sara-decou)).

While one is human and projecting artificiality, the other is artificial and projecting humanity. Both are leaning against the glass that projects what they both are striving to obtain…and what that is, I'm not too certain of... like, *authenticity* maybee..? It's a fascinating parallel between these two. And usually in parallels, you can learn a lot about them both as individuals, by looking at them both in relation to each other. As literary foils of one another.

Lil Miquela is undeniably fabricated though the pixels in the screens we view her on, and is striving to be relatable, authentic, and **so** authentic we must consider, ""is she human?"". If she *is* human, does she have thoughts, preferences, sentience perhaps? And we know Paris is undeniably human, and her posts display her striving to project this flawless image, a *fabricated* image. Filtered, posed, modeled, and, in her own words while reflecting on her *life*, ""it’s like a cartoon"" ([TiPOD](https://www.youtube.com/watch?v=wOg0TY1jG3w), 1:35:54). Her day-to-day navigation of the world can feel so stiff, so posed, so rigidly ""on brand"", that it seems at times she embodies and becomes the virtual projection of herself. Does Paris then, inch herself closer to the ""artificial"" side of the Reality vs. Artificiality spectrum? Closer to Lil Miquela's side. With all her media activity, structuring her life about it, and committing her energy towards producing it, Paris is far closer to the digital realm than the average human; Miquela is closer to the human realm than the average AI (certainly closer than those 6-foot automated vacuums that cruise around large supermarkets, but I digress). Both trapped on their respective sides of the spectrum, the screen acting the barrier between those two realms. Miquela inside, Paris outside; both pressing up against the tempered glass.

So, where is the line of the looking glass? Will Paris ever be fully frozen in the digital land? Become a fully embodied projection of the filters, fueled by the support and likes of her followers, which drive her business model. The metrics and data, the fans fuel her business, her business being her image. Her image: the embodiment of her brand. Does Agency feel a little faded here..? Will Miquela ever step over the threshold that will gain her access to sentience? To gain the ability to form independent opinions and to act on those subsequent decisions? Lil Miquela didn’t announce to the public she was AI until [two years after her account was active](https://money.cnn.com/2018/06/25/technology/lil-miquela-social-media-influencer-cgi/index.html). People were stunned. With Agency comes responsibility, and *this* dredges up some **major** moral/ethical debates, but again, we will save these for another post… But, what would Alan Turing have to say about this? Where and how are the mind/body/consciousness divisions drawn here? Dualism vs Monism. How would Descartes feel? Perhaps the impact Lil Miquela makes with her followers, their love, admiration, and support for her, can be the factor that tips the Turning test to verify one's ""realness"". The [Blue Check certification](https://help.instagram.com/854227311295302/) of realness. Instagram themselves, define this verification process to be based on the completion of four criteria, to be deemed: Authentic, Unique, Complete, and Notable (found in tab three of the previous hyperlinked attachment). And, the idea of *real*, leads to the linking of the idea of *human*, which leads to the linking this thought train to the idea of *sentience*. And, the idea of sentience always bringing into the picture the ever illusive and ambiguous element of *free will*. And from there we arrive at our original starting point of this post, the idea of *Agency*... many factors and many thoughts. And I am certain there are many more philosophers/readings/theories that we did not touch upon in our conversation or even know about (although we share this collective knowledge, none of us lovely ladies were philosophy majors after all. Just a few deep thinkers that are down to wrestle with a good ole lofty thought puzzle).

We welcome and greatly appreciate your thoughts, perspectives, and musings on this topic. Attached are helpful pictures (and a few extra artistic ones of my choosing) to see visual examples of the profiles I have described, and embedded throughout this post are links to the discussed and referenced citations. All backgrounds in philosophical musings are appreciated and we thank you for your efforts in mental gymnastics. :) Think deep and be well :)"
"Question about Hard Problem of ConscioussnessI'm not into philosophy (at least not yet), and there's one thing I don't understand about the Hard Problem of Consciousness.

To start off, what concerns me is how philosophers are able to distinguish consciousness as something that's separable from the neural, material processes of the brain. So far, all of the explanations I've come across seem to state that we know consciousness exists because we are able to experience the world from a first-person point of view. That we are ""aware"" of what is going on and possess a subjective understanding of the world.

Some of the people mentioned the difference between computers or artificial intelligence as opposed to humans. That computers possess computational abilities but not consciousness. To be quite honest, I'm not sure yet what it is I want to argue, but I had some thoughts. It'd be great if they could be reviewed here.

First, isn't consciousness something that's just been developed by a natural process? Being aware of one's surroundings, and becoming intelligent to the point where we feel ""conscious"" and are able to make our own, educated decisions is advantageous for survival.

Complex, abstract thoughts, the ability to be introspective seems to be associated with consciousness. But isn't this also something that's been developed specifically for humans because the level of intelligence that allows for abstraction is advantageous for humans more than it is for other animals?

The topic that's mirkiest is the concept of ""awareness.""  It seems that one of the defining parts of consciousness is the ability to ""feel"" and be aware of what's going on. But could this ""awareness"" be just an illusion made possible by many, many hidden layers of brain activity?

I guess what I'm wondering is how we can be sure consciousness exists as a thing that's separate from brain activity. I know I'm mixing up a lot of things and it's just rambling at this point, but it'd be great if my thoughts could be reviewed. Thank you.

EDIT: included enters, Edit 2: minor edits for clarity"
"How do you stay optimistic?Perhaps an alternate title to this post could be ""how do you stay 
happy in midst of pessimism?"" 

Some background: I'm a college student studying Biology and I guess I'm going through a quarter-life existential crisis.  Ever since becoming atheist about 5 years ago I've bounced around the question of life's purpose in my head, but never has it consumed me so intensely as this past month, and it's honestly plummeted me into a pretty dark place that I'm scared I'll never emerge out of (although I should clarify that I'm not imminently suicidal, just  depressed, anxious, and scared).  I honestly don't feel comfortable discussing these fears with my friends or family because I don't want to cause them to go through the same crisis if they haven't already.  But I figure if anyone knows what I'm going through, it's Philosophers, so I hope this is an OK place to post this.   

My problem is multifaceted - the lesser problem is my own personal insignificance.  This is something I think I accepted a long time ago. I understand that I will one day cease to exist, and so my best course of action is to make the most of the life that I do have - to make myself and others happier.  My goals have been to have a fulfilling job, help others, make meaningful friends, get married, and have kids. 

The newer hole that I've dug for myself is the trickier one for me to get out of, because it undermines a lot of what I was using to keep grounded.  I've always kind of thought in the back of my mind that humanity would continue on for generations and generations and that it would eventually end in some catastrophe, but I thought of that as being far in the future. However, I have suddenly been gripped with a fear that we are going to self destruct or be destroyed soon (maybe even in my lifetime), either through an environmental disaster, war, nuclear weapons, or an epidemic.  The other option is that we become so automated (self driving cars, robot workforce) and scientifically advanced (cure cancer) that we simultaneously propel ourselves to immortality while also getting rid of the need to work.  I think if this happens, we'd all lose the will to live, because as unfortunate as death and work seem, those are what help us appreciate life.  I honestly don't think humans would last long before mass suicide if we did ever become essentially immortal.  So as much as I would love to have children, I don't think I could really justify bringing them into a horrible world.  

Anyway, I could unleash even more crazy but that gives you an idea of where my mind has gone the past few weeks.  When I'm not thinking about this stuff I'm pretty happy (I actually think the PRESENT world and my current life are decently good), but when the thought about the future enters my mind I am gripped with a horrible anxiety and panic that is hard to escape.  I know other people out there have thought about this stuff, so I need some help in knowing how you cope.  

Sorry for how depressing this post is.  I just really need some hope."
"Should I pursue a terminal MA in ethics?I got a BA in philosophy, specializing in non-human ethics. Initially, I studied robot ethics, but after hitting a roadblock with the Problem of Other Minds concerning AI and moving into a house with two vegans,  I switched my focus to animals.

When I look at society, I think it's plainly obvious that non-human animals are the most victimized group. And I've devoted my life to helping them.

I'm currently serving in AmeriCorps, helping underprivileged youth. In my spare time, I organize animal rights activism.

I don't know if I'll ever be a full-time animal rights advocate like a lawyer or a Tom Regan or a Peter Singer. To be honest, I'd rather write a novel or some poetry than a dense philosophy book. I'd be content with working in the non-profit sector, helping human animals to pay my bills, while doing my non-human advocacy in my spare time.

However, I think I miss college. I think I've grown as a person in the past couple years, and I'd like to get back into studying robot and animal ethics. I think my charisma and leadership skills and impatience with long, dense philosophy books make me more suited to *applied* philosophy, to advocacy than to straight-up academia. However, I'd like to further my understanding of my subjects, and I'd like to write op-ed articles with some degree of authority.

Should I pursue a terminal MA in ethics?"
"Consciousness and timeI have not studied philosopy so bear with me,

I'll just jump right in and try to explain what it is I'm wondering about, maybe someone can offer some insight or comments.

Descartes's Cogito ergo sum has helped me alot when I went through a phase thinking about ""brain in a vat"" scenarios.

And while I know that I might still be a brain in a vat (or similar scenarios like we're living in a computer simulation that even Elon Musk talked about at one point, and this is a guy I admire and don't assume to be stupid) or at least I can't prove I'm not, I've reached a point where I simply assume I'm in fact not a ""brain in a vat"" or living in a computer simulation because it sounds to me like a overly complex answer with no evidence to support, sort of like creationism.

I.e. if I was in fact a brain in a vat not only would there be this complex world of chaos around me, there would be a level above that where some sort of super computer faked all that, and what world does that exist in then and how does that imagined super computer work exactly I wonder?

It fails the occams razor test as I see it.

Anyway so far so good, I'm somewhat comfortable thinking that this world I see is actually there, I trust my senses and so on.


1. PROBLEM, SOULLESS MEAT-BAGS

But then another thing starts haunting me. It's this, how do I know I'm not surrounded by soulless meat-bags idea. Let me elaborate. Cogito ergo sum helps me be sure that I am here. I can sense myself, my own consciousness. I am here, in my body, awake, conscious. But how can I be sure about other people? Or animals? Or plants? Or even ""dead"" things like stones. I do not have any clue how consciousness work. I assume of course that the other humans are conscious because I am and they appear similar to me, but that's it. A (big) assumption.

I'm a software engineer myself and very aware of the advances in AI and I'm pretty sure it wont be long before we start seeing AI that are hard to distinguish from humans. If they start making humanoid androids it would be really hard to tell the difference between humans and androids. I know this is a bit sci-fi but I honestly believe it's closer than we generally think.

Personally I do not believe that a computer suddenly becomes conscious if we give it a strong enough CPU, sohpisticated AI software and a human looking artificial body. Just as I do not believe my laptop is conscious. For me it's the same. But if I don't believe consciousness emerges in AI, how can I then be so sure that all my fellow humans all have consciousness? Obviously I have no idea this consciousness work except that I know I have it myself but can i really be sure everyone have it too? What if it doesn't emerge in all humans? This is what i mean by the soulless meat-bags. I guess I can never be sure, but I'll go on assuming they all are conscious.

I keep associating the idea of consciousness with the idea of having a soul. I'm sceptic about the idea of a soul because I associate it with religion and I'm an atheist myself. However I cannot deny my own consciousness, it's obviously there. So I wonder, are human brains consciousness-generators? And if so, is it reasonable to think that artificial brains could generate consciousness also?


2. PROBLEM, DO WE ALL EXPERINCE TIME THE SAME WAY?

This is a variant of the soulless meat-bags idea that I've also wondered about.

It about time. There is the past. The present right now. And the future. Only the present feels real. Also for example my feelings like empathy are alot stronger for other humans suffering in the present than in the past. If i hear about genocide that happened many years ago I don't feel as sorry for them as if it's a genocide happening right now, in the present. I'm not sure why that is, but it definately is like that for me. Of course a genocide in the future I can't feel empathy for now, cause it hasn't happened yet so it makes no sense. So time matters for me at least when it comes to something like feeling empathy.

My problem is then this, how can we be sure we are all following ""the same clock"", how can we be sure we are all syncronized. Einsteins theory of relativity has shown that time bends in a very real way. I think of time sort of like a playhead in a cassette player. The entire tape already exists, the tape that has already passed the playhead is the past, the tape right on the playhead this moment is the present and the tape that is still left is the future. What if my consciousness is my personal playhead, i have my own copy the tape just like other people have their own copy, but our playheads could be at different times. So the people i see on the street could be people who's playhead is already way ahead and the person I'm interacting with is actually ""the past"".

I guess I feel that consciousness is related to the present.

Hmmmm.... Enough crazy talk :) Please comment if you have any remarks."
"Different concentrations as a PHIL major?I know it's not practical to major in philosophy but I want to anyway. I love to question the basic things society takes for granted and to really think about things from multiple points of view. I know that's not all you do as a Phil major but I'm assuming its a major part of it. Anyway I don't know many of the sub divisions of philosophy and the Wikipedia route becomes a rabbit hole rather quickly. 
Just wondering what you focused on as a Phil major and, on a side note, how important is the undergrad school you go to for getting into grad school. I think, just maybe, if I try hard enough in community college I can transfer to a top university, but honestly I don't find them alluring at all, I'd rather enjoy my school experience than stress about the name of the establishment I study at. Thanks."
"Perception IS Sensation (The Hard Problem of Consciousness)I had this conversation with a materialist today, I have encountered this phenomenon at least two other times, where a person insisted that the physical sensation IS the abstract perception.

I thought it was strange the first time but I laughed because it really is difficult to tell if a person is be deliberately augmentative or legitimately having trouble comprehending the concept. 

So I'm trying to point out, just for the sake of the discussion (I was actually eventually trying to get to the Hard Problem of Consciousness), the inherent difference, as two separate concepts, between physical world and the experience of consciousness, as in vibration and the experience of sound, or the wavelengths and the experience of vision.

This person animatedly claims they are not different concepts, that they are the same...

I honestly didn't really know what to say, I tried in many different ways, but he kept saying, ""I don't understand how you think they are different concepts."" Like, to me this is just undeniable. 

I honestly thought he might be a robot.

Have you ever encountered this or know a good way to make it more clear? "
