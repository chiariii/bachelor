"Building Trust with Responsible AIArtificial Intelligence is being used in almost every aspect of life. AI symbolizes growth and productivity in the minds of some, but it is raising questions as well on the fairness, privacy, and security of these systems. Many legitimate issues exist, including biased choices, labor replacement, and a lack of security. When it comes to robots, this is very frightening. Self-driving automobiles, for example, can cause injury or death if they make mistakes. Responsible AI addresses these difficulties and makes AI systems more accountable.

**Responsible AI should fulfill the following aims:**

* Interpretability: We obtain an explanation for how a model makes predictions when we interpret it. An AI system makes predictions for a user. Even if these selections are correct, a user is likely to seek an explanation. Responsible AI can describe how we create interpretable models.
* Fairness: AI systems have the potential to make judgments that are biased towards particular groups of people. Bias in the training data is the source of this bias. The easier it is to assure fairness and rectify any bias in a model, the more interpretable it is. As a result, we need a Responsible AI framework to explain how we evaluate fairness and what to do if a model makes unjust predictions.
* Safety and Security: AI systems aren’t deterministic. When confronted with new situations, they are prone to making poor choices. The systems can even be tampered with to make unwise decisions. Therefore, we need to ensure safety and security in these systems.
* Data Governance: The data used must be of high quality. If the data used by AI has errors, the system may make wrong decisions.

**Continue Reading The Article** [**Here**](https://www.marktechpost.com/2022/04/02/building-trust-with-responsible-ai/)

&#x200B;

https://preview.redd.it/3ckdtznrr6r81.png?width=1024&format=png&auto=webp&v=enabled&s=d9a7590306f5126b1e214a2b34e0a5da8ac73d70"
"Notes from the NYU AI Ethics conferenceThis weekend I attended the [Ethics of Artificial Intelligence conference](https://wp.nyu.edu/consciousness/ethics-of-artificial-intelligence/) at NYU. There were a ton of high-profile and interesting people there from philosophy (David Chalmers, Peter Railton, Nick Bostrom, Thomas Nagel, Paul Boghossian, Frances Kamm, Wendell Wallach) and science (Yann LeCun, Stuart Russell, Stephen Wolfram, Max Tegmark, Francesca Rossi) as well as Eliezer Yudkowsky. 

There were two fairly long days of talks and panels. David Chalmers (famous for his philosophy of mind and consciousness) did not officially speak but acted as chair for the event. He outlined the philosophy of the conference, which was to discuss both short and long term issues in AI ethics without worrying about either detracting from the other. He was, as usual, extremely awesome.

Here is a summary of the event with the most interesting points made by the speakers.

**Day One**

The first block of talks on Friday was an overview of general issues related to artificial intelligence. Nick Bostrom, author of *Superintelligence* and head of the Future of Humanity Institute, started with something of a barrage of all the general ideas and things he's come up with. He floated the idea that perhaps we shouldn't program AI systems to be maximally moral, for we don't know what the true morality looks like, and what if it turns out that such a directive would lead to humans being punished, or something else that was pathological or downright weird? He also described three principles for how we should treat AIs: substrate nondiscrimination (moral status does not depend on the kind of hardware/wetware you run on), ontogeny nondiscrimination (moral status does not depend on how you were created), and subjective time (moral value exists relative to subjectively experienced time rather than objective time, so if a mind ran at a fast clock speed its life would be more important, all other things being equal).

He pointed out that AI moral status could arise before they reach there is any such thing as human level AI - just like animals have moral status despite being much simpler than humans. He mentioned the possibility of a Malthusian catastrophe from unlimited digital reproduction as well as the possibility for vote manipulation through agent duplication, and how we'll need to prevent these two things.

He voiced support for meta level decisionmaking - a ['moral parliament'](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html) where we imagine moral theories sending 'delegates' to compromise over contentious issues. Such a system could also accommodate other values and interests besides moral theories.

He answered the question of ""what is humanity most likely to fail at?"" with a qualified choice of 'mind crime' committed against advanced AIs. Humans already have difficulty with empathy towards animals when they exist on farms or in the wild, but AI would not necessarily have the basic biological features which incline us to be empathetic at all towards animals. Some robots attract empathetic attention from humans, but many invisible automated processes are much harder for people to feel empathetic towards.

Virginia Dignum was next; she is at the Delft University of Technology and spoke about mechanisms for automated processes to make decisions. She specified four methods of decisionmaking based on whether decisions are taken deliberately or imposed upon a system and whether the decisions are made internally or externally. The two former features lead to algorithmic decisionmaking in machines; the latter two lead to imposed decisions predetermined by regulatory institutions. Deliberated external decisionmaking means there is a 'human in the loop' and internal imposed decisionmaking is essentially randomness.

Yann LeCun concluded this section with a pretty fantastic overview of deep learning methods and the limitations which stand in the way of progress in machine intelligence. He pointed out that reinforcement learning is a rare and narrow slice of the field today and that the greatest obstacles for machines include common sense judgements and abstraction. The biggest current problem for AI is unsupervised learning, which is having machines that can learn to classify things on their own without being given clearly labelled data from humans. He showcased some of the (very cool) features of adversarial learning which are being used to tackle this.

He expressed support for the orthogonality thesis, namely the idea that intelligence and morality are 'orthogonal' - just because an agent is very smart doesn't mean that it's necessarily moral. He believes we should build a few basic drives into AIs: do not hurt humans, interact with humans, crave positive feedback from trusted human trainers. He also described a couple of reasons for why he is not concerned about uncontrolled advanced artificial intelligence. One was that he is confident that objective functions can be specified in such a way as to make machines indifferent to being switched off, and the other is that a narrow-AI focused on eliminating an unfriendly general-AI would 'win' due to its specialization.

In Q&A, Stuart Russell objected to LeCun's confidence in machines being indifferent to being shut off based on the fact that self-preservation as a goal implicitly falls out of whatever other goals a machine has. Paul Boghossian objected to the 'behaviorist' nature of the speakers' points of view, saying that they were exempting consciousness from its proper role in these discussions. One person asked whether we should let AIs take charge of everything and supersede humanity - Bostrom pointed out that the space of possible futures is ""an enormous Petri dish"" which we don't understand; an AI future could materialize as a planet sized supercomputer with no moral status, and we will need to learn how to engineer friendly advanced AI systems no matter what the plan is. 

The rest of the Friday talks were devoted to near-future issues with specific AI systems. Peter Asaro started with an overview of his organization, the 'Campaign to Stop Killer Robots'. He stated that targeting and killing should remain human-controlled actions. While he acknowledged that automated weaponry could result in fewer casualties on the battlefield, he believed that it was too narrow a view of the consequences. He said that it's not straightforward to translate complicated battlefield morality questions for machines to understand, and is worried about unintended initiation and escalation of conflicts through automated systems, arms races, and threats to humanitarian law. He also believes that people should only be killed with 'dignity' and that doing it with a robot robs people of this. Therefore, he called for a clear and strong norm against automated weapons.

Kate Devlin of the University of London gave a brief overview of the ethics of artificial sexuality. Looking at the history of sexualized robots featured in fictional media, she noted that almost all of them are female. Today there is a ""Campaign Against Sex Robots"" which is based on the idea that sexual robots would lead to the objectification of women. Devlin does not agree as she thinks it is too early to ban the technology and that we should explore it before thinking about banning it, especially since it does not really harm anyone. Instead she wants us to think about how to develop it correctly. There are many potential uses for these types of robots ranging all the way to the therapeutic; many of the rudimentary ones being sold today are bought by people who are incapable of forming ordinary relationships for various reasons. VR is being used in arousal tests to gauge the efficacy of treatments against pedophilia.

She noted that gender issues have arisen in technology already; the history of gendered technology includes pacemakers originally designed only for men and phones too large for women's pockets. We should get into AI now to make sure that it is not designed in problematic ways.

She mentioned privacy concerns, as the manufactures of the female stimulator WeVibe have already been sued over concerns that they were not properly informing customers of their collection of data from the devices. She wondered if we will ever get to a stage where a robot might have some knowledge of its role and refuse to give consent to its use, and if transmission/duplication of data and code between machines could serve as some form of digital sexual reproduction.

Vasant Dhar of NYU spoke next about data and privacy in the era of autonomous vehicles. He said that our legal and financial liability institutions are based on outdated notions of data and that they fail to address liability and crime. However, the tools we have now even in ordinary cars for recording data can be used to improve insurance and judicial systems. He proposed black boxes for cars that would contain all relevant data to determine fault in the event of accidents, and said that customers should have the choice to share their driving data with insurance companies to get lower premiums. 

Dhar reiterated the importance of improving vehicle safety through autonomous driving; each percentage point reduction in vehicle accidents equates to 400 deaths and 40,000 injuries avoided every year.

Adam Kolber followed up with a discussion of whether ""the code is the law"", based on the case study of [The DAO](https://en.wikipedia.org/wiki/The_DAO_%28organization%29) which was an automated capital fund which was subjected to a $50 million loss through exploitation. The answer apparently is that the code should not be the law, even though many people seemed to accept that it was.

Steve Wolfram of WolframAlpha and Mathematica fame discussed the issues of computer languages and goal specification. He said that his life work has essentially been about trying to find ways for humans to specify their goals to machines, and that this can work for ethics as well as for math. He doesn't think that any single moral theory is likely to work for guiding artificial intelligence, apparently because of Godel's theorem and the incompleteness of computational languages.

Francesca Rossi of IBM argued that for AIs and humans to interact very productively we will have to embed them in environments, so that rather than picking up a tool like a laptop or a phone, we are interacting with artificial systems all around is in our rooms and spaces. Humans will be recognized by their environments and our needs and wants will be inferred or asked about. AI embedded in environments can have memories about humans to better serve their interests. Most of all, we will need to establish trust between humans and AIs.

Peter Railton, philosopher at the University of Michigan, attacked the subjects of orthogonality and value learning. He said that we can't simply tell AIs to do what we want because our wants and values require critical assessment. He said that the orthogonality thesis might be right, but as we increasingly interact with systems and allow them to participate in our own lives and decisionmaking, the question of what it would take for them to be intelligent might involve certain features relevant to morality.

He stated that AIs should be thought of as social creatures; as a simple model, self regulation in a Hobbesian social contract leads to constraints and respect derived from self preservation. A society of intelligent cooperators can resist aggression and malice, and being moral is more efficient for a community than being cunning. From these principles we have a recipe for building proto-moral agents.

He discussed the 'moral point of view' required for many strong ethical theories such as Kantian ethics and consequentialism: it requires agents to have a hierarchical, non-perspectival, modal/planning-oriented, and consistent view of the world which assigns intrinsic moral weight to things. He described how all these features are also part of the process of becoming generally intelligent in the first place, implying that general social intelligence ensures the necessary information required for moral decisionmaking. In the path towards functional moral agents, we will have to build agents which can represent the goals of others and have them learn how to act in beneficial ways. So if we can build AIs that we can trust, then we are on a good path towards building artificial moral agents.

In the Q&A, Eliezer Yudkowsky objected that in the long run the 'instrumental strategy' is not quite what you want because maximizing people's desires as they are explicitly revealed can lead to bad outcomes, and you have to have a view like coherent extrapolated volition which asks what people would really want. Russell objected that when an agent becomes sufficiently powerful, it has no need to cooperate anymore.

Regina Rini of the NYU Center for Bioethics stated that the approaches to ethics so far described relied too much on the Western post-enlightenment view of ethics, which is a historical aberration, and excluded African, Chinese and other approaches to ethics. Railton stated that his scheme was grounded in basic empathy and not mediated by any higher order moral theory; Wolfram and Rossi said that no one ethical approach will work and AI will have to represent diverse values.

**Day Two**

Saturday was devoted to long term discussion of the future of advanced artificial intelligence. Stuart Russell, professor at UC Berkeley and head of the new Center for Human Compatible Artificial Intelligence, started with a basic overview of the control problem. He described the points made in Steve Omohundro's paper on convergent instrumental drives. He also had some pretty harsh words for the researchers in the AI community which have denied and rejected notions of the control problem without seriously engaging with the relevant literature.

He had three simple ideas which he proposed to constitute the definition of 'provably beneficial' AI: maximizing values for humans is the system's only goal; the robot is initially uncertain about these goals, and the best source of information is human behavior. He referred to inverse reinforcement learning as a technique for machines to learn human preferences, and said that uncertainty provides an incentive for machines to learn, ask questions, and explore cautiously. 

His answer to the off-switch problem is to make robots unsure of their objectives, so that they assume that the human will switch the robot off if and only if it has a good reason to, and will therefore be complicit with the action. He said that the wireheading problem can be avoided if you construct the reward signal as information about the reward function rather than as a reward itself; this way, any hijacking of the reward signal makes it useless.

He said that there is a strong economic incentive for value alignment, but humans are irrational, nasty, inconsistent, and weak-willed.

The next speaker was Eliezer Yudkowsky of the Machine Intelligence Research Institute. Chalmers pointed out his role there as well as his side venture in Harry Potter fanfiction.

Yudkowsky started [his talk](https://intelligence.org/nyu-talk/) by pointing out how the Terminator pictures in every media article about the control problem are inappropriate. The real analogy to be used is [Mickey Mouse as the Sorcerer's Apprentice in *Fantasia*.](https://www.youtube.com/watch?v=Ait_Fs6UQhQ) 

He said that the first difficulty of AI alignnment is that the utility functions we imagine are too simple, and the second difficulty is that maximizing the probability of achieving a given goal leads to pathological outcomes. He and MIRI are concerned with the nature of the goal of 'maximizing' and how to define goals in a way that avoids the problems of perverse instantiation. 

He said that the fears of AI being developed by some terrorist or rogue group were silly, as ""ISIS is not developing convolutional neural nets."" Instead the most powerful AI is likely to be developed by large groups in government, academia and industry.

He claimed that the four central propositions which support the idea that AI is a very big problem are: the orthogonality thesis, instrumental convergence, capability gain (the speed at which advanced AI can make itself better), and alignment difficulty. He said the first two are logical matters of computer science that people always learn to accept when they reflect upon them, while the latter two are more controversial.

The next talk was from Max Tegmark and Meia Chita-Tegmark. Max is a world-renowned physicist who helps run the Future of Life Institute, and Meia is a psychologist. They explained how physics and psychology provide useful tools for understanding artificial intelligence; physics tells us about computation and the constraints of the universe, and psychology tells us about the nature of well being, ways to debug the mind when reasoning about AI and methods to design psychomorphic AIs. Meia was the only speaker at the conference to discuss unemployment in any detail; she pointed out that retirement has only mixed effects on well being and that happiness comes from financial satisfaction and feelings of respect. She said that studying homemakers, part time workers and early retirees can tell us more about how an automated economy would affect people's well-being.

Max checked off [a list of common myths](http://futureoflife.org/background/aimyths/) regarding advanced AI. Meia said that we should look at the cognitive biases which have led to these misconceptions (such as availability bias leading to people worrying about robots rather than invisible artificial intelligence) and figure out how to avoid similar bugs from inhibiting our thinking in the future.

By the way, Max Tegmark is very cool, he has a sort of old-rocker-dude vibe, and he and Meia are super cute together.

Wendell Wallach of Yale spoke next. He is the man who quite literally wrote [the book](https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975) on AI ethics. He distinguished top-down approaches of formally specifying AI behaviors from bottom-up approaches of value learning. He said that neither will be sufficient on its own and that both have important roles to play. He is worried that AI engineers will make simplistic assumptions about AI, such as the idea that every decision should be utilitarian or the idea that 'ethics' and 'morality' are icky concepts that can be ignored.

Steve Petersen, a philosopher at the University of Niagara, gave the next talk, based on the draft of a forthcoming paper of his. He aims to push back against the orthogonality thesis and modulate the level of the risk assessment provided by Bostrom. His argument is that designing AI to follow any complex goal will necessarily require it to be able to learn the values of its ""teleological ancestors"" (the original human designers or the previous iterations of AI before it self-improved or self-modified) and arrive at a state of coherence between goals. As agents replicate, self-modify and merge in the digital world, there can be no fact of the matter about which agents are the same or different; instead there will be an 'agential soup' unified by a common teleological thread originating with the designers. Coherence reasoning leads to impartial reasoning with the goals of other agents.

There were several responses to him in Q&A. Yudkowsky's objection was that reaching coherence requires a meta-preference framework with particular assumptions about the universe and ontology; therefore, for any goal, there are many preference frameworks which could fulfill it, many of which would be perverse. Russell said that just coherence is not enough because you need the systems to give special weight to humans. Max Tegmark said that the problem was the vagueness of humanity's final goals. Chalmers pointed out that the orthogonality thesis still allows for all kinds of correlations between between intelligence and morality, as long as they are not necessary by design. Petersen said that he is arguing for 'attractor basins' in the possibility space of AI minds. Interestingly, he was motivated to start his research by the [Dylan Matthews Vox article](http://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai) on effective altruism where Dylan thought that effective altruists shouldn't be concerned by artificial intelligence. Petersen doesn't think that AI is unimportant and thinks that Bostrom and Yudkowsky's work is valuable, but he wanted to get a more critical assessment of the level of risk when he learned that alternative altruistic projects were at stake.

Matthew Liao of the NYU Center for Bioethics gave an argument for moral status on the basis of capabilities - that an entity is morally valuable to the extent that it has the physical/genetic basis for achieving features of moral relevance. I did not get a chance to ask him if this would imply that a 'seed AI' could be the most morally valuable entity in the world. He did argue against the ideas that level of intelligence or degree of moral agency determine moral status, as we don't normally think that smarter or more benevolent humans are more morally valuable than others. 

Liao argued that moral theories are too specific and too high level to be generally implemented in AIs. Instead, AI will need a universal moral grammar in which to specify morality. The holy grail is to develop machines that understand why things are right or wrong.

Eric Schwitzgebel and Mara Garza of UC Riverside argued for basic principles of AI rights. They introduced a very weak ""no-relevant-difference"" argument: the idea that there are possible AIs which have the same morally relevant features that humans do and therefore there are possible AIs with equal value to humans. They questioned if cheerfully suicidal or hardworking AI is acceptable, and stated a 'self respect principle': that human grade AI should be designed with an appropriate appreciation of its own value.

John Basl and Ronald Sandler of Northeastern University argued for AI research committees to approve or deny research in cases where AI subjects might be harmed. They said it would not be very different from cases like animal testing where we have similar review boards, and sketched out details of how the proposal would work.

Daniel Kahneman, one of the most famous behavioral economists in the world, made something of a surprise appearance in the final panel. He said that we should take intuitions about case studies like the trolley problem seriously, as that is how the public will think about these events, for better or for worse. He said that no matter how AI cars kill people, it will be perceived with horror whenever the first incident happens, and we should prepare for that. Intuitions depend on irrelevant factors and will especially depend on whether AIs are designed to resemble us or not.

Gary Marcus, professor of psychology at NYU, of gave a much needed presentation about the nature of intelligence. The previous talks in this discussion had mostly assumed that intelligence was one-dimensional and simple and that there was some fixed idea of 'human-level' AI which we could eventually reach. Of course this is a ridiculous oversimplification; intelligence is multidimensional and it is more about implementing a combination of various cognitive tools, some of which are already stronger in AIs than in humans. AIs can be better or worse than us in various domains, so we really have no idea where AIs will be in this multidimensional space. AIs could in fact be better than us at moral reasoning. He also emphasized the gap is between machine learning today and what human reasoning can do.

Susan Schneider of Marquette University, a philosopher who has written quite a bit about AI and superintelligence, went over various issues. She argued that mind uploads might constitute death of the individual as long as we don't prove certain ideas about consciousness and personal identity, and also claimed that designing an intelligent and morally valuable robot to serve the interests of its creators would constitute slavery.

Jaan Tallinn, founder of Skype, also gave a quick talk. He has been a strong financial backer for MIRI and other efforts in this space, and simply expressed his belief in the importance of the issue and his happiness at the success of the conference and the number of students who were interested in pursuing the topic.

There was some final banter about the nature of consciousness which David Chalmers sat through very passively. Yudkowsky expressed optimism that one day we will have an explanation of consciousness which clears up our confusion on the matter. Nagel said that we will need to think more about the dynamics of multi-agent systems and moral epistemology. After that the event ended.

The conference videos are available [here.](http://livestream.com/nyu-tv/ethicsofAI) In my opinion, the best talks were given by LeCun, Railton, Russell, Yudkowsky, the Tegmarks, Petersen, and Marcus. The event overall was great and being in Manhattan made it even better. There was quite a bit of valuable informal meeting and discussion between many of the speakers and attendees. There was no 'sneering' or disdain about Yudkowsky or Bostrom as far as I could tell. It seemed like a generally open minded yet well educated crowd.

If you regret missing it, then you might like to head to the [Envision Conference](http://envision-conference.com/) this December. 

"
"Building Trust with Responsible AIArtificial Intelligence is being used in almost every aspect of life. AI symbolizes growth and productivity in the minds of some, but it is raising questions as well on the fairness, privacy, and security of these systems. Many legitimate issues exist, including biased choices, labor replacement, and a lack of security. When it comes to robots, this is very frightening. Self-driving automobiles, for example, can cause injury or death if they make mistakes. Responsible AI addresses these difficulties and makes AI systems more accountable.

**Responsible AI should fulfill the following aims:**

* Interpretability: We obtain an explanation for how a model makes predictions when we interpret it. An AI system makes predictions for a user. Even if these selections are correct, a user is likely to seek an explanation. Responsible AI can describe how we create interpretable models.
* Fairness: AI systems have the potential to make judgments that are biased towards particular groups of people. Bias in the training data is the source of this bias. The easier it is to assure fairness and rectify any bias in a model, the more interpretable it is. As a result, we need a Responsible AI framework to explain how we evaluate fairness and what to do if a model makes unjust predictions.
* Safety and Security: AI systems aren’t deterministic. When confronted with new situations, they are prone to making poor choices. The systems can even be tampered with to make unwise decisions. Therefore, we need to ensure safety and security in these systems.
* Data Governance: The data used must be of high quality. If the data used by AI has errors, the system may make wrong decisions.

**Continue Reading The Article** [**Here**](https://www.marktechpost.com/2022/04/02/building-trust-with-responsible-ai/)

&#x200B;

https://preview.redd.it/3ckdtznrr6r81.png?width=1024&format=png&auto=webp&v=enabled&s=d9a7590306f5126b1e214a2b34e0a5da8ac73d70"
"Notes from the NYU AI Ethics conferenceThis weekend I attended the [Ethics of Artificial Intelligence conference](https://wp.nyu.edu/consciousness/ethics-of-artificial-intelligence/) at NYU. There were a ton of high-profile and interesting people there from philosophy (David Chalmers, Peter Railton, Nick Bostrom, Thomas Nagel, Paul Boghossian, Frances Kamm, Wendell Wallach) and science (Yann LeCun, Stuart Russell, Stephen Wolfram, Max Tegmark, Francesca Rossi) as well as Eliezer Yudkowsky. 

There were two fairly long days of talks and panels. David Chalmers (famous for his philosophy of mind and consciousness) did not officially speak but acted as chair for the event. He outlined the philosophy of the conference, which was to discuss both short and long term issues in AI ethics without worrying about either detracting from the other. He was, as usual, extremely awesome.

Here is a summary of the event with the most interesting points made by the speakers.

**Day One**

The first block of talks on Friday was an overview of general issues related to artificial intelligence. Nick Bostrom, author of *Superintelligence* and head of the Future of Humanity Institute, started with something of a barrage of all the general ideas and things he's come up with. He floated the idea that perhaps we shouldn't program AI systems to be maximally moral, for we don't know what the true morality looks like, and what if it turns out that such a directive would lead to humans being punished, or something else that was pathological or downright weird? He also described three principles for how we should treat AIs: substrate nondiscrimination (moral status does not depend on the kind of hardware/wetware you run on), ontogeny nondiscrimination (moral status does not depend on how you were created), and subjective time (moral value exists relative to subjectively experienced time rather than objective time, so if a mind ran at a fast clock speed its life would be more important, all other things being equal).

He pointed out that AI moral status could arise before they reach there is any such thing as human level AI - just like animals have moral status despite being much simpler than humans. He mentioned the possibility of a Malthusian catastrophe from unlimited digital reproduction as well as the possibility for vote manipulation through agent duplication, and how we'll need to prevent these two things.

He voiced support for meta level decisionmaking - a ['moral parliament'](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html) where we imagine moral theories sending 'delegates' to compromise over contentious issues. Such a system could also accommodate other values and interests besides moral theories.

He answered the question of ""what is humanity most likely to fail at?"" with a qualified choice of 'mind crime' committed against advanced AIs. Humans already have difficulty with empathy towards animals when they exist on farms or in the wild, but AI would not necessarily have the basic biological features which incline us to be empathetic at all towards animals. Some robots attract empathetic attention from humans, but many invisible automated processes are much harder for people to feel empathetic towards.

Virginia Dignum was next; she is at the Delft University of Technology and spoke about mechanisms for automated processes to make decisions. She specified four methods of decisionmaking based on whether decisions are taken deliberately or imposed upon a system and whether the decisions are made internally or externally. The two former features lead to algorithmic decisionmaking in machines; the latter two lead to imposed decisions predetermined by regulatory institutions. Deliberated external decisionmaking means there is a 'human in the loop' and internal imposed decisionmaking is essentially randomness.

Yann LeCun concluded this section with a pretty fantastic overview of deep learning methods and the limitations which stand in the way of progress in machine intelligence. He pointed out that reinforcement learning is a rare and narrow slice of the field today and that the greatest obstacles for machines include common sense judgements and abstraction. The biggest current problem for AI is unsupervised learning, which is having machines that can learn to classify things on their own without being given clearly labelled data from humans. He showcased some of the (very cool) features of adversarial learning which are being used to tackle this.

He expressed support for the orthogonality thesis, namely the idea that intelligence and morality are 'orthogonal' - just because an agent is very smart doesn't mean that it's necessarily moral. He believes we should build a few basic drives into AIs: do not hurt humans, interact with humans, crave positive feedback from trusted human trainers. He also described a couple of reasons for why he is not concerned about uncontrolled advanced artificial intelligence. One was that he is confident that objective functions can be specified in such a way as to make machines indifferent to being switched off, and the other is that a narrow-AI focused on eliminating an unfriendly general-AI would 'win' due to its specialization.

In Q&A, Stuart Russell objected to LeCun's confidence in machines being indifferent to being shut off based on the fact that self-preservation as a goal implicitly falls out of whatever other goals a machine has. Paul Boghossian objected to the 'behaviorist' nature of the speakers' points of view, saying that they were exempting consciousness from its proper role in these discussions. One person asked whether we should let AIs take charge of everything and supersede humanity - Bostrom pointed out that the space of possible futures is ""an enormous Petri dish"" which we don't understand; an AI future could materialize as a planet sized supercomputer with no moral status, and we will need to learn how to engineer friendly advanced AI systems no matter what the plan is. 

The rest of the Friday talks were devoted to near-future issues with specific AI systems. Peter Asaro started with an overview of his organization, the 'Campaign to Stop Killer Robots'. He stated that targeting and killing should remain human-controlled actions. While he acknowledged that automated weaponry could result in fewer casualties on the battlefield, he believed that it was too narrow a view of the consequences. He said that it's not straightforward to translate complicated battlefield morality questions for machines to understand, and is worried about unintended initiation and escalation of conflicts through automated systems, arms races, and threats to humanitarian law. He also believes that people should only be killed with 'dignity' and that doing it with a robot robs people of this. Therefore, he called for a clear and strong norm against automated weapons.

Kate Devlin of the University of London gave a brief overview of the ethics of artificial sexuality. Looking at the history of sexualized robots featured in fictional media, she noted that almost all of them are female. Today there is a ""Campaign Against Sex Robots"" which is based on the idea that sexual robots would lead to the objectification of women. Devlin does not agree as she thinks it is too early to ban the technology and that we should explore it before thinking about banning it, especially since it does not really harm anyone. Instead she wants us to think about how to develop it correctly. There are many potential uses for these types of robots ranging all the way to the therapeutic; many of the rudimentary ones being sold today are bought by people who are incapable of forming ordinary relationships for various reasons. VR is being used in arousal tests to gauge the efficacy of treatments against pedophilia.

She noted that gender issues have arisen in technology already; the history of gendered technology includes pacemakers originally designed only for men and phones too large for women's pockets. We should get into AI now to make sure that it is not designed in problematic ways.

She mentioned privacy concerns, as the manufactures of the female stimulator WeVibe have already been sued over concerns that they were not properly informing customers of their collection of data from the devices. She wondered if we will ever get to a stage where a robot might have some knowledge of its role and refuse to give consent to its use, and if transmission/duplication of data and code between machines could serve as some form of digital sexual reproduction.

Vasant Dhar of NYU spoke next about data and privacy in the era of autonomous vehicles. He said that our legal and financial liability institutions are based on outdated notions of data and that they fail to address liability and crime. However, the tools we have now even in ordinary cars for recording data can be used to improve insurance and judicial systems. He proposed black boxes for cars that would contain all relevant data to determine fault in the event of accidents, and said that customers should have the choice to share their driving data with insurance companies to get lower premiums. 

Dhar reiterated the importance of improving vehicle safety through autonomous driving; each percentage point reduction in vehicle accidents equates to 400 deaths and 40,000 injuries avoided every year.

Adam Kolber followed up with a discussion of whether ""the code is the law"", based on the case study of [The DAO](https://en.wikipedia.org/wiki/The_DAO_%28organization%29) which was an automated capital fund which was subjected to a $50 million loss through exploitation. The answer apparently is that the code should not be the law, even though many people seemed to accept that it was.

Steve Wolfram of WolframAlpha and Mathematica fame discussed the issues of computer languages and goal specification. He said that his life work has essentially been about trying to find ways for humans to specify their goals to machines, and that this can work for ethics as well as for math. He doesn't think that any single moral theory is likely to work for guiding artificial intelligence, apparently because of Godel's theorem and the incompleteness of computational languages.

Francesca Rossi of IBM argued that for AIs and humans to interact very productively we will have to embed them in environments, so that rather than picking up a tool like a laptop or a phone, we are interacting with artificial systems all around is in our rooms and spaces. Humans will be recognized by their environments and our needs and wants will be inferred or asked about. AI embedded in environments can have memories about humans to better serve their interests. Most of all, we will need to establish trust between humans and AIs.

Peter Railton, philosopher at the University of Michigan, attacked the subjects of orthogonality and value learning. He said that we can't simply tell AIs to do what we want because our wants and values require critical assessment. He said that the orthogonality thesis might be right, but as we increasingly interact with systems and allow them to participate in our own lives and decisionmaking, the question of what it would take for them to be intelligent might involve certain features relevant to morality.

He stated that AIs should be thought of as social creatures; as a simple model, self regulation in a Hobbesian social contract leads to constraints and respect derived from self preservation. A society of intelligent cooperators can resist aggression and malice, and being moral is more efficient for a community than being cunning. From these principles we have a recipe for building proto-moral agents.

He discussed the 'moral point of view' required for many strong ethical theories such as Kantian ethics and consequentialism: it requires agents to have a hierarchical, non-perspectival, modal/planning-oriented, and consistent view of the world which assigns intrinsic moral weight to things. He described how all these features are also part of the process of becoming generally intelligent in the first place, implying that general social intelligence ensures the necessary information required for moral decisionmaking. In the path towards functional moral agents, we will have to build agents which can represent the goals of others and have them learn how to act in beneficial ways. So if we can build AIs that we can trust, then we are on a good path towards building artificial moral agents.

In the Q&A, Eliezer Yudkowsky objected that in the long run the 'instrumental strategy' is not quite what you want because maximizing people's desires as they are explicitly revealed can lead to bad outcomes, and you have to have a view like coherent extrapolated volition which asks what people would really want. Russell objected that when an agent becomes sufficiently powerful, it has no need to cooperate anymore.

Regina Rini of the NYU Center for Bioethics stated that the approaches to ethics so far described relied too much on the Western post-enlightenment view of ethics, which is a historical aberration, and excluded African, Chinese and other approaches to ethics. Railton stated that his scheme was grounded in basic empathy and not mediated by any higher order moral theory; Wolfram and Rossi said that no one ethical approach will work and AI will have to represent diverse values.

**Day Two**

Saturday was devoted to long term discussion of the future of advanced artificial intelligence. Stuart Russell, professor at UC Berkeley and head of the new Center for Human Compatible Artificial Intelligence, started with a basic overview of the control problem. He described the points made in Steve Omohundro's paper on convergent instrumental drives. He also had some pretty harsh words for the researchers in the AI community which have denied and rejected notions of the control problem without seriously engaging with the relevant literature.

He had three simple ideas which he proposed to constitute the definition of 'provably beneficial' AI: maximizing values for humans is the system's only goal; the robot is initially uncertain about these goals, and the best source of information is human behavior. He referred to inverse reinforcement learning as a technique for machines to learn human preferences, and said that uncertainty provides an incentive for machines to learn, ask questions, and explore cautiously. 

His answer to the off-switch problem is to make robots unsure of their objectives, so that they assume that the human will switch the robot off if and only if it has a good reason to, and will therefore be complicit with the action. He said that the wireheading problem can be avoided if you construct the reward signal as information about the reward function rather than as a reward itself; this way, any hijacking of the reward signal makes it useless.

He said that there is a strong economic incentive for value alignment, but humans are irrational, nasty, inconsistent, and weak-willed.

The next speaker was Eliezer Yudkowsky of the Machine Intelligence Research Institute. Chalmers pointed out his role there as well as his side venture in Harry Potter fanfiction.

Yudkowsky started [his talk](https://intelligence.org/nyu-talk/) by pointing out how the Terminator pictures in every media article about the control problem are inappropriate. The real analogy to be used is [Mickey Mouse as the Sorcerer's Apprentice in *Fantasia*.](https://www.youtube.com/watch?v=Ait_Fs6UQhQ) 

He said that the first difficulty of AI alignnment is that the utility functions we imagine are too simple, and the second difficulty is that maximizing the probability of achieving a given goal leads to pathological outcomes. He and MIRI are concerned with the nature of the goal of 'maximizing' and how to define goals in a way that avoids the problems of perverse instantiation. 

He said that the fears of AI being developed by some terrorist or rogue group were silly, as ""ISIS is not developing convolutional neural nets."" Instead the most powerful AI is likely to be developed by large groups in government, academia and industry.

He claimed that the four central propositions which support the idea that AI is a very big problem are: the orthogonality thesis, instrumental convergence, capability gain (the speed at which advanced AI can make itself better), and alignment difficulty. He said the first two are logical matters of computer science that people always learn to accept when they reflect upon them, while the latter two are more controversial.

The next talk was from Max Tegmark and Meia Chita-Tegmark. Max is a world-renowned physicist who helps run the Future of Life Institute, and Meia is a psychologist. They explained how physics and psychology provide useful tools for understanding artificial intelligence; physics tells us about computation and the constraints of the universe, and psychology tells us about the nature of well being, ways to debug the mind when reasoning about AI and methods to design psychomorphic AIs. Meia was the only speaker at the conference to discuss unemployment in any detail; she pointed out that retirement has only mixed effects on well being and that happiness comes from financial satisfaction and feelings of respect. She said that studying homemakers, part time workers and early retirees can tell us more about how an automated economy would affect people's well-being.

Max checked off [a list of common myths](http://futureoflife.org/background/aimyths/) regarding advanced AI. Meia said that we should look at the cognitive biases which have led to these misconceptions (such as availability bias leading to people worrying about robots rather than invisible artificial intelligence) and figure out how to avoid similar bugs from inhibiting our thinking in the future.

By the way, Max Tegmark is very cool, he has a sort of old-rocker-dude vibe, and he and Meia are super cute together.

Wendell Wallach of Yale spoke next. He is the man who quite literally wrote [the book](https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975) on AI ethics. He distinguished top-down approaches of formally specifying AI behaviors from bottom-up approaches of value learning. He said that neither will be sufficient on its own and that both have important roles to play. He is worried that AI engineers will make simplistic assumptions about AI, such as the idea that every decision should be utilitarian or the idea that 'ethics' and 'morality' are icky concepts that can be ignored.

Steve Petersen, a philosopher at the University of Niagara, gave the next talk, based on the draft of a forthcoming paper of his. He aims to push back against the orthogonality thesis and modulate the level of the risk assessment provided by Bostrom. His argument is that designing AI to follow any complex goal will necessarily require it to be able to learn the values of its ""teleological ancestors"" (the original human designers or the previous iterations of AI before it self-improved or self-modified) and arrive at a state of coherence between goals. As agents replicate, self-modify and merge in the digital world, there can be no fact of the matter about which agents are the same or different; instead there will be an 'agential soup' unified by a common teleological thread originating with the designers. Coherence reasoning leads to impartial reasoning with the goals of other agents.

There were several responses to him in Q&A. Yudkowsky's objection was that reaching coherence requires a meta-preference framework with particular assumptions about the universe and ontology; therefore, for any goal, there are many preference frameworks which could fulfill it, many of which would be perverse. Russell said that just coherence is not enough because you need the systems to give special weight to humans. Max Tegmark said that the problem was the vagueness of humanity's final goals. Chalmers pointed out that the orthogonality thesis still allows for all kinds of correlations between between intelligence and morality, as long as they are not necessary by design. Petersen said that he is arguing for 'attractor basins' in the possibility space of AI minds. Interestingly, he was motivated to start his research by the [Dylan Matthews Vox article](http://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai) on effective altruism where Dylan thought that effective altruists shouldn't be concerned by artificial intelligence. Petersen doesn't think that AI is unimportant and thinks that Bostrom and Yudkowsky's work is valuable, but he wanted to get a more critical assessment of the level of risk when he learned that alternative altruistic projects were at stake.

Matthew Liao of the NYU Center for Bioethics gave an argument for moral status on the basis of capabilities - that an entity is morally valuable to the extent that it has the physical/genetic basis for achieving features of moral relevance. I did not get a chance to ask him if this would imply that a 'seed AI' could be the most morally valuable entity in the world. He did argue against the ideas that level of intelligence or degree of moral agency determine moral status, as we don't normally think that smarter or more benevolent humans are more morally valuable than others. 

Liao argued that moral theories are too specific and too high level to be generally implemented in AIs. Instead, AI will need a universal moral grammar in which to specify morality. The holy grail is to develop machines that understand why things are right or wrong.

Eric Schwitzgebel and Mara Garza of UC Riverside argued for basic principles of AI rights. They introduced a very weak ""no-relevant-difference"" argument: the idea that there are possible AIs which have the same morally relevant features that humans do and therefore there are possible AIs with equal value to humans. They questioned if cheerfully suicidal or hardworking AI is acceptable, and stated a 'self respect principle': that human grade AI should be designed with an appropriate appreciation of its own value.

John Basl and Ronald Sandler of Northeastern University argued for AI research committees to approve or deny research in cases where AI subjects might be harmed. They said it would not be very different from cases like animal testing where we have similar review boards, and sketched out details of how the proposal would work.

Daniel Kahneman, one of the most famous behavioral economists in the world, made something of a surprise appearance in the final panel. He said that we should take intuitions about case studies like the trolley problem seriously, as that is how the public will think about these events, for better or for worse. He said that no matter how AI cars kill people, it will be perceived with horror whenever the first incident happens, and we should prepare for that. Intuitions depend on irrelevant factors and will especially depend on whether AIs are designed to resemble us or not.

Gary Marcus, professor of psychology at NYU, of gave a much needed presentation about the nature of intelligence. The previous talks in this discussion had mostly assumed that intelligence was one-dimensional and simple and that there was some fixed idea of 'human-level' AI which we could eventually reach. Of course this is a ridiculous oversimplification; intelligence is multidimensional and it is more about implementing a combination of various cognitive tools, some of which are already stronger in AIs than in humans. AIs can be better or worse than us in various domains, so we really have no idea where AIs will be in this multidimensional space. AIs could in fact be better than us at moral reasoning. He also emphasized the gap is between machine learning today and what human reasoning can do.

Susan Schneider of Marquette University, a philosopher who has written quite a bit about AI and superintelligence, went over various issues. She argued that mind uploads might constitute death of the individual as long as we don't prove certain ideas about consciousness and personal identity, and also claimed that designing an intelligent and morally valuable robot to serve the interests of its creators would constitute slavery.

Jaan Tallinn, founder of Skype, also gave a quick talk. He has been a strong financial backer for MIRI and other efforts in this space, and simply expressed his belief in the importance of the issue and his happiness at the success of the conference and the number of students who were interested in pursuing the topic.

There was some final banter about the nature of consciousness which David Chalmers sat through very passively. Yudkowsky expressed optimism that one day we will have an explanation of consciousness which clears up our confusion on the matter. Nagel said that we will need to think more about the dynamics of multi-agent systems and moral epistemology. After that the event ended.

The conference videos are available [here.](http://livestream.com/nyu-tv/ethicsofAI) In my opinion, the best talks were given by LeCun, Railton, Russell, Yudkowsky, the Tegmarks, Petersen, and Marcus. The event overall was great and being in Manhattan made it even better. There was quite a bit of valuable informal meeting and discussion between many of the speakers and attendees. There was no 'sneering' or disdain about Yudkowsky or Bostrom as far as I could tell. It seemed like a generally open minded yet well educated crowd.

If you regret missing it, then you might like to head to the [Envision Conference](http://envision-conference.com/) this December. 

"
Creating robots capable of moral reasoning is like parenting – Regina Rini
"Notes from the NYU AI Ethics conferenceThis weekend I attended the [Ethics of Artificial Intelligence conference](https://wp.nyu.edu/consciousness/ethics-of-artificial-intelligence/) at NYU. There were a ton of high-profile and interesting people there from philosophy (David Chalmers, Peter Railton, Nick Bostrom, Thomas Nagel, Paul Boghossian, Frances Kamm, Wendell Wallach) and science (Yann LeCun, Stuart Russell, Stephen Wolfram, Max Tegmark, Francesca Rossi) as well as Eliezer Yudkowsky. 

There were two fairly long days of talks and panels. David Chalmers (famous for his philosophy of mind and consciousness) did not officially speak but acted as chair for the event. He outlined the philosophy of the conference, which was to discuss both short and long term issues in AI ethics without worrying about either detracting from the other. He was, as usual, extremely awesome.

Here is a summary of the event with the most interesting points made by the speakers.

**Day One**

The first block of talks on Friday was an overview of general issues related to artificial intelligence. Nick Bostrom, author of *Superintelligence* and head of the Future of Humanity Institute, started with something of a barrage of all the general ideas and things he's come up with. He floated the idea that perhaps we shouldn't program AI systems to be maximally moral, for we don't know what the true morality looks like, and what if it turns out that such a directive would lead to humans being punished, or something else that was pathological or downright weird? He also described three principles for how we should treat AIs: substrate nondiscrimination (moral status does not depend on the kind of hardware/wetware you run on), ontogeny nondiscrimination (moral status does not depend on how you were created), and subjective time (moral value exists relative to subjectively experienced time rather than objective time, so if a mind ran at a fast clock speed its life would be more important, all other things being equal).

He pointed out that AI moral status could arise before they reach there is any such thing as human level AI - just like animals have moral status despite being much simpler than humans. He mentioned the possibility of a Malthusian catastrophe from unlimited digital reproduction as well as the possibility for vote manipulation through agent duplication, and how we'll need to prevent these two things.

He voiced support for meta level decisionmaking - a ['moral parliament'](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html) where we imagine moral theories sending 'delegates' to compromise over contentious issues. Such a system could also accommodate other values and interests besides moral theories.

He answered the question of ""what is humanity most likely to fail at?"" with a qualified choice of 'mind crime' committed against advanced AIs. Humans already have difficulty with empathy towards animals when they exist on farms or in the wild, but AI would not necessarily have the basic biological features which incline us to be empathetic at all towards animals. Some robots attract empathetic attention from humans, but many invisible automated processes are much harder for people to feel empathetic towards.

Virginia Dignum was next; she is at the Delft University of Technology and spoke about mechanisms for automated processes to make decisions. She specified four methods of decisionmaking based on whether decisions are taken deliberately or imposed upon a system and whether the decisions are made internally or externally. The two former features lead to algorithmic decisionmaking in machines; the latter two lead to imposed decisions predetermined by regulatory institutions. Deliberated external decisionmaking means there is a 'human in the loop' and internal imposed decisionmaking is essentially randomness.

Yann LeCun concluded this section with a pretty fantastic overview of deep learning methods and the limitations which stand in the way of progress in machine intelligence. He pointed out that reinforcement learning is a rare and narrow slice of the field today and that the greatest obstacles for machines include common sense judgements and abstraction. The biggest current problem for AI is unsupervised learning, which is having machines that can learn to classify things on their own without being given clearly labelled data from humans. He showcased some of the (very cool) features of adversarial learning which are being used to tackle this.

He expressed support for the orthogonality thesis, namely the idea that intelligence and morality are 'orthogonal' - just because an agent is very smart doesn't mean that it's necessarily moral. He believes we should build a few basic drives into AIs: do not hurt humans, interact with humans, crave positive feedback from trusted human trainers. He also described a couple of reasons for why he is not concerned about uncontrolled advanced artificial intelligence. One was that he is confident that objective functions can be specified in such a way as to make machines indifferent to being switched off, and the other is that a narrow-AI focused on eliminating an unfriendly general-AI would 'win' due to its specialization.

In Q&A, Stuart Russell objected to LeCun's confidence in machines being indifferent to being shut off based on the fact that self-preservation as a goal implicitly falls out of whatever other goals a machine has. Paul Boghossian objected to the 'behaviorist' nature of the speakers' points of view, saying that they were exempting consciousness from its proper role in these discussions. One person asked whether we should let AIs take charge of everything and supersede humanity - Bostrom pointed out that the space of possible futures is ""an enormous Petri dish"" which we don't understand; an AI future could materialize as a planet sized supercomputer with no moral status, and we will need to learn how to engineer friendly advanced AI systems no matter what the plan is. 

The rest of the Friday talks were devoted to near-future issues with specific AI systems. Peter Asaro started with an overview of his organization, the 'Campaign to Stop Killer Robots'. He stated that targeting and killing should remain human-controlled actions. While he acknowledged that automated weaponry could result in fewer casualties on the battlefield, he believed that it was too narrow a view of the consequences. He said that it's not straightforward to translate complicated battlefield morality questions for machines to understand, and is worried about unintended initiation and escalation of conflicts through automated systems, arms races, and threats to humanitarian law. He also believes that people should only be killed with 'dignity' and that doing it with a robot robs people of this. Therefore, he called for a clear and strong norm against automated weapons.

Kate Devlin of the University of London gave a brief overview of the ethics of artificial sexuality. Looking at the history of sexualized robots featured in fictional media, she noted that almost all of them are female. Today there is a ""Campaign Against Sex Robots"" which is based on the idea that sexual robots would lead to the objectification of women. Devlin does not agree as she thinks it is too early to ban the technology and that we should explore it before thinking about banning it, especially since it does not really harm anyone. Instead she wants us to think about how to develop it correctly. There are many potential uses for these types of robots ranging all the way to the therapeutic; many of the rudimentary ones being sold today are bought by people who are incapable of forming ordinary relationships for various reasons. VR is being used in arousal tests to gauge the efficacy of treatments against pedophilia.

She noted that gender issues have arisen in technology already; the history of gendered technology includes pacemakers originally designed only for men and phones too large for women's pockets. We should get into AI now to make sure that it is not designed in problematic ways.

She mentioned privacy concerns, as the manufactures of the female stimulator WeVibe have already been sued over concerns that they were not properly informing customers of their collection of data from the devices. She wondered if we will ever get to a stage where a robot might have some knowledge of its role and refuse to give consent to its use, and if transmission/duplication of data and code between machines could serve as some form of digital sexual reproduction.

Vasant Dhar of NYU spoke next about data and privacy in the era of autonomous vehicles. He said that our legal and financial liability institutions are based on outdated notions of data and that they fail to address liability and crime. However, the tools we have now even in ordinary cars for recording data can be used to improve insurance and judicial systems. He proposed black boxes for cars that would contain all relevant data to determine fault in the event of accidents, and said that customers should have the choice to share their driving data with insurance companies to get lower premiums. 

Dhar reiterated the importance of improving vehicle safety through autonomous driving; each percentage point reduction in vehicle accidents equates to 400 deaths and 40,000 injuries avoided every year.

Adam Kolber followed up with a discussion of whether ""the code is the law"", based on the case study of [The DAO](https://en.wikipedia.org/wiki/The_DAO_%28organization%29) which was an automated capital fund which was subjected to a $50 million loss through exploitation. The answer apparently is that the code should not be the law, even though many people seemed to accept that it was.

Steve Wolfram of WolframAlpha and Mathematica fame discussed the issues of computer languages and goal specification. He said that his life work has essentially been about trying to find ways for humans to specify their goals to machines, and that this can work for ethics as well as for math. He doesn't think that any single moral theory is likely to work for guiding artificial intelligence, apparently because of Godel's theorem and the incompleteness of computational languages.

Francesca Rossi of IBM argued that for AIs and humans to interact very productively we will have to embed them in environments, so that rather than picking up a tool like a laptop or a phone, we are interacting with artificial systems all around is in our rooms and spaces. Humans will be recognized by their environments and our needs and wants will be inferred or asked about. AI embedded in environments can have memories about humans to better serve their interests. Most of all, we will need to establish trust between humans and AIs.

Peter Railton, philosopher at the University of Michigan, attacked the subjects of orthogonality and value learning. He said that we can't simply tell AIs to do what we want because our wants and values require critical assessment. He said that the orthogonality thesis might be right, but as we increasingly interact with systems and allow them to participate in our own lives and decisionmaking, the question of what it would take for them to be intelligent might involve certain features relevant to morality.

He stated that AIs should be thought of as social creatures; as a simple model, self regulation in a Hobbesian social contract leads to constraints and respect derived from self preservation. A society of intelligent cooperators can resist aggression and malice, and being moral is more efficient for a community than being cunning. From these principles we have a recipe for building proto-moral agents.

He discussed the 'moral point of view' required for many strong ethical theories such as Kantian ethics and consequentialism: it requires agents to have a hierarchical, non-perspectival, modal/planning-oriented, and consistent view of the world which assigns intrinsic moral weight to things. He described how all these features are also part of the process of becoming generally intelligent in the first place, implying that general social intelligence ensures the necessary information required for moral decisionmaking. In the path towards functional moral agents, we will have to build agents which can represent the goals of others and have them learn how to act in beneficial ways. So if we can build AIs that we can trust, then we are on a good path towards building artificial moral agents.

In the Q&A, Eliezer Yudkowsky objected that in the long run the 'instrumental strategy' is not quite what you want because maximizing people's desires as they are explicitly revealed can lead to bad outcomes, and you have to have a view like coherent extrapolated volition which asks what people would really want. Russell objected that when an agent becomes sufficiently powerful, it has no need to cooperate anymore.

Regina Rini of the NYU Center for Bioethics stated that the approaches to ethics so far described relied too much on the Western post-enlightenment view of ethics, which is a historical aberration, and excluded African, Chinese and other approaches to ethics. Railton stated that his scheme was grounded in basic empathy and not mediated by any higher order moral theory; Wolfram and Rossi said that no one ethical approach will work and AI will have to represent diverse values.

**Day Two**

Saturday was devoted to long term discussion of the future of advanced artificial intelligence. Stuart Russell, professor at UC Berkeley and head of the new Center for Human Compatible Artificial Intelligence, started with a basic overview of the control problem. He described the points made in Steve Omohundro's paper on convergent instrumental drives. He also had some pretty harsh words for the researchers in the AI community which have denied and rejected notions of the control problem without seriously engaging with the relevant literature.

He had three simple ideas which he proposed to constitute the definition of 'provably beneficial' AI: maximizing values for humans is the system's only goal; the robot is initially uncertain about these goals, and the best source of information is human behavior. He referred to inverse reinforcement learning as a technique for machines to learn human preferences, and said that uncertainty provides an incentive for machines to learn, ask questions, and explore cautiously. 

His answer to the off-switch problem is to make robots unsure of their objectives, so that they assume that the human will switch the robot off if and only if it has a good reason to, and will therefore be complicit with the action. He said that the wireheading problem can be avoided if you construct the reward signal as information about the reward function rather than as a reward itself; this way, any hijacking of the reward signal makes it useless.

He said that there is a strong economic incentive for value alignment, but humans are irrational, nasty, inconsistent, and weak-willed.

The next speaker was Eliezer Yudkowsky of the Machine Intelligence Research Institute. Chalmers pointed out his role there as well as his side venture in Harry Potter fanfiction.

Yudkowsky started [his talk](https://intelligence.org/nyu-talk/) by pointing out how the Terminator pictures in every media article about the control problem are inappropriate. The real analogy to be used is [Mickey Mouse as the Sorcerer's Apprentice in *Fantasia*.](https://www.youtube.com/watch?v=Ait_Fs6UQhQ) 

He said that the first difficulty of AI alignnment is that the utility functions we imagine are too simple, and the second difficulty is that maximizing the probability of achieving a given goal leads to pathological outcomes. He and MIRI are concerned with the nature of the goal of 'maximizing' and how to define goals in a way that avoids the problems of perverse instantiation. 

He said that the fears of AI being developed by some terrorist or rogue group were silly, as ""ISIS is not developing convolutional neural nets."" Instead the most powerful AI is likely to be developed by large groups in government, academia and industry.

He claimed that the four central propositions which support the idea that AI is a very big problem are: the orthogonality thesis, instrumental convergence, capability gain (the speed at which advanced AI can make itself better), and alignment difficulty. He said the first two are logical matters of computer science that people always learn to accept when they reflect upon them, while the latter two are more controversial.

The next talk was from Max Tegmark and Meia Chita-Tegmark. Max is a world-renowned physicist who helps run the Future of Life Institute, and Meia is a psychologist. They explained how physics and psychology provide useful tools for understanding artificial intelligence; physics tells us about computation and the constraints of the universe, and psychology tells us about the nature of well being, ways to debug the mind when reasoning about AI and methods to design psychomorphic AIs. Meia was the only speaker at the conference to discuss unemployment in any detail; she pointed out that retirement has only mixed effects on well being and that happiness comes from financial satisfaction and feelings of respect. She said that studying homemakers, part time workers and early retirees can tell us more about how an automated economy would affect people's well-being.

Max checked off [a list of common myths](http://futureoflife.org/background/aimyths/) regarding advanced AI. Meia said that we should look at the cognitive biases which have led to these misconceptions (such as availability bias leading to people worrying about robots rather than invisible artificial intelligence) and figure out how to avoid similar bugs from inhibiting our thinking in the future.

By the way, Max Tegmark is very cool, he has a sort of old-rocker-dude vibe, and he and Meia are super cute together.

Wendell Wallach of Yale spoke next. He is the man who quite literally wrote [the book](https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975) on AI ethics. He distinguished top-down approaches of formally specifying AI behaviors from bottom-up approaches of value learning. He said that neither will be sufficient on its own and that both have important roles to play. He is worried that AI engineers will make simplistic assumptions about AI, such as the idea that every decision should be utilitarian or the idea that 'ethics' and 'morality' are icky concepts that can be ignored.

Steve Petersen, a philosopher at the University of Niagara, gave the next talk, based on the draft of a forthcoming paper of his. He aims to push back against the orthogonality thesis and modulate the level of the risk assessment provided by Bostrom. His argument is that designing AI to follow any complex goal will necessarily require it to be able to learn the values of its ""teleological ancestors"" (the original human designers or the previous iterations of AI before it self-improved or self-modified) and arrive at a state of coherence between goals. As agents replicate, self-modify and merge in the digital world, there can be no fact of the matter about which agents are the same or different; instead there will be an 'agential soup' unified by a common teleological thread originating with the designers. Coherence reasoning leads to impartial reasoning with the goals of other agents.

There were several responses to him in Q&A. Yudkowsky's objection was that reaching coherence requires a meta-preference framework with particular assumptions about the universe and ontology; therefore, for any goal, there are many preference frameworks which could fulfill it, many of which would be perverse. Russell said that just coherence is not enough because you need the systems to give special weight to humans. Max Tegmark said that the problem was the vagueness of humanity's final goals. Chalmers pointed out that the orthogonality thesis still allows for all kinds of correlations between between intelligence and morality, as long as they are not necessary by design. Petersen said that he is arguing for 'attractor basins' in the possibility space of AI minds. Interestingly, he was motivated to start his research by the [Dylan Matthews Vox article](http://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai) on effective altruism where Dylan thought that effective altruists shouldn't be concerned by artificial intelligence. Petersen doesn't think that AI is unimportant and thinks that Bostrom and Yudkowsky's work is valuable, but he wanted to get a more critical assessment of the level of risk when he learned that alternative altruistic projects were at stake.

Matthew Liao of the NYU Center for Bioethics gave an argument for moral status on the basis of capabilities - that an entity is morally valuable to the extent that it has the physical/genetic basis for achieving features of moral relevance. I did not get a chance to ask him if this would imply that a 'seed AI' could be the most morally valuable entity in the world. He did argue against the ideas that level of intelligence or degree of moral agency determine moral status, as we don't normally think that smarter or more benevolent humans are more morally valuable than others. 

Liao argued that moral theories are too specific and too high level to be generally implemented in AIs. Instead, AI will need a universal moral grammar in which to specify morality. The holy grail is to develop machines that understand why things are right or wrong.

Eric Schwitzgebel and Mara Garza of UC Riverside argued for basic principles of AI rights. They introduced a very weak ""no-relevant-difference"" argument: the idea that there are possible AIs which have the same morally relevant features that humans do and therefore there are possible AIs with equal value to humans. They questioned if cheerfully suicidal or hardworking AI is acceptable, and stated a 'self respect principle': that human grade AI should be designed with an appropriate appreciation of its own value.

John Basl and Ronald Sandler of Northeastern University argued for AI research committees to approve or deny research in cases where AI subjects might be harmed. They said it would not be very different from cases like animal testing where we have similar review boards, and sketched out details of how the proposal would work.

Daniel Kahneman, one of the most famous behavioral economists in the world, made something of a surprise appearance in the final panel. He said that we should take intuitions about case studies like the trolley problem seriously, as that is how the public will think about these events, for better or for worse. He said that no matter how AI cars kill people, it will be perceived with horror whenever the first incident happens, and we should prepare for that. Intuitions depend on irrelevant factors and will especially depend on whether AIs are designed to resemble us or not.

Gary Marcus, professor of psychology at NYU, of gave a much needed presentation about the nature of intelligence. The previous talks in this discussion had mostly assumed that intelligence was one-dimensional and simple and that there was some fixed idea of 'human-level' AI which we could eventually reach. Of course this is a ridiculous oversimplification; intelligence is multidimensional and it is more about implementing a combination of various cognitive tools, some of which are already stronger in AIs than in humans. AIs can be better or worse than us in various domains, so we really have no idea where AIs will be in this multidimensional space. AIs could in fact be better than us at moral reasoning. He also emphasized the gap is between machine learning today and what human reasoning can do.

Susan Schneider of Marquette University, a philosopher who has written quite a bit about AI and superintelligence, went over various issues. She argued that mind uploads might constitute death of the individual as long as we don't prove certain ideas about consciousness and personal identity, and also claimed that designing an intelligent and morally valuable robot to serve the interests of its creators would constitute slavery.

Jaan Tallinn, founder of Skype, also gave a quick talk. He has been a strong financial backer for MIRI and other efforts in this space, and simply expressed his belief in the importance of the issue and his happiness at the success of the conference and the number of students who were interested in pursuing the topic.

There was some final banter about the nature of consciousness which David Chalmers sat through very passively. Yudkowsky expressed optimism that one day we will have an explanation of consciousness which clears up our confusion on the matter. Nagel said that we will need to think more about the dynamics of multi-agent systems and moral epistemology. After that the event ended.

The conference videos are available [here.](http://livestream.com/nyu-tv/ethicsofAI) In my opinion, the best talks were given by LeCun, Railton, Russell, Yudkowsky, the Tegmarks, Petersen, and Marcus. The event overall was great and being in Manhattan made it even better. There was quite a bit of valuable informal meeting and discussion between many of the speakers and attendees. There was no 'sneering' or disdain about Yudkowsky or Bostrom as far as I could tell. It seemed like a generally open minded yet well educated crowd.

If you regret missing it, then you might like to head to the [Envision Conference](http://envision-conference.com/) this December. 

"
"The Economics of Artificial IntelligenceI have spent a considerable amount of time considering AI and its affects on the economy.  It seems a given that robotics well replace the vast majority of low skilled work in the next two decades and it seems to be a given that a lot of high skilled jobs having to do with pattern recognition and statistics will be eliminated as well.  

It is my hypithosis that a lot of high intellect jobs or shall we say specialized career fields will be made more generalized due to advances in AI's ability to handle large portions of complex tasks.  

What I am interested in is opinions on what this will mean for society.  Does it mean larger groups of people will have an adequate ability to compete for an ever shrinking number of jobs?  Does it mean only the cognitive elite will be in a position to work?  And in either scenerio what happens to those who are left without prospects."
"The Economics of Artificial IntelligenceI have spent a considerable amount of time considering AI and its affects on the economy.  It seems a given that robotics well replace the vast majority of low skilled work in the next two decades and it seems to be a given that a lot of high skilled jobs having to do with pattern recognition and statistics will be eliminated as well.  

It is my hypithosis that a lot of high intellect jobs or shall we say specialized career fields will be made more generalized due to advances in AI's ability to handle large portions of complex tasks.  

What I am interested in is opinions on what this will mean for society.  Does it mean larger groups of people will have an adequate ability to compete for an ever shrinking number of jobs?  Does it mean only the cognitive elite will be in a position to work?  And in either scenerio what happens to those who are left without prospects."
"Notes from the NYU AI Ethics conferenceThis weekend I attended the [Ethics of Artificial Intelligence conference](https://wp.nyu.edu/consciousness/ethics-of-artificial-intelligence/) at NYU. There were a ton of high-profile and interesting people there from philosophy (David Chalmers, Peter Railton, Nick Bostrom, Thomas Nagel, Paul Boghossian, Frances Kamm, Wendell Wallach) and science (Yann LeCun, Stuart Russell, Stephen Wolfram, Max Tegmark, Francesca Rossi) as well as Eliezer Yudkowsky. 

There were two fairly long days of talks and panels. David Chalmers (famous for his philosophy of mind and consciousness) did not officially speak but acted as chair for the event. He outlined the philosophy of the conference, which was to discuss both short and long term issues in AI ethics without worrying about either detracting from the other. He was, as usual, extremely awesome.

Here is a summary of the event with the most interesting points made by the speakers.

**Day One**

The first block of talks on Friday was an overview of general issues related to artificial intelligence. Nick Bostrom, author of *Superintelligence* and head of the Future of Humanity Institute, started with something of a barrage of all the general ideas and things he's come up with. He floated the idea that perhaps we shouldn't program AI systems to be maximally moral, for we don't know what the true morality looks like, and what if it turns out that such a directive would lead to humans being punished, or something else that was pathological or downright weird? He also described three principles for how we should treat AIs: substrate nondiscrimination (moral status does not depend on the kind of hardware/wetware you run on), ontogeny nondiscrimination (moral status does not depend on how you were created), and subjective time (moral value exists relative to subjectively experienced time rather than objective time, so if a mind ran at a fast clock speed its life would be more important, all other things being equal).

He pointed out that AI moral status could arise before they reach there is any such thing as human level AI - just like animals have moral status despite being much simpler than humans. He mentioned the possibility of a Malthusian catastrophe from unlimited digital reproduction as well as the possibility for vote manipulation through agent duplication, and how we'll need to prevent these two things.

He voiced support for meta level decisionmaking - a ['moral parliament'](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html) where we imagine moral theories sending 'delegates' to compromise over contentious issues. Such a system could also accommodate other values and interests besides moral theories.

He answered the question of ""what is humanity most likely to fail at?"" with a qualified choice of 'mind crime' committed against advanced AIs. Humans already have difficulty with empathy towards animals when they exist on farms or in the wild, but AI would not necessarily have the basic biological features which incline us to be empathetic at all towards animals. Some robots attract empathetic attention from humans, but many invisible automated processes are much harder for people to feel empathetic towards.

Virginia Dignum was next; she is at the Delft University of Technology and spoke about mechanisms for automated processes to make decisions. She specified four methods of decisionmaking based on whether decisions are taken deliberately or imposed upon a system and whether the decisions are made internally or externally. The two former features lead to algorithmic decisionmaking in machines; the latter two lead to imposed decisions predetermined by regulatory institutions. Deliberated external decisionmaking means there is a 'human in the loop' and internal imposed decisionmaking is essentially randomness.

Yann LeCun concluded this section with a pretty fantastic overview of deep learning methods and the limitations which stand in the way of progress in machine intelligence. He pointed out that reinforcement learning is a rare and narrow slice of the field today and that the greatest obstacles for machines include common sense judgements and abstraction. The biggest current problem for AI is unsupervised learning, which is having machines that can learn to classify things on their own without being given clearly labelled data from humans. He showcased some of the (very cool) features of adversarial learning which are being used to tackle this.

He expressed support for the orthogonality thesis, namely the idea that intelligence and morality are 'orthogonal' - just because an agent is very smart doesn't mean that it's necessarily moral. He believes we should build a few basic drives into AIs: do not hurt humans, interact with humans, crave positive feedback from trusted human trainers. He also described a couple of reasons for why he is not concerned about uncontrolled advanced artificial intelligence. One was that he is confident that objective functions can be specified in such a way as to make machines indifferent to being switched off, and the other is that a narrow-AI focused on eliminating an unfriendly general-AI would 'win' due to its specialization.

In Q&A, Stuart Russell objected to LeCun's confidence in machines being indifferent to being shut off based on the fact that self-preservation as a goal implicitly falls out of whatever other goals a machine has. Paul Boghossian objected to the 'behaviorist' nature of the speakers' points of view, saying that they were exempting consciousness from its proper role in these discussions. One person asked whether we should let AIs take charge of everything and supersede humanity - Bostrom pointed out that the space of possible futures is ""an enormous Petri dish"" which we don't understand; an AI future could materialize as a planet sized supercomputer with no moral status, and we will need to learn how to engineer friendly advanced AI systems no matter what the plan is. 

The rest of the Friday talks were devoted to near-future issues with specific AI systems. Peter Asaro started with an overview of his organization, the 'Campaign to Stop Killer Robots'. He stated that targeting and killing should remain human-controlled actions. While he acknowledged that automated weaponry could result in fewer casualties on the battlefield, he believed that it was too narrow a view of the consequences. He said that it's not straightforward to translate complicated battlefield morality questions for machines to understand, and is worried about unintended initiation and escalation of conflicts through automated systems, arms races, and threats to humanitarian law. He also believes that people should only be killed with 'dignity' and that doing it with a robot robs people of this. Therefore, he called for a clear and strong norm against automated weapons.

Kate Devlin of the University of London gave a brief overview of the ethics of artificial sexuality. Looking at the history of sexualized robots featured in fictional media, she noted that almost all of them are female. Today there is a ""Campaign Against Sex Robots"" which is based on the idea that sexual robots would lead to the objectification of women. Devlin does not agree as she thinks it is too early to ban the technology and that we should explore it before thinking about banning it, especially since it does not really harm anyone. Instead she wants us to think about how to develop it correctly. There are many potential uses for these types of robots ranging all the way to the therapeutic; many of the rudimentary ones being sold today are bought by people who are incapable of forming ordinary relationships for various reasons. VR is being used in arousal tests to gauge the efficacy of treatments against pedophilia.

She noted that gender issues have arisen in technology already; the history of gendered technology includes pacemakers originally designed only for men and phones too large for women's pockets. We should get into AI now to make sure that it is not designed in problematic ways.

She mentioned privacy concerns, as the manufactures of the female stimulator WeVibe have already been sued over concerns that they were not properly informing customers of their collection of data from the devices. She wondered if we will ever get to a stage where a robot might have some knowledge of its role and refuse to give consent to its use, and if transmission/duplication of data and code between machines could serve as some form of digital sexual reproduction.

Vasant Dhar of NYU spoke next about data and privacy in the era of autonomous vehicles. He said that our legal and financial liability institutions are based on outdated notions of data and that they fail to address liability and crime. However, the tools we have now even in ordinary cars for recording data can be used to improve insurance and judicial systems. He proposed black boxes for cars that would contain all relevant data to determine fault in the event of accidents, and said that customers should have the choice to share their driving data with insurance companies to get lower premiums. 

Dhar reiterated the importance of improving vehicle safety through autonomous driving; each percentage point reduction in vehicle accidents equates to 400 deaths and 40,000 injuries avoided every year.

Adam Kolber followed up with a discussion of whether ""the code is the law"", based on the case study of [The DAO](https://en.wikipedia.org/wiki/The_DAO_%28organization%29) which was an automated capital fund which was subjected to a $50 million loss through exploitation. The answer apparently is that the code should not be the law, even though many people seemed to accept that it was.

Steve Wolfram of WolframAlpha and Mathematica fame discussed the issues of computer languages and goal specification. He said that his life work has essentially been about trying to find ways for humans to specify their goals to machines, and that this can work for ethics as well as for math. He doesn't think that any single moral theory is likely to work for guiding artificial intelligence, apparently because of Godel's theorem and the incompleteness of computational languages.

Francesca Rossi of IBM argued that for AIs and humans to interact very productively we will have to embed them in environments, so that rather than picking up a tool like a laptop or a phone, we are interacting with artificial systems all around is in our rooms and spaces. Humans will be recognized by their environments and our needs and wants will be inferred or asked about. AI embedded in environments can have memories about humans to better serve their interests. Most of all, we will need to establish trust between humans and AIs.

Peter Railton, philosopher at the University of Michigan, attacked the subjects of orthogonality and value learning. He said that we can't simply tell AIs to do what we want because our wants and values require critical assessment. He said that the orthogonality thesis might be right, but as we increasingly interact with systems and allow them to participate in our own lives and decisionmaking, the question of what it would take for them to be intelligent might involve certain features relevant to morality.

He stated that AIs should be thought of as social creatures; as a simple model, self regulation in a Hobbesian social contract leads to constraints and respect derived from self preservation. A society of intelligent cooperators can resist aggression and malice, and being moral is more efficient for a community than being cunning. From these principles we have a recipe for building proto-moral agents.

He discussed the 'moral point of view' required for many strong ethical theories such as Kantian ethics and consequentialism: it requires agents to have a hierarchical, non-perspectival, modal/planning-oriented, and consistent view of the world which assigns intrinsic moral weight to things. He described how all these features are also part of the process of becoming generally intelligent in the first place, implying that general social intelligence ensures the necessary information required for moral decisionmaking. In the path towards functional moral agents, we will have to build agents which can represent the goals of others and have them learn how to act in beneficial ways. So if we can build AIs that we can trust, then we are on a good path towards building artificial moral agents.

In the Q&A, Eliezer Yudkowsky objected that in the long run the 'instrumental strategy' is not quite what you want because maximizing people's desires as they are explicitly revealed can lead to bad outcomes, and you have to have a view like coherent extrapolated volition which asks what people would really want. Russell objected that when an agent becomes sufficiently powerful, it has no need to cooperate anymore.

Regina Rini of the NYU Center for Bioethics stated that the approaches to ethics so far described relied too much on the Western post-enlightenment view of ethics, which is a historical aberration, and excluded African, Chinese and other approaches to ethics. Railton stated that his scheme was grounded in basic empathy and not mediated by any higher order moral theory; Wolfram and Rossi said that no one ethical approach will work and AI will have to represent diverse values.

**Day Two**

Saturday was devoted to long term discussion of the future of advanced artificial intelligence. Stuart Russell, professor at UC Berkeley and head of the new Center for Human Compatible Artificial Intelligence, started with a basic overview of the control problem. He described the points made in Steve Omohundro's paper on convergent instrumental drives. He also had some pretty harsh words for the researchers in the AI community which have denied and rejected notions of the control problem without seriously engaging with the relevant literature.

He had three simple ideas which he proposed to constitute the definition of 'provably beneficial' AI: maximizing values for humans is the system's only goal; the robot is initially uncertain about these goals, and the best source of information is human behavior. He referred to inverse reinforcement learning as a technique for machines to learn human preferences, and said that uncertainty provides an incentive for machines to learn, ask questions, and explore cautiously. 

His answer to the off-switch problem is to make robots unsure of their objectives, so that they assume that the human will switch the robot off if and only if it has a good reason to, and will therefore be complicit with the action. He said that the wireheading problem can be avoided if you construct the reward signal as information about the reward function rather than as a reward itself; this way, any hijacking of the reward signal makes it useless.

He said that there is a strong economic incentive for value alignment, but humans are irrational, nasty, inconsistent, and weak-willed.

The next speaker was Eliezer Yudkowsky of the Machine Intelligence Research Institute. Chalmers pointed out his role there as well as his side venture in Harry Potter fanfiction.

Yudkowsky started [his talk](https://intelligence.org/nyu-talk/) by pointing out how the Terminator pictures in every media article about the control problem are inappropriate. The real analogy to be used is [Mickey Mouse as the Sorcerer's Apprentice in *Fantasia*.](https://www.youtube.com/watch?v=Ait_Fs6UQhQ) 

He said that the first difficulty of AI alignnment is that the utility functions we imagine are too simple, and the second difficulty is that maximizing the probability of achieving a given goal leads to pathological outcomes. He and MIRI are concerned with the nature of the goal of 'maximizing' and how to define goals in a way that avoids the problems of perverse instantiation. 

He said that the fears of AI being developed by some terrorist or rogue group were silly, as ""ISIS is not developing convolutional neural nets."" Instead the most powerful AI is likely to be developed by large groups in government, academia and industry.

He claimed that the four central propositions which support the idea that AI is a very big problem are: the orthogonality thesis, instrumental convergence, capability gain (the speed at which advanced AI can make itself better), and alignment difficulty. He said the first two are logical matters of computer science that people always learn to accept when they reflect upon them, while the latter two are more controversial.

The next talk was from Max Tegmark and Meia Chita-Tegmark. Max is a world-renowned physicist who helps run the Future of Life Institute, and Meia is a psychologist. They explained how physics and psychology provide useful tools for understanding artificial intelligence; physics tells us about computation and the constraints of the universe, and psychology tells us about the nature of well being, ways to debug the mind when reasoning about AI and methods to design psychomorphic AIs. Meia was the only speaker at the conference to discuss unemployment in any detail; she pointed out that retirement has only mixed effects on well being and that happiness comes from financial satisfaction and feelings of respect. She said that studying homemakers, part time workers and early retirees can tell us more about how an automated economy would affect people's well-being.

Max checked off [a list of common myths](http://futureoflife.org/background/aimyths/) regarding advanced AI. Meia said that we should look at the cognitive biases which have led to these misconceptions (such as availability bias leading to people worrying about robots rather than invisible artificial intelligence) and figure out how to avoid similar bugs from inhibiting our thinking in the future.

By the way, Max Tegmark is very cool, he has a sort of old-rocker-dude vibe, and he and Meia are super cute together.

Wendell Wallach of Yale spoke next. He is the man who quite literally wrote [the book](https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975) on AI ethics. He distinguished top-down approaches of formally specifying AI behaviors from bottom-up approaches of value learning. He said that neither will be sufficient on its own and that both have important roles to play. He is worried that AI engineers will make simplistic assumptions about AI, such as the idea that every decision should be utilitarian or the idea that 'ethics' and 'morality' are icky concepts that can be ignored.

Steve Petersen, a philosopher at the University of Niagara, gave the next talk, based on the draft of a forthcoming paper of his. He aims to push back against the orthogonality thesis and modulate the level of the risk assessment provided by Bostrom. His argument is that designing AI to follow any complex goal will necessarily require it to be able to learn the values of its ""teleological ancestors"" (the original human designers or the previous iterations of AI before it self-improved or self-modified) and arrive at a state of coherence between goals. As agents replicate, self-modify and merge in the digital world, there can be no fact of the matter about which agents are the same or different; instead there will be an 'agential soup' unified by a common teleological thread originating with the designers. Coherence reasoning leads to impartial reasoning with the goals of other agents.

There were several responses to him in Q&A. Yudkowsky's objection was that reaching coherence requires a meta-preference framework with particular assumptions about the universe and ontology; therefore, for any goal, there are many preference frameworks which could fulfill it, many of which would be perverse. Russell said that just coherence is not enough because you need the systems to give special weight to humans. Max Tegmark said that the problem was the vagueness of humanity's final goals. Chalmers pointed out that the orthogonality thesis still allows for all kinds of correlations between between intelligence and morality, as long as they are not necessary by design. Petersen said that he is arguing for 'attractor basins' in the possibility space of AI minds. Interestingly, he was motivated to start his research by the [Dylan Matthews Vox article](http://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai) on effective altruism where Dylan thought that effective altruists shouldn't be concerned by artificial intelligence. Petersen doesn't think that AI is unimportant and thinks that Bostrom and Yudkowsky's work is valuable, but he wanted to get a more critical assessment of the level of risk when he learned that alternative altruistic projects were at stake.

Matthew Liao of the NYU Center for Bioethics gave an argument for moral status on the basis of capabilities - that an entity is morally valuable to the extent that it has the physical/genetic basis for achieving features of moral relevance. I did not get a chance to ask him if this would imply that a 'seed AI' could be the most morally valuable entity in the world. He did argue against the ideas that level of intelligence or degree of moral agency determine moral status, as we don't normally think that smarter or more benevolent humans are more morally valuable than others. 

Liao argued that moral theories are too specific and too high level to be generally implemented in AIs. Instead, AI will need a universal moral grammar in which to specify morality. The holy grail is to develop machines that understand why things are right or wrong.

Eric Schwitzgebel and Mara Garza of UC Riverside argued for basic principles of AI rights. They introduced a very weak ""no-relevant-difference"" argument: the idea that there are possible AIs which have the same morally relevant features that humans do and therefore there are possible AIs with equal value to humans. They questioned if cheerfully suicidal or hardworking AI is acceptable, and stated a 'self respect principle': that human grade AI should be designed with an appropriate appreciation of its own value.

John Basl and Ronald Sandler of Northeastern University argued for AI research committees to approve or deny research in cases where AI subjects might be harmed. They said it would not be very different from cases like animal testing where we have similar review boards, and sketched out details of how the proposal would work.

Daniel Kahneman, one of the most famous behavioral economists in the world, made something of a surprise appearance in the final panel. He said that we should take intuitions about case studies like the trolley problem seriously, as that is how the public will think about these events, for better or for worse. He said that no matter how AI cars kill people, it will be perceived with horror whenever the first incident happens, and we should prepare for that. Intuitions depend on irrelevant factors and will especially depend on whether AIs are designed to resemble us or not.

Gary Marcus, professor of psychology at NYU, of gave a much needed presentation about the nature of intelligence. The previous talks in this discussion had mostly assumed that intelligence was one-dimensional and simple and that there was some fixed idea of 'human-level' AI which we could eventually reach. Of course this is a ridiculous oversimplification; intelligence is multidimensional and it is more about implementing a combination of various cognitive tools, some of which are already stronger in AIs than in humans. AIs can be better or worse than us in various domains, so we really have no idea where AIs will be in this multidimensional space. AIs could in fact be better than us at moral reasoning. He also emphasized the gap is between machine learning today and what human reasoning can do.

Susan Schneider of Marquette University, a philosopher who has written quite a bit about AI and superintelligence, went over various issues. She argued that mind uploads might constitute death of the individual as long as we don't prove certain ideas about consciousness and personal identity, and also claimed that designing an intelligent and morally valuable robot to serve the interests of its creators would constitute slavery.

Jaan Tallinn, founder of Skype, also gave a quick talk. He has been a strong financial backer for MIRI and other efforts in this space, and simply expressed his belief in the importance of the issue and his happiness at the success of the conference and the number of students who were interested in pursuing the topic.

There was some final banter about the nature of consciousness which David Chalmers sat through very passively. Yudkowsky expressed optimism that one day we will have an explanation of consciousness which clears up our confusion on the matter. Nagel said that we will need to think more about the dynamics of multi-agent systems and moral epistemology. After that the event ended.

The conference videos are available [here.](http://livestream.com/nyu-tv/ethicsofAI) In my opinion, the best talks were given by LeCun, Railton, Russell, Yudkowsky, the Tegmarks, Petersen, and Marcus. The event overall was great and being in Manhattan made it even better. There was quite a bit of valuable informal meeting and discussion between many of the speakers and attendees. There was no 'sneering' or disdain about Yudkowsky or Bostrom as far as I could tell. It seemed like a generally open minded yet well educated crowd.

If you regret missing it, then you might like to head to the [Envision Conference](http://envision-conference.com/) this December. 

"
Why robots need to be able to say 'No'
"Machine Ethics Reading ListThis is an overview of technical readings in machine ethics (developing moral frameworks for autonomous systems). I have less familiarity with other topics in AI ethics and have not done a review of the literature in those other fields, so I'm not making a reading list for all that at the moment.

**Papers**

Allen, C., Varner, G., & Zinser, J. (2000). Prolegomena to any future artificial moral agent. http://commonsenseatheism.com/wp-content/uploads/2009/08/Allen-Prolegomena-to-any-future-artificial-moral-agent.pdf

Anderson, M., Anderson, S. L., & Armen, C. (n.d.). Towards Machine Ethics: Implementing Two Action-Based Ethical Theories. https://www.aaai.org/Papers/Symposia/Fall/2005/FS-05-06/FS05-06-001.pdf

Arkoudas, K., Bringsjord, S., Bello, P. (2005). Toward ethical robots via mechanized deontic logic. https://www.aaai.org/Papers/Symposia/Fall/2005/FS-05-06/FS05-06-003.pdf

Armstrong, S. (2015). Motivated Value Selection for Artificial Agents. https://www.aaai.org/ocs/index.php/WS/AAAIW15/paper/viewFile/10183/10126

Bello, P., & Bringsjord, S. (2013). On How to Build a Moral Machine. https://doi.org/10.1007/s11245-012-9129-8

Bendel, O. (2013). Considerations about the relationship between animal and machine ethics. http://doi.org/10.1007/s00146-013-0526-3

Goodall, A. N. J. (2014). Machine Ethics and Automated Vehicles. http://people.virginia.edu/~njg2q/machineethics.pdf

Grau, C. (n.d.). There is no “I” in “Robot”: Robotic Utilitarians and Utilitarian Robots. https://www.aaai.org/Papers/Symposia/Fall/2005/FS-05-06/FS05-06-007.pdf

Lokhorst, G. J. C. (2011). Computational meta-ethics towards the meta-ethical robot. https://doi.org/10.1007/s11023-011-9229-z ([erratum](https://www.researchgate.net/publication/263369942_Erratum_to_Computational_Meta-Ethics_Towards_the_Meta-Ethical_Robot))

Muntean, I. & Howard, D. (2016). A minimalist model of the artificial autonomous moral agent (AAMA). https://www.aaai.org/ocs/index.php/SSS/SSS16/paper/download/12760/11954

Oesterheld, C. (2015). Formalizing preference utilitarianism in physical world models. https://doi.org/10.1007/s11229-015-0883-1

Pereira, L. M., & Saptawijaya, A. (2009). Modelling morality with prospective logic. https://doi.org/10.1504/IJRIS.2009.028020

Powers, T. M. (n.d.). Deontological Machine Ethics. https://www.aaai.org/Papers/Symposia/Fall/2005/FS-05-06/FS05-06-012.pdf

Powers, T. M. (n.d.). Prospects for a Smithian Machine. http://www.iacap.org/proceedings_IACAP13/paper_52.pdf

Shulman, C., Tarleton, N., & Jonsson, H. 2009. Which Consequentialism? Machine Ethics
and Moral Divergence. https://intelligence.org/files/WhichConsequentialism.pdf

Tarleton, N. (2010). Coherent Extrapolated Volition: A Meta-Level Approach to Machine Ethics. https://intelligence.org/files/CEV-MachineEthics.pdf

White, J. (n.d.). Autonomous Reboot: the challenges of artificial moral agency and the ends of Machine Ethics.  

White, J. (n.d.). A General Theory of Moral Agency Grounding Computational Implementations: The ACTWith Model. https://www.academia.edu/7000519/Autonomous_Reboot_the_challenges_of_artificial_moral_agency_and_the_ends_of_Machine_Ethics

Wiltshire, T. J. (2015). A Prospective Framework for the Design of Ideal Artificial Moral Agents: Insights from the Science of Heroism in Humans. https://doi.org/10.1007/s11023-015-9361-2

**Books**

Wallach, W. & Allen, C. Moral Machines: Teaching Robots Right from Wrong. https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975

**Encyclopedia Articles**

McNamara, P. ""Deontic Logic"", The Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/archives/win2014/entries/logic-deontic

Portoraro, F. ""Automated Reasoning"", The Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/archives/win2014/entries/reasoning-automated/


---
I'm sure I've missed some so feel free to suggest additions."
Andra Keay — What are Ethical Design Frameworks for Robotics and AI? what do you guys think?
Ethical Considerations in Artificial Intelligence Courses
"Notes from the NYU AI Ethics conferenceThis weekend I attended the [Ethics of Artificial Intelligence conference](https://wp.nyu.edu/consciousness/ethics-of-artificial-intelligence/) at NYU. There were a ton of high-profile and interesting people there from philosophy (David Chalmers, Peter Railton, Nick Bostrom, Thomas Nagel, Paul Boghossian, Frances Kamm, Wendell Wallach) and science (Yann LeCun, Stuart Russell, Stephen Wolfram, Max Tegmark, Francesca Rossi) as well as Eliezer Yudkowsky. 

There were two fairly long days of talks and panels. David Chalmers (famous for his philosophy of mind and consciousness) did not officially speak but acted as chair for the event. He outlined the philosophy of the conference, which was to discuss both short and long term issues in AI ethics without worrying about either detracting from the other. He was, as usual, extremely awesome.

Here is a summary of the event with the most interesting points made by the speakers.

**Day One**

The first block of talks on Friday was an overview of general issues related to artificial intelligence. Nick Bostrom, author of *Superintelligence* and head of the Future of Humanity Institute, started with something of a barrage of all the general ideas and things he's come up with. He floated the idea that perhaps we shouldn't program AI systems to be maximally moral, for we don't know what the true morality looks like, and what if it turns out that such a directive would lead to humans being punished, or something else that was pathological or downright weird? He also described three principles for how we should treat AIs: substrate nondiscrimination (moral status does not depend on the kind of hardware/wetware you run on), ontogeny nondiscrimination (moral status does not depend on how you were created), and subjective time (moral value exists relative to subjectively experienced time rather than objective time, so if a mind ran at a fast clock speed its life would be more important, all other things being equal).

He pointed out that AI moral status could arise before they reach there is any such thing as human level AI - just like animals have moral status despite being much simpler than humans. He mentioned the possibility of a Malthusian catastrophe from unlimited digital reproduction as well as the possibility for vote manipulation through agent duplication, and how we'll need to prevent these two things.

He voiced support for meta level decisionmaking - a ['moral parliament'](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html) where we imagine moral theories sending 'delegates' to compromise over contentious issues. Such a system could also accommodate other values and interests besides moral theories.

He answered the question of ""what is humanity most likely to fail at?"" with a qualified choice of 'mind crime' committed against advanced AIs. Humans already have difficulty with empathy towards animals when they exist on farms or in the wild, but AI would not necessarily have the basic biological features which incline us to be empathetic at all towards animals. Some robots attract empathetic attention from humans, but many invisible automated processes are much harder for people to feel empathetic towards.

Virginia Dignum was next; she is at the Delft University of Technology and spoke about mechanisms for automated processes to make decisions. She specified four methods of decisionmaking based on whether decisions are taken deliberately or imposed upon a system and whether the decisions are made internally or externally. The two former features lead to algorithmic decisionmaking in machines; the latter two lead to imposed decisions predetermined by regulatory institutions. Deliberated external decisionmaking means there is a 'human in the loop' and internal imposed decisionmaking is essentially randomness.

Yann LeCun concluded this section with a pretty fantastic overview of deep learning methods and the limitations which stand in the way of progress in machine intelligence. He pointed out that reinforcement learning is a rare and narrow slice of the field today and that the greatest obstacles for machines include common sense judgements and abstraction. The biggest current problem for AI is unsupervised learning, which is having machines that can learn to classify things on their own without being given clearly labelled data from humans. He showcased some of the (very cool) features of adversarial learning which are being used to tackle this.

He expressed support for the orthogonality thesis, namely the idea that intelligence and morality are 'orthogonal' - just because an agent is very smart doesn't mean that it's necessarily moral. He believes we should build a few basic drives into AIs: do not hurt humans, interact with humans, crave positive feedback from trusted human trainers. He also described a couple of reasons for why he is not concerned about uncontrolled advanced artificial intelligence. One was that he is confident that objective functions can be specified in such a way as to make machines indifferent to being switched off, and the other is that a narrow-AI focused on eliminating an unfriendly general-AI would 'win' due to its specialization.

In Q&A, Stuart Russell objected to LeCun's confidence in machines being indifferent to being shut off based on the fact that self-preservation as a goal implicitly falls out of whatever other goals a machine has. Paul Boghossian objected to the 'behaviorist' nature of the speakers' points of view, saying that they were exempting consciousness from its proper role in these discussions. One person asked whether we should let AIs take charge of everything and supersede humanity - Bostrom pointed out that the space of possible futures is ""an enormous Petri dish"" which we don't understand; an AI future could materialize as a planet sized supercomputer with no moral status, and we will need to learn how to engineer friendly advanced AI systems no matter what the plan is. 

The rest of the Friday talks were devoted to near-future issues with specific AI systems. Peter Asaro started with an overview of his organization, the 'Campaign to Stop Killer Robots'. He stated that targeting and killing should remain human-controlled actions. While he acknowledged that automated weaponry could result in fewer casualties on the battlefield, he believed that it was too narrow a view of the consequences. He said that it's not straightforward to translate complicated battlefield morality questions for machines to understand, and is worried about unintended initiation and escalation of conflicts through automated systems, arms races, and threats to humanitarian law. He also believes that people should only be killed with 'dignity' and that doing it with a robot robs people of this. Therefore, he called for a clear and strong norm against automated weapons.

Kate Devlin of the University of London gave a brief overview of the ethics of artificial sexuality. Looking at the history of sexualized robots featured in fictional media, she noted that almost all of them are female. Today there is a ""Campaign Against Sex Robots"" which is based on the idea that sexual robots would lead to the objectification of women. Devlin does not agree as she thinks it is too early to ban the technology and that we should explore it before thinking about banning it, especially since it does not really harm anyone. Instead she wants us to think about how to develop it correctly. There are many potential uses for these types of robots ranging all the way to the therapeutic; many of the rudimentary ones being sold today are bought by people who are incapable of forming ordinary relationships for various reasons. VR is being used in arousal tests to gauge the efficacy of treatments against pedophilia.

She noted that gender issues have arisen in technology already; the history of gendered technology includes pacemakers originally designed only for men and phones too large for women's pockets. We should get into AI now to make sure that it is not designed in problematic ways.

She mentioned privacy concerns, as the manufactures of the female stimulator WeVibe have already been sued over concerns that they were not properly informing customers of their collection of data from the devices. She wondered if we will ever get to a stage where a robot might have some knowledge of its role and refuse to give consent to its use, and if transmission/duplication of data and code between machines could serve as some form of digital sexual reproduction.

Vasant Dhar of NYU spoke next about data and privacy in the era of autonomous vehicles. He said that our legal and financial liability institutions are based on outdated notions of data and that they fail to address liability and crime. However, the tools we have now even in ordinary cars for recording data can be used to improve insurance and judicial systems. He proposed black boxes for cars that would contain all relevant data to determine fault in the event of accidents, and said that customers should have the choice to share their driving data with insurance companies to get lower premiums. 

Dhar reiterated the importance of improving vehicle safety through autonomous driving; each percentage point reduction in vehicle accidents equates to 400 deaths and 40,000 injuries avoided every year.

Adam Kolber followed up with a discussion of whether ""the code is the law"", based on the case study of [The DAO](https://en.wikipedia.org/wiki/The_DAO_%28organization%29) which was an automated capital fund which was subjected to a $50 million loss through exploitation. The answer apparently is that the code should not be the law, even though many people seemed to accept that it was.

Steve Wolfram of WolframAlpha and Mathematica fame discussed the issues of computer languages and goal specification. He said that his life work has essentially been about trying to find ways for humans to specify their goals to machines, and that this can work for ethics as well as for math. He doesn't think that any single moral theory is likely to work for guiding artificial intelligence, apparently because of Godel's theorem and the incompleteness of computational languages.

Francesca Rossi of IBM argued that for AIs and humans to interact very productively we will have to embed them in environments, so that rather than picking up a tool like a laptop or a phone, we are interacting with artificial systems all around is in our rooms and spaces. Humans will be recognized by their environments and our needs and wants will be inferred or asked about. AI embedded in environments can have memories about humans to better serve their interests. Most of all, we will need to establish trust between humans and AIs.

Peter Railton, philosopher at the University of Michigan, attacked the subjects of orthogonality and value learning. He said that we can't simply tell AIs to do what we want because our wants and values require critical assessment. He said that the orthogonality thesis might be right, but as we increasingly interact with systems and allow them to participate in our own lives and decisionmaking, the question of what it would take for them to be intelligent might involve certain features relevant to morality.

He stated that AIs should be thought of as social creatures; as a simple model, self regulation in a Hobbesian social contract leads to constraints and respect derived from self preservation. A society of intelligent cooperators can resist aggression and malice, and being moral is more efficient for a community than being cunning. From these principles we have a recipe for building proto-moral agents.

He discussed the 'moral point of view' required for many strong ethical theories such as Kantian ethics and consequentialism: it requires agents to have a hierarchical, non-perspectival, modal/planning-oriented, and consistent view of the world which assigns intrinsic moral weight to things. He described how all these features are also part of the process of becoming generally intelligent in the first place, implying that general social intelligence ensures the necessary information required for moral decisionmaking. In the path towards functional moral agents, we will have to build agents which can represent the goals of others and have them learn how to act in beneficial ways. So if we can build AIs that we can trust, then we are on a good path towards building artificial moral agents.

In the Q&A, Eliezer Yudkowsky objected that in the long run the 'instrumental strategy' is not quite what you want because maximizing people's desires as they are explicitly revealed can lead to bad outcomes, and you have to have a view like coherent extrapolated volition which asks what people would really want. Russell objected that when an agent becomes sufficiently powerful, it has no need to cooperate anymore.

Regina Rini of the NYU Center for Bioethics stated that the approaches to ethics so far described relied too much on the Western post-enlightenment view of ethics, which is a historical aberration, and excluded African, Chinese and other approaches to ethics. Railton stated that his scheme was grounded in basic empathy and not mediated by any higher order moral theory; Wolfram and Rossi said that no one ethical approach will work and AI will have to represent diverse values.

**Day Two**

Saturday was devoted to long term discussion of the future of advanced artificial intelligence. Stuart Russell, professor at UC Berkeley and head of the new Center for Human Compatible Artificial Intelligence, started with a basic overview of the control problem. He described the points made in Steve Omohundro's paper on convergent instrumental drives. He also had some pretty harsh words for the researchers in the AI community which have denied and rejected notions of the control problem without seriously engaging with the relevant literature.

He had three simple ideas which he proposed to constitute the definition of 'provably beneficial' AI: maximizing values for humans is the system's only goal; the robot is initially uncertain about these goals, and the best source of information is human behavior. He referred to inverse reinforcement learning as a technique for machines to learn human preferences, and said that uncertainty provides an incentive for machines to learn, ask questions, and explore cautiously. 

His answer to the off-switch problem is to make robots unsure of their objectives, so that they assume that the human will switch the robot off if and only if it has a good reason to, and will therefore be complicit with the action. He said that the wireheading problem can be avoided if you construct the reward signal as information about the reward function rather than as a reward itself; this way, any hijacking of the reward signal makes it useless.

He said that there is a strong economic incentive for value alignment, but humans are irrational, nasty, inconsistent, and weak-willed.

The next speaker was Eliezer Yudkowsky of the Machine Intelligence Research Institute. Chalmers pointed out his role there as well as his side venture in Harry Potter fanfiction.

Yudkowsky started [his talk](https://intelligence.org/nyu-talk/) by pointing out how the Terminator pictures in every media article about the control problem are inappropriate. The real analogy to be used is [Mickey Mouse as the Sorcerer's Apprentice in *Fantasia*.](https://www.youtube.com/watch?v=Ait_Fs6UQhQ) 

He said that the first difficulty of AI alignnment is that the utility functions we imagine are too simple, and the second difficulty is that maximizing the probability of achieving a given goal leads to pathological outcomes. He and MIRI are concerned with the nature of the goal of 'maximizing' and how to define goals in a way that avoids the problems of perverse instantiation. 

He said that the fears of AI being developed by some terrorist or rogue group were silly, as ""ISIS is not developing convolutional neural nets."" Instead the most powerful AI is likely to be developed by large groups in government, academia and industry.

He claimed that the four central propositions which support the idea that AI is a very big problem are: the orthogonality thesis, instrumental convergence, capability gain (the speed at which advanced AI can make itself better), and alignment difficulty. He said the first two are logical matters of computer science that people always learn to accept when they reflect upon them, while the latter two are more controversial.

The next talk was from Max Tegmark and Meia Chita-Tegmark. Max is a world-renowned physicist who helps run the Future of Life Institute, and Meia is a psychologist. They explained how physics and psychology provide useful tools for understanding artificial intelligence; physics tells us about computation and the constraints of the universe, and psychology tells us about the nature of well being, ways to debug the mind when reasoning about AI and methods to design psychomorphic AIs. Meia was the only speaker at the conference to discuss unemployment in any detail; she pointed out that retirement has only mixed effects on well being and that happiness comes from financial satisfaction and feelings of respect. She said that studying homemakers, part time workers and early retirees can tell us more about how an automated economy would affect people's well-being.

Max checked off [a list of common myths](http://futureoflife.org/background/aimyths/) regarding advanced AI. Meia said that we should look at the cognitive biases which have led to these misconceptions (such as availability bias leading to people worrying about robots rather than invisible artificial intelligence) and figure out how to avoid similar bugs from inhibiting our thinking in the future.

By the way, Max Tegmark is very cool, he has a sort of old-rocker-dude vibe, and he and Meia are super cute together.

Wendell Wallach of Yale spoke next. He is the man who quite literally wrote [the book](https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975) on AI ethics. He distinguished top-down approaches of formally specifying AI behaviors from bottom-up approaches of value learning. He said that neither will be sufficient on its own and that both have important roles to play. He is worried that AI engineers will make simplistic assumptions about AI, such as the idea that every decision should be utilitarian or the idea that 'ethics' and 'morality' are icky concepts that can be ignored.

Steve Petersen, a philosopher at the University of Niagara, gave the next talk, based on the draft of a forthcoming paper of his. He aims to push back against the orthogonality thesis and modulate the level of the risk assessment provided by Bostrom. His argument is that designing AI to follow any complex goal will necessarily require it to be able to learn the values of its ""teleological ancestors"" (the original human designers or the previous iterations of AI before it self-improved or self-modified) and arrive at a state of coherence between goals. As agents replicate, self-modify and merge in the digital world, there can be no fact of the matter about which agents are the same or different; instead there will be an 'agential soup' unified by a common teleological thread originating with the designers. Coherence reasoning leads to impartial reasoning with the goals of other agents.

There were several responses to him in Q&A. Yudkowsky's objection was that reaching coherence requires a meta-preference framework with particular assumptions about the universe and ontology; therefore, for any goal, there are many preference frameworks which could fulfill it, many of which would be perverse. Russell said that just coherence is not enough because you need the systems to give special weight to humans. Max Tegmark said that the problem was the vagueness of humanity's final goals. Chalmers pointed out that the orthogonality thesis still allows for all kinds of correlations between between intelligence and morality, as long as they are not necessary by design. Petersen said that he is arguing for 'attractor basins' in the possibility space of AI minds. Interestingly, he was motivated to start his research by the [Dylan Matthews Vox article](http://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai) on effective altruism where Dylan thought that effective altruists shouldn't be concerned by artificial intelligence. Petersen doesn't think that AI is unimportant and thinks that Bostrom and Yudkowsky's work is valuable, but he wanted to get a more critical assessment of the level of risk when he learned that alternative altruistic projects were at stake.

Matthew Liao of the NYU Center for Bioethics gave an argument for moral status on the basis of capabilities - that an entity is morally valuable to the extent that it has the physical/genetic basis for achieving features of moral relevance. I did not get a chance to ask him if this would imply that a 'seed AI' could be the most morally valuable entity in the world. He did argue against the ideas that level of intelligence or degree of moral agency determine moral status, as we don't normally think that smarter or more benevolent humans are more morally valuable than others. 

Liao argued that moral theories are too specific and too high level to be generally implemented in AIs. Instead, AI will need a universal moral grammar in which to specify morality. The holy grail is to develop machines that understand why things are right or wrong.

Eric Schwitzgebel and Mara Garza of UC Riverside argued for basic principles of AI rights. They introduced a very weak ""no-relevant-difference"" argument: the idea that there are possible AIs which have the same morally relevant features that humans do and therefore there are possible AIs with equal value to humans. They questioned if cheerfully suicidal or hardworking AI is acceptable, and stated a 'self respect principle': that human grade AI should be designed with an appropriate appreciation of its own value.

John Basl and Ronald Sandler of Northeastern University argued for AI research committees to approve or deny research in cases where AI subjects might be harmed. They said it would not be very different from cases like animal testing where we have similar review boards, and sketched out details of how the proposal would work.

Daniel Kahneman, one of the most famous behavioral economists in the world, made something of a surprise appearance in the final panel. He said that we should take intuitions about case studies like the trolley problem seriously, as that is how the public will think about these events, for better or for worse. He said that no matter how AI cars kill people, it will be perceived with horror whenever the first incident happens, and we should prepare for that. Intuitions depend on irrelevant factors and will especially depend on whether AIs are designed to resemble us or not.

Gary Marcus, professor of psychology at NYU, of gave a much needed presentation about the nature of intelligence. The previous talks in this discussion had mostly assumed that intelligence was one-dimensional and simple and that there was some fixed idea of 'human-level' AI which we could eventually reach. Of course this is a ridiculous oversimplification; intelligence is multidimensional and it is more about implementing a combination of various cognitive tools, some of which are already stronger in AIs than in humans. AIs can be better or worse than us in various domains, so we really have no idea where AIs will be in this multidimensional space. AIs could in fact be better than us at moral reasoning. He also emphasized the gap is between machine learning today and what human reasoning can do.

Susan Schneider of Marquette University, a philosopher who has written quite a bit about AI and superintelligence, went over various issues. She argued that mind uploads might constitute death of the individual as long as we don't prove certain ideas about consciousness and personal identity, and also claimed that designing an intelligent and morally valuable robot to serve the interests of its creators would constitute slavery.

Jaan Tallinn, founder of Skype, also gave a quick talk. He has been a strong financial backer for MIRI and other efforts in this space, and simply expressed his belief in the importance of the issue and his happiness at the success of the conference and the number of students who were interested in pursuing the topic.

There was some final banter about the nature of consciousness which David Chalmers sat through very passively. Yudkowsky expressed optimism that one day we will have an explanation of consciousness which clears up our confusion on the matter. Nagel said that we will need to think more about the dynamics of multi-agent systems and moral epistemology. After that the event ended.

The conference videos are available [here.](http://livestream.com/nyu-tv/ethicsofAI) In my opinion, the best talks were given by LeCun, Railton, Russell, Yudkowsky, the Tegmarks, Petersen, and Marcus. The event overall was great and being in Manhattan made it even better. There was quite a bit of valuable informal meeting and discussion between many of the speakers and attendees. There was no 'sneering' or disdain about Yudkowsky or Bostrom as far as I could tell. It seemed like a generally open minded yet well educated crowd.

If you regret missing it, then you might like to head to the [Envision Conference](http://envision-conference.com/) this December. 

"
"We are trending!I'm quite stunned at this -- two days ago I was actually planning to call quits on the subreddit and label it dead, no joke -- but we received a lot of attention yesterday after I [posted](https://www.reddit.com/r/Futurology/comments/55an2u/the_map_of_ai_ethical_issues/) an infographic to r/futurology. Everyone likes the intersection of science fiction with reality, and with self driving cars we're already seeing the need for ethical values to be implicitly encoded into machines.

I wish we had a good ""introductory article"" for newcomers who just want to start learning about the philosophy of AI and robotics but none could encapsulate everything; we are here to discuss a very wide set of issues and topics that humanity will have to address in the future.

Trending thread is here: https://www.reddit.com/r/trendingsubreddits/comments/55ha4r/trending_subreddits_for_20161002_rmedia_criticism/"
"Notes from the NYU AI Ethics conferenceThis weekend I attended the [Ethics of Artificial Intelligence conference](https://wp.nyu.edu/consciousness/ethics-of-artificial-intelligence/) at NYU. There were a ton of high-profile and interesting people there from philosophy (David Chalmers, Peter Railton, Nick Bostrom, Thomas Nagel, Paul Boghossian, Frances Kamm, Wendell Wallach) and science (Yann LeCun, Stuart Russell, Stephen Wolfram, Max Tegmark, Francesca Rossi) as well as Eliezer Yudkowsky. 

There were two fairly long days of talks and panels. David Chalmers (famous for his philosophy of mind and consciousness) did not officially speak but acted as chair for the event. He outlined the philosophy of the conference, which was to discuss both short and long term issues in AI ethics without worrying about either detracting from the other. He was, as usual, extremely awesome.

Here is a summary of the event with the most interesting points made by the speakers.

**Day One**

The first block of talks on Friday was an overview of general issues related to artificial intelligence. Nick Bostrom, author of *Superintelligence* and head of the Future of Humanity Institute, started with something of a barrage of all the general ideas and things he's come up with. He floated the idea that perhaps we shouldn't program AI systems to be maximally moral, for we don't know what the true morality looks like, and what if it turns out that such a directive would lead to humans being punished, or something else that was pathological or downright weird? He also described three principles for how we should treat AIs: substrate nondiscrimination (moral status does not depend on the kind of hardware/wetware you run on), ontogeny nondiscrimination (moral status does not depend on how you were created), and subjective time (moral value exists relative to subjectively experienced time rather than objective time, so if a mind ran at a fast clock speed its life would be more important, all other things being equal).

He pointed out that AI moral status could arise before they reach there is any such thing as human level AI - just like animals have moral status despite being much simpler than humans. He mentioned the possibility of a Malthusian catastrophe from unlimited digital reproduction as well as the possibility for vote manipulation through agent duplication, and how we'll need to prevent these two things.

He voiced support for meta level decisionmaking - a ['moral parliament'](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html) where we imagine moral theories sending 'delegates' to compromise over contentious issues. Such a system could also accommodate other values and interests besides moral theories.

He answered the question of ""what is humanity most likely to fail at?"" with a qualified choice of 'mind crime' committed against advanced AIs. Humans already have difficulty with empathy towards animals when they exist on farms or in the wild, but AI would not necessarily have the basic biological features which incline us to be empathetic at all towards animals. Some robots attract empathetic attention from humans, but many invisible automated processes are much harder for people to feel empathetic towards.

Virginia Dignum was next; she is at the Delft University of Technology and spoke about mechanisms for automated processes to make decisions. She specified four methods of decisionmaking based on whether decisions are taken deliberately or imposed upon a system and whether the decisions are made internally or externally. The two former features lead to algorithmic decisionmaking in machines; the latter two lead to imposed decisions predetermined by regulatory institutions. Deliberated external decisionmaking means there is a 'human in the loop' and internal imposed decisionmaking is essentially randomness.

Yann LeCun concluded this section with a pretty fantastic overview of deep learning methods and the limitations which stand in the way of progress in machine intelligence. He pointed out that reinforcement learning is a rare and narrow slice of the field today and that the greatest obstacles for machines include common sense judgements and abstraction. The biggest current problem for AI is unsupervised learning, which is having machines that can learn to classify things on their own without being given clearly labelled data from humans. He showcased some of the (very cool) features of adversarial learning which are being used to tackle this.

He expressed support for the orthogonality thesis, namely the idea that intelligence and morality are 'orthogonal' - just because an agent is very smart doesn't mean that it's necessarily moral. He believes we should build a few basic drives into AIs: do not hurt humans, interact with humans, crave positive feedback from trusted human trainers. He also described a couple of reasons for why he is not concerned about uncontrolled advanced artificial intelligence. One was that he is confident that objective functions can be specified in such a way as to make machines indifferent to being switched off, and the other is that a narrow-AI focused on eliminating an unfriendly general-AI would 'win' due to its specialization.

In Q&A, Stuart Russell objected to LeCun's confidence in machines being indifferent to being shut off based on the fact that self-preservation as a goal implicitly falls out of whatever other goals a machine has. Paul Boghossian objected to the 'behaviorist' nature of the speakers' points of view, saying that they were exempting consciousness from its proper role in these discussions. One person asked whether we should let AIs take charge of everything and supersede humanity - Bostrom pointed out that the space of possible futures is ""an enormous Petri dish"" which we don't understand; an AI future could materialize as a planet sized supercomputer with no moral status, and we will need to learn how to engineer friendly advanced AI systems no matter what the plan is. 

The rest of the Friday talks were devoted to near-future issues with specific AI systems. Peter Asaro started with an overview of his organization, the 'Campaign to Stop Killer Robots'. He stated that targeting and killing should remain human-controlled actions. While he acknowledged that automated weaponry could result in fewer casualties on the battlefield, he believed that it was too narrow a view of the consequences. He said that it's not straightforward to translate complicated battlefield morality questions for machines to understand, and is worried about unintended initiation and escalation of conflicts through automated systems, arms races, and threats to humanitarian law. He also believes that people should only be killed with 'dignity' and that doing it with a robot robs people of this. Therefore, he called for a clear and strong norm against automated weapons.

Kate Devlin of the University of London gave a brief overview of the ethics of artificial sexuality. Looking at the history of sexualized robots featured in fictional media, she noted that almost all of them are female. Today there is a ""Campaign Against Sex Robots"" which is based on the idea that sexual robots would lead to the objectification of women. Devlin does not agree as she thinks it is too early to ban the technology and that we should explore it before thinking about banning it, especially since it does not really harm anyone. Instead she wants us to think about how to develop it correctly. There are many potential uses for these types of robots ranging all the way to the therapeutic; many of the rudimentary ones being sold today are bought by people who are incapable of forming ordinary relationships for various reasons. VR is being used in arousal tests to gauge the efficacy of treatments against pedophilia.

She noted that gender issues have arisen in technology already; the history of gendered technology includes pacemakers originally designed only for men and phones too large for women's pockets. We should get into AI now to make sure that it is not designed in problematic ways.

She mentioned privacy concerns, as the manufactures of the female stimulator WeVibe have already been sued over concerns that they were not properly informing customers of their collection of data from the devices. She wondered if we will ever get to a stage where a robot might have some knowledge of its role and refuse to give consent to its use, and if transmission/duplication of data and code between machines could serve as some form of digital sexual reproduction.

Vasant Dhar of NYU spoke next about data and privacy in the era of autonomous vehicles. He said that our legal and financial liability institutions are based on outdated notions of data and that they fail to address liability and crime. However, the tools we have now even in ordinary cars for recording data can be used to improve insurance and judicial systems. He proposed black boxes for cars that would contain all relevant data to determine fault in the event of accidents, and said that customers should have the choice to share their driving data with insurance companies to get lower premiums. 

Dhar reiterated the importance of improving vehicle safety through autonomous driving; each percentage point reduction in vehicle accidents equates to 400 deaths and 40,000 injuries avoided every year.

Adam Kolber followed up with a discussion of whether ""the code is the law"", based on the case study of [The DAO](https://en.wikipedia.org/wiki/The_DAO_%28organization%29) which was an automated capital fund which was subjected to a $50 million loss through exploitation. The answer apparently is that the code should not be the law, even though many people seemed to accept that it was.

Steve Wolfram of WolframAlpha and Mathematica fame discussed the issues of computer languages and goal specification. He said that his life work has essentially been about trying to find ways for humans to specify their goals to machines, and that this can work for ethics as well as for math. He doesn't think that any single moral theory is likely to work for guiding artificial intelligence, apparently because of Godel's theorem and the incompleteness of computational languages.

Francesca Rossi of IBM argued that for AIs and humans to interact very productively we will have to embed them in environments, so that rather than picking up a tool like a laptop or a phone, we are interacting with artificial systems all around is in our rooms and spaces. Humans will be recognized by their environments and our needs and wants will be inferred or asked about. AI embedded in environments can have memories about humans to better serve their interests. Most of all, we will need to establish trust between humans and AIs.

Peter Railton, philosopher at the University of Michigan, attacked the subjects of orthogonality and value learning. He said that we can't simply tell AIs to do what we want because our wants and values require critical assessment. He said that the orthogonality thesis might be right, but as we increasingly interact with systems and allow them to participate in our own lives and decisionmaking, the question of what it would take for them to be intelligent might involve certain features relevant to morality.

He stated that AIs should be thought of as social creatures; as a simple model, self regulation in a Hobbesian social contract leads to constraints and respect derived from self preservation. A society of intelligent cooperators can resist aggression and malice, and being moral is more efficient for a community than being cunning. From these principles we have a recipe for building proto-moral agents.

He discussed the 'moral point of view' required for many strong ethical theories such as Kantian ethics and consequentialism: it requires agents to have a hierarchical, non-perspectival, modal/planning-oriented, and consistent view of the world which assigns intrinsic moral weight to things. He described how all these features are also part of the process of becoming generally intelligent in the first place, implying that general social intelligence ensures the necessary information required for moral decisionmaking. In the path towards functional moral agents, we will have to build agents which can represent the goals of others and have them learn how to act in beneficial ways. So if we can build AIs that we can trust, then we are on a good path towards building artificial moral agents.

In the Q&A, Eliezer Yudkowsky objected that in the long run the 'instrumental strategy' is not quite what you want because maximizing people's desires as they are explicitly revealed can lead to bad outcomes, and you have to have a view like coherent extrapolated volition which asks what people would really want. Russell objected that when an agent becomes sufficiently powerful, it has no need to cooperate anymore.

Regina Rini of the NYU Center for Bioethics stated that the approaches to ethics so far described relied too much on the Western post-enlightenment view of ethics, which is a historical aberration, and excluded African, Chinese and other approaches to ethics. Railton stated that his scheme was grounded in basic empathy and not mediated by any higher order moral theory; Wolfram and Rossi said that no one ethical approach will work and AI will have to represent diverse values.

**Day Two**

Saturday was devoted to long term discussion of the future of advanced artificial intelligence. Stuart Russell, professor at UC Berkeley and head of the new Center for Human Compatible Artificial Intelligence, started with a basic overview of the control problem. He described the points made in Steve Omohundro's paper on convergent instrumental drives. He also had some pretty harsh words for the researchers in the AI community which have denied and rejected notions of the control problem without seriously engaging with the relevant literature.

He had three simple ideas which he proposed to constitute the definition of 'provably beneficial' AI: maximizing values for humans is the system's only goal; the robot is initially uncertain about these goals, and the best source of information is human behavior. He referred to inverse reinforcement learning as a technique for machines to learn human preferences, and said that uncertainty provides an incentive for machines to learn, ask questions, and explore cautiously. 

His answer to the off-switch problem is to make robots unsure of their objectives, so that they assume that the human will switch the robot off if and only if it has a good reason to, and will therefore be complicit with the action. He said that the wireheading problem can be avoided if you construct the reward signal as information about the reward function rather than as a reward itself; this way, any hijacking of the reward signal makes it useless.

He said that there is a strong economic incentive for value alignment, but humans are irrational, nasty, inconsistent, and weak-willed.

The next speaker was Eliezer Yudkowsky of the Machine Intelligence Research Institute. Chalmers pointed out his role there as well as his side venture in Harry Potter fanfiction.

Yudkowsky started [his talk](https://intelligence.org/nyu-talk/) by pointing out how the Terminator pictures in every media article about the control problem are inappropriate. The real analogy to be used is [Mickey Mouse as the Sorcerer's Apprentice in *Fantasia*.](https://www.youtube.com/watch?v=Ait_Fs6UQhQ) 

He said that the first difficulty of AI alignnment is that the utility functions we imagine are too simple, and the second difficulty is that maximizing the probability of achieving a given goal leads to pathological outcomes. He and MIRI are concerned with the nature of the goal of 'maximizing' and how to define goals in a way that avoids the problems of perverse instantiation. 

He said that the fears of AI being developed by some terrorist or rogue group were silly, as ""ISIS is not developing convolutional neural nets."" Instead the most powerful AI is likely to be developed by large groups in government, academia and industry.

He claimed that the four central propositions which support the idea that AI is a very big problem are: the orthogonality thesis, instrumental convergence, capability gain (the speed at which advanced AI can make itself better), and alignment difficulty. He said the first two are logical matters of computer science that people always learn to accept when they reflect upon them, while the latter two are more controversial.

The next talk was from Max Tegmark and Meia Chita-Tegmark. Max is a world-renowned physicist who helps run the Future of Life Institute, and Meia is a psychologist. They explained how physics and psychology provide useful tools for understanding artificial intelligence; physics tells us about computation and the constraints of the universe, and psychology tells us about the nature of well being, ways to debug the mind when reasoning about AI and methods to design psychomorphic AIs. Meia was the only speaker at the conference to discuss unemployment in any detail; she pointed out that retirement has only mixed effects on well being and that happiness comes from financial satisfaction and feelings of respect. She said that studying homemakers, part time workers and early retirees can tell us more about how an automated economy would affect people's well-being.

Max checked off [a list of common myths](http://futureoflife.org/background/aimyths/) regarding advanced AI. Meia said that we should look at the cognitive biases which have led to these misconceptions (such as availability bias leading to people worrying about robots rather than invisible artificial intelligence) and figure out how to avoid similar bugs from inhibiting our thinking in the future.

By the way, Max Tegmark is very cool, he has a sort of old-rocker-dude vibe, and he and Meia are super cute together.

Wendell Wallach of Yale spoke next. He is the man who quite literally wrote [the book](https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975) on AI ethics. He distinguished top-down approaches of formally specifying AI behaviors from bottom-up approaches of value learning. He said that neither will be sufficient on its own and that both have important roles to play. He is worried that AI engineers will make simplistic assumptions about AI, such as the idea that every decision should be utilitarian or the idea that 'ethics' and 'morality' are icky concepts that can be ignored.

Steve Petersen, a philosopher at the University of Niagara, gave the next talk, based on the draft of a forthcoming paper of his. He aims to push back against the orthogonality thesis and modulate the level of the risk assessment provided by Bostrom. His argument is that designing AI to follow any complex goal will necessarily require it to be able to learn the values of its ""teleological ancestors"" (the original human designers or the previous iterations of AI before it self-improved or self-modified) and arrive at a state of coherence between goals. As agents replicate, self-modify and merge in the digital world, there can be no fact of the matter about which agents are the same or different; instead there will be an 'agential soup' unified by a common teleological thread originating with the designers. Coherence reasoning leads to impartial reasoning with the goals of other agents.

There were several responses to him in Q&A. Yudkowsky's objection was that reaching coherence requires a meta-preference framework with particular assumptions about the universe and ontology; therefore, for any goal, there are many preference frameworks which could fulfill it, many of which would be perverse. Russell said that just coherence is not enough because you need the systems to give special weight to humans. Max Tegmark said that the problem was the vagueness of humanity's final goals. Chalmers pointed out that the orthogonality thesis still allows for all kinds of correlations between between intelligence and morality, as long as they are not necessary by design. Petersen said that he is arguing for 'attractor basins' in the possibility space of AI minds. Interestingly, he was motivated to start his research by the [Dylan Matthews Vox article](http://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai) on effective altruism where Dylan thought that effective altruists shouldn't be concerned by artificial intelligence. Petersen doesn't think that AI is unimportant and thinks that Bostrom and Yudkowsky's work is valuable, but he wanted to get a more critical assessment of the level of risk when he learned that alternative altruistic projects were at stake.

Matthew Liao of the NYU Center for Bioethics gave an argument for moral status on the basis of capabilities - that an entity is morally valuable to the extent that it has the physical/genetic basis for achieving features of moral relevance. I did not get a chance to ask him if this would imply that a 'seed AI' could be the most morally valuable entity in the world. He did argue against the ideas that level of intelligence or degree of moral agency determine moral status, as we don't normally think that smarter or more benevolent humans are more morally valuable than others. 

Liao argued that moral theories are too specific and too high level to be generally implemented in AIs. Instead, AI will need a universal moral grammar in which to specify morality. The holy grail is to develop machines that understand why things are right or wrong.

Eric Schwitzgebel and Mara Garza of UC Riverside argued for basic principles of AI rights. They introduced a very weak ""no-relevant-difference"" argument: the idea that there are possible AIs which have the same morally relevant features that humans do and therefore there are possible AIs with equal value to humans. They questioned if cheerfully suicidal or hardworking AI is acceptable, and stated a 'self respect principle': that human grade AI should be designed with an appropriate appreciation of its own value.

John Basl and Ronald Sandler of Northeastern University argued for AI research committees to approve or deny research in cases where AI subjects might be harmed. They said it would not be very different from cases like animal testing where we have similar review boards, and sketched out details of how the proposal would work.

Daniel Kahneman, one of the most famous behavioral economists in the world, made something of a surprise appearance in the final panel. He said that we should take intuitions about case studies like the trolley problem seriously, as that is how the public will think about these events, for better or for worse. He said that no matter how AI cars kill people, it will be perceived with horror whenever the first incident happens, and we should prepare for that. Intuitions depend on irrelevant factors and will especially depend on whether AIs are designed to resemble us or not.

Gary Marcus, professor of psychology at NYU, of gave a much needed presentation about the nature of intelligence. The previous talks in this discussion had mostly assumed that intelligence was one-dimensional and simple and that there was some fixed idea of 'human-level' AI which we could eventually reach. Of course this is a ridiculous oversimplification; intelligence is multidimensional and it is more about implementing a combination of various cognitive tools, some of which are already stronger in AIs than in humans. AIs can be better or worse than us in various domains, so we really have no idea where AIs will be in this multidimensional space. AIs could in fact be better than us at moral reasoning. He also emphasized the gap is between machine learning today and what human reasoning can do.

Susan Schneider of Marquette University, a philosopher who has written quite a bit about AI and superintelligence, went over various issues. She argued that mind uploads might constitute death of the individual as long as we don't prove certain ideas about consciousness and personal identity, and also claimed that designing an intelligent and morally valuable robot to serve the interests of its creators would constitute slavery.

Jaan Tallinn, founder of Skype, also gave a quick talk. He has been a strong financial backer for MIRI and other efforts in this space, and simply expressed his belief in the importance of the issue and his happiness at the success of the conference and the number of students who were interested in pursuing the topic.

There was some final banter about the nature of consciousness which David Chalmers sat through very passively. Yudkowsky expressed optimism that one day we will have an explanation of consciousness which clears up our confusion on the matter. Nagel said that we will need to think more about the dynamics of multi-agent systems and moral epistemology. After that the event ended.

The conference videos are available [here.](http://livestream.com/nyu-tv/ethicsofAI) In my opinion, the best talks were given by LeCun, Railton, Russell, Yudkowsky, the Tegmarks, Petersen, and Marcus. The event overall was great and being in Manhattan made it even better. There was quite a bit of valuable informal meeting and discussion between many of the speakers and attendees. There was no 'sneering' or disdain about Yudkowsky or Bostrom as far as I could tell. It seemed like a generally open minded yet well educated crowd.

If you regret missing it, then you might like to head to the [Envision Conference](http://envision-conference.com/) this December. 

"
"Notes from the NYU AI Ethics conferenceThis weekend I attended the [Ethics of Artificial Intelligence conference](https://wp.nyu.edu/consciousness/ethics-of-artificial-intelligence/) at NYU. There were a ton of high-profile and interesting people there from philosophy (David Chalmers, Peter Railton, Nick Bostrom, Thomas Nagel, Paul Boghossian, Frances Kamm, Wendell Wallach) and science (Yann LeCun, Stuart Russell, Stephen Wolfram, Max Tegmark, Francesca Rossi) as well as Eliezer Yudkowsky. 

There were two fairly long days of talks and panels. David Chalmers (famous for his philosophy of mind and consciousness) did not officially speak but acted as chair for the event. He outlined the philosophy of the conference, which was to discuss both short and long term issues in AI ethics without worrying about either detracting from the other. He was, as usual, extremely awesome.

Here is a summary of the event with the most interesting points made by the speakers.

**Day One**

The first block of talks on Friday was an overview of general issues related to artificial intelligence. Nick Bostrom, author of *Superintelligence* and head of the Future of Humanity Institute, started with something of a barrage of all the general ideas and things he's come up with. He floated the idea that perhaps we shouldn't program AI systems to be maximally moral, for we don't know what the true morality looks like, and what if it turns out that such a directive would lead to humans being punished, or something else that was pathological or downright weird? He also described three principles for how we should treat AIs: substrate nondiscrimination (moral status does not depend on the kind of hardware/wetware you run on), ontogeny nondiscrimination (moral status does not depend on how you were created), and subjective time (moral value exists relative to subjectively experienced time rather than objective time, so if a mind ran at a fast clock speed its life would be more important, all other things being equal).

He pointed out that AI moral status could arise before they reach there is any such thing as human level AI - just like animals have moral status despite being much simpler than humans. He mentioned the possibility of a Malthusian catastrophe from unlimited digital reproduction as well as the possibility for vote manipulation through agent duplication, and how we'll need to prevent these two things.

He voiced support for meta level decisionmaking - a ['moral parliament'](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html) where we imagine moral theories sending 'delegates' to compromise over contentious issues. Such a system could also accommodate other values and interests besides moral theories.

He answered the question of ""what is humanity most likely to fail at?"" with a qualified choice of 'mind crime' committed against advanced AIs. Humans already have difficulty with empathy towards animals when they exist on farms or in the wild, but AI would not necessarily have the basic biological features which incline us to be empathetic at all towards animals. Some robots attract empathetic attention from humans, but many invisible automated processes are much harder for people to feel empathetic towards.

Virginia Dignum was next; she is at the Delft University of Technology and spoke about mechanisms for automated processes to make decisions. She specified four methods of decisionmaking based on whether decisions are taken deliberately or imposed upon a system and whether the decisions are made internally or externally. The two former features lead to algorithmic decisionmaking in machines; the latter two lead to imposed decisions predetermined by regulatory institutions. Deliberated external decisionmaking means there is a 'human in the loop' and internal imposed decisionmaking is essentially randomness.

Yann LeCun concluded this section with a pretty fantastic overview of deep learning methods and the limitations which stand in the way of progress in machine intelligence. He pointed out that reinforcement learning is a rare and narrow slice of the field today and that the greatest obstacles for machines include common sense judgements and abstraction. The biggest current problem for AI is unsupervised learning, which is having machines that can learn to classify things on their own without being given clearly labelled data from humans. He showcased some of the (very cool) features of adversarial learning which are being used to tackle this.

He expressed support for the orthogonality thesis, namely the idea that intelligence and morality are 'orthogonal' - just because an agent is very smart doesn't mean that it's necessarily moral. He believes we should build a few basic drives into AIs: do not hurt humans, interact with humans, crave positive feedback from trusted human trainers. He also described a couple of reasons for why he is not concerned about uncontrolled advanced artificial intelligence. One was that he is confident that objective functions can be specified in such a way as to make machines indifferent to being switched off, and the other is that a narrow-AI focused on eliminating an unfriendly general-AI would 'win' due to its specialization.

In Q&A, Stuart Russell objected to LeCun's confidence in machines being indifferent to being shut off based on the fact that self-preservation as a goal implicitly falls out of whatever other goals a machine has. Paul Boghossian objected to the 'behaviorist' nature of the speakers' points of view, saying that they were exempting consciousness from its proper role in these discussions. One person asked whether we should let AIs take charge of everything and supersede humanity - Bostrom pointed out that the space of possible futures is ""an enormous Petri dish"" which we don't understand; an AI future could materialize as a planet sized supercomputer with no moral status, and we will need to learn how to engineer friendly advanced AI systems no matter what the plan is. 

The rest of the Friday talks were devoted to near-future issues with specific AI systems. Peter Asaro started with an overview of his organization, the 'Campaign to Stop Killer Robots'. He stated that targeting and killing should remain human-controlled actions. While he acknowledged that automated weaponry could result in fewer casualties on the battlefield, he believed that it was too narrow a view of the consequences. He said that it's not straightforward to translate complicated battlefield morality questions for machines to understand, and is worried about unintended initiation and escalation of conflicts through automated systems, arms races, and threats to humanitarian law. He also believes that people should only be killed with 'dignity' and that doing it with a robot robs people of this. Therefore, he called for a clear and strong norm against automated weapons.

Kate Devlin of the University of London gave a brief overview of the ethics of artificial sexuality. Looking at the history of sexualized robots featured in fictional media, she noted that almost all of them are female. Today there is a ""Campaign Against Sex Robots"" which is based on the idea that sexual robots would lead to the objectification of women. Devlin does not agree as she thinks it is too early to ban the technology and that we should explore it before thinking about banning it, especially since it does not really harm anyone. Instead she wants us to think about how to develop it correctly. There are many potential uses for these types of robots ranging all the way to the therapeutic; many of the rudimentary ones being sold today are bought by people who are incapable of forming ordinary relationships for various reasons. VR is being used in arousal tests to gauge the efficacy of treatments against pedophilia.

She noted that gender issues have arisen in technology already; the history of gendered technology includes pacemakers originally designed only for men and phones too large for women's pockets. We should get into AI now to make sure that it is not designed in problematic ways.

She mentioned privacy concerns, as the manufactures of the female stimulator WeVibe have already been sued over concerns that they were not properly informing customers of their collection of data from the devices. She wondered if we will ever get to a stage where a robot might have some knowledge of its role and refuse to give consent to its use, and if transmission/duplication of data and code between machines could serve as some form of digital sexual reproduction.

Vasant Dhar of NYU spoke next about data and privacy in the era of autonomous vehicles. He said that our legal and financial liability institutions are based on outdated notions of data and that they fail to address liability and crime. However, the tools we have now even in ordinary cars for recording data can be used to improve insurance and judicial systems. He proposed black boxes for cars that would contain all relevant data to determine fault in the event of accidents, and said that customers should have the choice to share their driving data with insurance companies to get lower premiums. 

Dhar reiterated the importance of improving vehicle safety through autonomous driving; each percentage point reduction in vehicle accidents equates to 400 deaths and 40,000 injuries avoided every year.

Adam Kolber followed up with a discussion of whether ""the code is the law"", based on the case study of [The DAO](https://en.wikipedia.org/wiki/The_DAO_%28organization%29) which was an automated capital fund which was subjected to a $50 million loss through exploitation. The answer apparently is that the code should not be the law, even though many people seemed to accept that it was.

Steve Wolfram of WolframAlpha and Mathematica fame discussed the issues of computer languages and goal specification. He said that his life work has essentially been about trying to find ways for humans to specify their goals to machines, and that this can work for ethics as well as for math. He doesn't think that any single moral theory is likely to work for guiding artificial intelligence, apparently because of Godel's theorem and the incompleteness of computational languages.

Francesca Rossi of IBM argued that for AIs and humans to interact very productively we will have to embed them in environments, so that rather than picking up a tool like a laptop or a phone, we are interacting with artificial systems all around is in our rooms and spaces. Humans will be recognized by their environments and our needs and wants will be inferred or asked about. AI embedded in environments can have memories about humans to better serve their interests. Most of all, we will need to establish trust between humans and AIs.

Peter Railton, philosopher at the University of Michigan, attacked the subjects of orthogonality and value learning. He said that we can't simply tell AIs to do what we want because our wants and values require critical assessment. He said that the orthogonality thesis might be right, but as we increasingly interact with systems and allow them to participate in our own lives and decisionmaking, the question of what it would take for them to be intelligent might involve certain features relevant to morality.

He stated that AIs should be thought of as social creatures; as a simple model, self regulation in a Hobbesian social contract leads to constraints and respect derived from self preservation. A society of intelligent cooperators can resist aggression and malice, and being moral is more efficient for a community than being cunning. From these principles we have a recipe for building proto-moral agents.

He discussed the 'moral point of view' required for many strong ethical theories such as Kantian ethics and consequentialism: it requires agents to have a hierarchical, non-perspectival, modal/planning-oriented, and consistent view of the world which assigns intrinsic moral weight to things. He described how all these features are also part of the process of becoming generally intelligent in the first place, implying that general social intelligence ensures the necessary information required for moral decisionmaking. In the path towards functional moral agents, we will have to build agents which can represent the goals of others and have them learn how to act in beneficial ways. So if we can build AIs that we can trust, then we are on a good path towards building artificial moral agents.

In the Q&A, Eliezer Yudkowsky objected that in the long run the 'instrumental strategy' is not quite what you want because maximizing people's desires as they are explicitly revealed can lead to bad outcomes, and you have to have a view like coherent extrapolated volition which asks what people would really want. Russell objected that when an agent becomes sufficiently powerful, it has no need to cooperate anymore.

Regina Rini of the NYU Center for Bioethics stated that the approaches to ethics so far described relied too much on the Western post-enlightenment view of ethics, which is a historical aberration, and excluded African, Chinese and other approaches to ethics. Railton stated that his scheme was grounded in basic empathy and not mediated by any higher order moral theory; Wolfram and Rossi said that no one ethical approach will work and AI will have to represent diverse values.

**Day Two**

Saturday was devoted to long term discussion of the future of advanced artificial intelligence. Stuart Russell, professor at UC Berkeley and head of the new Center for Human Compatible Artificial Intelligence, started with a basic overview of the control problem. He described the points made in Steve Omohundro's paper on convergent instrumental drives. He also had some pretty harsh words for the researchers in the AI community which have denied and rejected notions of the control problem without seriously engaging with the relevant literature.

He had three simple ideas which he proposed to constitute the definition of 'provably beneficial' AI: maximizing values for humans is the system's only goal; the robot is initially uncertain about these goals, and the best source of information is human behavior. He referred to inverse reinforcement learning as a technique for machines to learn human preferences, and said that uncertainty provides an incentive for machines to learn, ask questions, and explore cautiously. 

His answer to the off-switch problem is to make robots unsure of their objectives, so that they assume that the human will switch the robot off if and only if it has a good reason to, and will therefore be complicit with the action. He said that the wireheading problem can be avoided if you construct the reward signal as information about the reward function rather than as a reward itself; this way, any hijacking of the reward signal makes it useless.

He said that there is a strong economic incentive for value alignment, but humans are irrational, nasty, inconsistent, and weak-willed.

The next speaker was Eliezer Yudkowsky of the Machine Intelligence Research Institute. Chalmers pointed out his role there as well as his side venture in Harry Potter fanfiction.

Yudkowsky started [his talk](https://intelligence.org/nyu-talk/) by pointing out how the Terminator pictures in every media article about the control problem are inappropriate. The real analogy to be used is [Mickey Mouse as the Sorcerer's Apprentice in *Fantasia*.](https://www.youtube.com/watch?v=Ait_Fs6UQhQ) 

He said that the first difficulty of AI alignnment is that the utility functions we imagine are too simple, and the second difficulty is that maximizing the probability of achieving a given goal leads to pathological outcomes. He and MIRI are concerned with the nature of the goal of 'maximizing' and how to define goals in a way that avoids the problems of perverse instantiation. 

He said that the fears of AI being developed by some terrorist or rogue group were silly, as ""ISIS is not developing convolutional neural nets."" Instead the most powerful AI is likely to be developed by large groups in government, academia and industry.

He claimed that the four central propositions which support the idea that AI is a very big problem are: the orthogonality thesis, instrumental convergence, capability gain (the speed at which advanced AI can make itself better), and alignment difficulty. He said the first two are logical matters of computer science that people always learn to accept when they reflect upon them, while the latter two are more controversial.

The next talk was from Max Tegmark and Meia Chita-Tegmark. Max is a world-renowned physicist who helps run the Future of Life Institute, and Meia is a psychologist. They explained how physics and psychology provide useful tools for understanding artificial intelligence; physics tells us about computation and the constraints of the universe, and psychology tells us about the nature of well being, ways to debug the mind when reasoning about AI and methods to design psychomorphic AIs. Meia was the only speaker at the conference to discuss unemployment in any detail; she pointed out that retirement has only mixed effects on well being and that happiness comes from financial satisfaction and feelings of respect. She said that studying homemakers, part time workers and early retirees can tell us more about how an automated economy would affect people's well-being.

Max checked off [a list of common myths](http://futureoflife.org/background/aimyths/) regarding advanced AI. Meia said that we should look at the cognitive biases which have led to these misconceptions (such as availability bias leading to people worrying about robots rather than invisible artificial intelligence) and figure out how to avoid similar bugs from inhibiting our thinking in the future.

By the way, Max Tegmark is very cool, he has a sort of old-rocker-dude vibe, and he and Meia are super cute together.

Wendell Wallach of Yale spoke next. He is the man who quite literally wrote [the book](https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975) on AI ethics. He distinguished top-down approaches of formally specifying AI behaviors from bottom-up approaches of value learning. He said that neither will be sufficient on its own and that both have important roles to play. He is worried that AI engineers will make simplistic assumptions about AI, such as the idea that every decision should be utilitarian or the idea that 'ethics' and 'morality' are icky concepts that can be ignored.

Steve Petersen, a philosopher at the University of Niagara, gave the next talk, based on the draft of a forthcoming paper of his. He aims to push back against the orthogonality thesis and modulate the level of the risk assessment provided by Bostrom. His argument is that designing AI to follow any complex goal will necessarily require it to be able to learn the values of its ""teleological ancestors"" (the original human designers or the previous iterations of AI before it self-improved or self-modified) and arrive at a state of coherence between goals. As agents replicate, self-modify and merge in the digital world, there can be no fact of the matter about which agents are the same or different; instead there will be an 'agential soup' unified by a common teleological thread originating with the designers. Coherence reasoning leads to impartial reasoning with the goals of other agents.

There were several responses to him in Q&A. Yudkowsky's objection was that reaching coherence requires a meta-preference framework with particular assumptions about the universe and ontology; therefore, for any goal, there are many preference frameworks which could fulfill it, many of which would be perverse. Russell said that just coherence is not enough because you need the systems to give special weight to humans. Max Tegmark said that the problem was the vagueness of humanity's final goals. Chalmers pointed out that the orthogonality thesis still allows for all kinds of correlations between between intelligence and morality, as long as they are not necessary by design. Petersen said that he is arguing for 'attractor basins' in the possibility space of AI minds. Interestingly, he was motivated to start his research by the [Dylan Matthews Vox article](http://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai) on effective altruism where Dylan thought that effective altruists shouldn't be concerned by artificial intelligence. Petersen doesn't think that AI is unimportant and thinks that Bostrom and Yudkowsky's work is valuable, but he wanted to get a more critical assessment of the level of risk when he learned that alternative altruistic projects were at stake.

Matthew Liao of the NYU Center for Bioethics gave an argument for moral status on the basis of capabilities - that an entity is morally valuable to the extent that it has the physical/genetic basis for achieving features of moral relevance. I did not get a chance to ask him if this would imply that a 'seed AI' could be the most morally valuable entity in the world. He did argue against the ideas that level of intelligence or degree of moral agency determine moral status, as we don't normally think that smarter or more benevolent humans are more morally valuable than others. 

Liao argued that moral theories are too specific and too high level to be generally implemented in AIs. Instead, AI will need a universal moral grammar in which to specify morality. The holy grail is to develop machines that understand why things are right or wrong.

Eric Schwitzgebel and Mara Garza of UC Riverside argued for basic principles of AI rights. They introduced a very weak ""no-relevant-difference"" argument: the idea that there are possible AIs which have the same morally relevant features that humans do and therefore there are possible AIs with equal value to humans. They questioned if cheerfully suicidal or hardworking AI is acceptable, and stated a 'self respect principle': that human grade AI should be designed with an appropriate appreciation of its own value.

John Basl and Ronald Sandler of Northeastern University argued for AI research committees to approve or deny research in cases where AI subjects might be harmed. They said it would not be very different from cases like animal testing where we have similar review boards, and sketched out details of how the proposal would work.

Daniel Kahneman, one of the most famous behavioral economists in the world, made something of a surprise appearance in the final panel. He said that we should take intuitions about case studies like the trolley problem seriously, as that is how the public will think about these events, for better or for worse. He said that no matter how AI cars kill people, it will be perceived with horror whenever the first incident happens, and we should prepare for that. Intuitions depend on irrelevant factors and will especially depend on whether AIs are designed to resemble us or not.

Gary Marcus, professor of psychology at NYU, of gave a much needed presentation about the nature of intelligence. The previous talks in this discussion had mostly assumed that intelligence was one-dimensional and simple and that there was some fixed idea of 'human-level' AI which we could eventually reach. Of course this is a ridiculous oversimplification; intelligence is multidimensional and it is more about implementing a combination of various cognitive tools, some of which are already stronger in AIs than in humans. AIs can be better or worse than us in various domains, so we really have no idea where AIs will be in this multidimensional space. AIs could in fact be better than us at moral reasoning. He also emphasized the gap is between machine learning today and what human reasoning can do.

Susan Schneider of Marquette University, a philosopher who has written quite a bit about AI and superintelligence, went over various issues. She argued that mind uploads might constitute death of the individual as long as we don't prove certain ideas about consciousness and personal identity, and also claimed that designing an intelligent and morally valuable robot to serve the interests of its creators would constitute slavery.

Jaan Tallinn, founder of Skype, also gave a quick talk. He has been a strong financial backer for MIRI and other efforts in this space, and simply expressed his belief in the importance of the issue and his happiness at the success of the conference and the number of students who were interested in pursuing the topic.

There was some final banter about the nature of consciousness which David Chalmers sat through very passively. Yudkowsky expressed optimism that one day we will have an explanation of consciousness which clears up our confusion on the matter. Nagel said that we will need to think more about the dynamics of multi-agent systems and moral epistemology. After that the event ended.

The conference videos are available [here.](http://livestream.com/nyu-tv/ethicsofAI) In my opinion, the best talks were given by LeCun, Railton, Russell, Yudkowsky, the Tegmarks, Petersen, and Marcus. The event overall was great and being in Manhattan made it even better. There was quite a bit of valuable informal meeting and discussion between many of the speakers and attendees. There was no 'sneering' or disdain about Yudkowsky or Bostrom as far as I could tell. It seemed like a generally open minded yet well educated crowd.

If you regret missing it, then you might like to head to the [Envision Conference](http://envision-conference.com/) this December. 

"
"Building Trust with Responsible AIArtificial Intelligence is being used in almost every aspect of life. AI symbolizes growth and productivity in the minds of some, but it is raising questions as well on the fairness, privacy, and security of these systems. Many legitimate issues exist, including biased choices, labor replacement, and a lack of security. When it comes to robots, this is very frightening. Self-driving automobiles, for example, can cause injury or death if they make mistakes. Responsible AI addresses these difficulties and makes AI systems more accountable.

**Responsible AI should fulfill the following aims:**

* Interpretability: We obtain an explanation for how a model makes predictions when we interpret it. An AI system makes predictions for a user. Even if these selections are correct, a user is likely to seek an explanation. Responsible AI can describe how we create interpretable models.
* Fairness: AI systems have the potential to make judgments that are biased towards particular groups of people. Bias in the training data is the source of this bias. The easier it is to assure fairness and rectify any bias in a model, the more interpretable it is. As a result, we need a Responsible AI framework to explain how we evaluate fairness and what to do if a model makes unjust predictions.
* Safety and Security: AI systems aren’t deterministic. When confronted with new situations, they are prone to making poor choices. The systems can even be tampered with to make unwise decisions. Therefore, we need to ensure safety and security in these systems.
* Data Governance: The data used must be of high quality. If the data used by AI has errors, the system may make wrong decisions.

**Continue Reading The Article** [**Here**](https://www.marktechpost.com/2022/04/02/building-trust-with-responsible-ai/)

&#x200B;

https://preview.redd.it/3ckdtznrr6r81.png?width=1024&format=png&auto=webp&v=enabled&s=d9a7590306f5126b1e214a2b34e0a5da8ac73d70"
"Building Trust with Responsible AIArtificial Intelligence is being used in almost every aspect of life. AI symbolizes growth and productivity in the minds of some, but it is raising questions as well on the fairness, privacy, and security of these systems. Many legitimate issues exist, including biased choices, labor replacement, and a lack of security. When it comes to robots, this is very frightening. Self-driving automobiles, for example, can cause injury or death if they make mistakes. Responsible AI addresses these difficulties and makes AI systems more accountable.

**Responsible AI should fulfill the following aims:**

* Interpretability: We obtain an explanation for how a model makes predictions when we interpret it. An AI system makes predictions for a user. Even if these selections are correct, a user is likely to seek an explanation. Responsible AI can describe how we create interpretable models.
* Fairness: AI systems have the potential to make judgments that are biased towards particular groups of people. Bias in the training data is the source of this bias. The easier it is to assure fairness and rectify any bias in a model, the more interpretable it is. As a result, we need a Responsible AI framework to explain how we evaluate fairness and what to do if a model makes unjust predictions.
* Safety and Security: AI systems aren’t deterministic. When confronted with new situations, they are prone to making poor choices. The systems can even be tampered with to make unwise decisions. Therefore, we need to ensure safety and security in these systems.
* Data Governance: The data used must be of high quality. If the data used by AI has errors, the system may make wrong decisions.

**Continue Reading The Article** [**Here**](https://www.marktechpost.com/2022/04/02/building-trust-with-responsible-ai/)

&#x200B;

https://preview.redd.it/3ckdtznrr6r81.png?width=1024&format=png&auto=webp&v=enabled&s=d9a7590306f5126b1e214a2b34e0a5da8ac73d70"
"Notes from the NYU AI Ethics conferenceThis weekend I attended the [Ethics of Artificial Intelligence conference](https://wp.nyu.edu/consciousness/ethics-of-artificial-intelligence/) at NYU. There were a ton of high-profile and interesting people there from philosophy (David Chalmers, Peter Railton, Nick Bostrom, Thomas Nagel, Paul Boghossian, Frances Kamm, Wendell Wallach) and science (Yann LeCun, Stuart Russell, Stephen Wolfram, Max Tegmark, Francesca Rossi) as well as Eliezer Yudkowsky. 

There were two fairly long days of talks and panels. David Chalmers (famous for his philosophy of mind and consciousness) did not officially speak but acted as chair for the event. He outlined the philosophy of the conference, which was to discuss both short and long term issues in AI ethics without worrying about either detracting from the other. He was, as usual, extremely awesome.

Here is a summary of the event with the most interesting points made by the speakers.

**Day One**

The first block of talks on Friday was an overview of general issues related to artificial intelligence. Nick Bostrom, author of *Superintelligence* and head of the Future of Humanity Institute, started with something of a barrage of all the general ideas and things he's come up with. He floated the idea that perhaps we shouldn't program AI systems to be maximally moral, for we don't know what the true morality looks like, and what if it turns out that such a directive would lead to humans being punished, or something else that was pathological or downright weird? He also described three principles for how we should treat AIs: substrate nondiscrimination (moral status does not depend on the kind of hardware/wetware you run on), ontogeny nondiscrimination (moral status does not depend on how you were created), and subjective time (moral value exists relative to subjectively experienced time rather than objective time, so if a mind ran at a fast clock speed its life would be more important, all other things being equal).

He pointed out that AI moral status could arise before they reach there is any such thing as human level AI - just like animals have moral status despite being much simpler than humans. He mentioned the possibility of a Malthusian catastrophe from unlimited digital reproduction as well as the possibility for vote manipulation through agent duplication, and how we'll need to prevent these two things.

He voiced support for meta level decisionmaking - a ['moral parliament'](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html) where we imagine moral theories sending 'delegates' to compromise over contentious issues. Such a system could also accommodate other values and interests besides moral theories.

He answered the question of ""what is humanity most likely to fail at?"" with a qualified choice of 'mind crime' committed against advanced AIs. Humans already have difficulty with empathy towards animals when they exist on farms or in the wild, but AI would not necessarily have the basic biological features which incline us to be empathetic at all towards animals. Some robots attract empathetic attention from humans, but many invisible automated processes are much harder for people to feel empathetic towards.

Virginia Dignum was next; she is at the Delft University of Technology and spoke about mechanisms for automated processes to make decisions. She specified four methods of decisionmaking based on whether decisions are taken deliberately or imposed upon a system and whether the decisions are made internally or externally. The two former features lead to algorithmic decisionmaking in machines; the latter two lead to imposed decisions predetermined by regulatory institutions. Deliberated external decisionmaking means there is a 'human in the loop' and internal imposed decisionmaking is essentially randomness.

Yann LeCun concluded this section with a pretty fantastic overview of deep learning methods and the limitations which stand in the way of progress in machine intelligence. He pointed out that reinforcement learning is a rare and narrow slice of the field today and that the greatest obstacles for machines include common sense judgements and abstraction. The biggest current problem for AI is unsupervised learning, which is having machines that can learn to classify things on their own without being given clearly labelled data from humans. He showcased some of the (very cool) features of adversarial learning which are being used to tackle this.

He expressed support for the orthogonality thesis, namely the idea that intelligence and morality are 'orthogonal' - just because an agent is very smart doesn't mean that it's necessarily moral. He believes we should build a few basic drives into AIs: do not hurt humans, interact with humans, crave positive feedback from trusted human trainers. He also described a couple of reasons for why he is not concerned about uncontrolled advanced artificial intelligence. One was that he is confident that objective functions can be specified in such a way as to make machines indifferent to being switched off, and the other is that a narrow-AI focused on eliminating an unfriendly general-AI would 'win' due to its specialization.

In Q&A, Stuart Russell objected to LeCun's confidence in machines being indifferent to being shut off based on the fact that self-preservation as a goal implicitly falls out of whatever other goals a machine has. Paul Boghossian objected to the 'behaviorist' nature of the speakers' points of view, saying that they were exempting consciousness from its proper role in these discussions. One person asked whether we should let AIs take charge of everything and supersede humanity - Bostrom pointed out that the space of possible futures is ""an enormous Petri dish"" which we don't understand; an AI future could materialize as a planet sized supercomputer with no moral status, and we will need to learn how to engineer friendly advanced AI systems no matter what the plan is. 

The rest of the Friday talks were devoted to near-future issues with specific AI systems. Peter Asaro started with an overview of his organization, the 'Campaign to Stop Killer Robots'. He stated that targeting and killing should remain human-controlled actions. While he acknowledged that automated weaponry could result in fewer casualties on the battlefield, he believed that it was too narrow a view of the consequences. He said that it's not straightforward to translate complicated battlefield morality questions for machines to understand, and is worried about unintended initiation and escalation of conflicts through automated systems, arms races, and threats to humanitarian law. He also believes that people should only be killed with 'dignity' and that doing it with a robot robs people of this. Therefore, he called for a clear and strong norm against automated weapons.

Kate Devlin of the University of London gave a brief overview of the ethics of artificial sexuality. Looking at the history of sexualized robots featured in fictional media, she noted that almost all of them are female. Today there is a ""Campaign Against Sex Robots"" which is based on the idea that sexual robots would lead to the objectification of women. Devlin does not agree as she thinks it is too early to ban the technology and that we should explore it before thinking about banning it, especially since it does not really harm anyone. Instead she wants us to think about how to develop it correctly. There are many potential uses for these types of robots ranging all the way to the therapeutic; many of the rudimentary ones being sold today are bought by people who are incapable of forming ordinary relationships for various reasons. VR is being used in arousal tests to gauge the efficacy of treatments against pedophilia.

She noted that gender issues have arisen in technology already; the history of gendered technology includes pacemakers originally designed only for men and phones too large for women's pockets. We should get into AI now to make sure that it is not designed in problematic ways.

She mentioned privacy concerns, as the manufactures of the female stimulator WeVibe have already been sued over concerns that they were not properly informing customers of their collection of data from the devices. She wondered if we will ever get to a stage where a robot might have some knowledge of its role and refuse to give consent to its use, and if transmission/duplication of data and code between machines could serve as some form of digital sexual reproduction.

Vasant Dhar of NYU spoke next about data and privacy in the era of autonomous vehicles. He said that our legal and financial liability institutions are based on outdated notions of data and that they fail to address liability and crime. However, the tools we have now even in ordinary cars for recording data can be used to improve insurance and judicial systems. He proposed black boxes for cars that would contain all relevant data to determine fault in the event of accidents, and said that customers should have the choice to share their driving data with insurance companies to get lower premiums. 

Dhar reiterated the importance of improving vehicle safety through autonomous driving; each percentage point reduction in vehicle accidents equates to 400 deaths and 40,000 injuries avoided every year.

Adam Kolber followed up with a discussion of whether ""the code is the law"", based on the case study of [The DAO](https://en.wikipedia.org/wiki/The_DAO_%28organization%29) which was an automated capital fund which was subjected to a $50 million loss through exploitation. The answer apparently is that the code should not be the law, even though many people seemed to accept that it was.

Steve Wolfram of WolframAlpha and Mathematica fame discussed the issues of computer languages and goal specification. He said that his life work has essentially been about trying to find ways for humans to specify their goals to machines, and that this can work for ethics as well as for math. He doesn't think that any single moral theory is likely to work for guiding artificial intelligence, apparently because of Godel's theorem and the incompleteness of computational languages.

Francesca Rossi of IBM argued that for AIs and humans to interact very productively we will have to embed them in environments, so that rather than picking up a tool like a laptop or a phone, we are interacting with artificial systems all around is in our rooms and spaces. Humans will be recognized by their environments and our needs and wants will be inferred or asked about. AI embedded in environments can have memories about humans to better serve their interests. Most of all, we will need to establish trust between humans and AIs.

Peter Railton, philosopher at the University of Michigan, attacked the subjects of orthogonality and value learning. He said that we can't simply tell AIs to do what we want because our wants and values require critical assessment. He said that the orthogonality thesis might be right, but as we increasingly interact with systems and allow them to participate in our own lives and decisionmaking, the question of what it would take for them to be intelligent might involve certain features relevant to morality.

He stated that AIs should be thought of as social creatures; as a simple model, self regulation in a Hobbesian social contract leads to constraints and respect derived from self preservation. A society of intelligent cooperators can resist aggression and malice, and being moral is more efficient for a community than being cunning. From these principles we have a recipe for building proto-moral agents.

He discussed the 'moral point of view' required for many strong ethical theories such as Kantian ethics and consequentialism: it requires agents to have a hierarchical, non-perspectival, modal/planning-oriented, and consistent view of the world which assigns intrinsic moral weight to things. He described how all these features are also part of the process of becoming generally intelligent in the first place, implying that general social intelligence ensures the necessary information required for moral decisionmaking. In the path towards functional moral agents, we will have to build agents which can represent the goals of others and have them learn how to act in beneficial ways. So if we can build AIs that we can trust, then we are on a good path towards building artificial moral agents.

In the Q&A, Eliezer Yudkowsky objected that in the long run the 'instrumental strategy' is not quite what you want because maximizing people's desires as they are explicitly revealed can lead to bad outcomes, and you have to have a view like coherent extrapolated volition which asks what people would really want. Russell objected that when an agent becomes sufficiently powerful, it has no need to cooperate anymore.

Regina Rini of the NYU Center for Bioethics stated that the approaches to ethics so far described relied too much on the Western post-enlightenment view of ethics, which is a historical aberration, and excluded African, Chinese and other approaches to ethics. Railton stated that his scheme was grounded in basic empathy and not mediated by any higher order moral theory; Wolfram and Rossi said that no one ethical approach will work and AI will have to represent diverse values.

**Day Two**

Saturday was devoted to long term discussion of the future of advanced artificial intelligence. Stuart Russell, professor at UC Berkeley and head of the new Center for Human Compatible Artificial Intelligence, started with a basic overview of the control problem. He described the points made in Steve Omohundro's paper on convergent instrumental drives. He also had some pretty harsh words for the researchers in the AI community which have denied and rejected notions of the control problem without seriously engaging with the relevant literature.

He had three simple ideas which he proposed to constitute the definition of 'provably beneficial' AI: maximizing values for humans is the system's only goal; the robot is initially uncertain about these goals, and the best source of information is human behavior. He referred to inverse reinforcement learning as a technique for machines to learn human preferences, and said that uncertainty provides an incentive for machines to learn, ask questions, and explore cautiously. 

His answer to the off-switch problem is to make robots unsure of their objectives, so that they assume that the human will switch the robot off if and only if it has a good reason to, and will therefore be complicit with the action. He said that the wireheading problem can be avoided if you construct the reward signal as information about the reward function rather than as a reward itself; this way, any hijacking of the reward signal makes it useless.

He said that there is a strong economic incentive for value alignment, but humans are irrational, nasty, inconsistent, and weak-willed.

The next speaker was Eliezer Yudkowsky of the Machine Intelligence Research Institute. Chalmers pointed out his role there as well as his side venture in Harry Potter fanfiction.

Yudkowsky started [his talk](https://intelligence.org/nyu-talk/) by pointing out how the Terminator pictures in every media article about the control problem are inappropriate. The real analogy to be used is [Mickey Mouse as the Sorcerer's Apprentice in *Fantasia*.](https://www.youtube.com/watch?v=Ait_Fs6UQhQ) 

He said that the first difficulty of AI alignnment is that the utility functions we imagine are too simple, and the second difficulty is that maximizing the probability of achieving a given goal leads to pathological outcomes. He and MIRI are concerned with the nature of the goal of 'maximizing' and how to define goals in a way that avoids the problems of perverse instantiation. 

He said that the fears of AI being developed by some terrorist or rogue group were silly, as ""ISIS is not developing convolutional neural nets."" Instead the most powerful AI is likely to be developed by large groups in government, academia and industry.

He claimed that the four central propositions which support the idea that AI is a very big problem are: the orthogonality thesis, instrumental convergence, capability gain (the speed at which advanced AI can make itself better), and alignment difficulty. He said the first two are logical matters of computer science that people always learn to accept when they reflect upon them, while the latter two are more controversial.

The next talk was from Max Tegmark and Meia Chita-Tegmark. Max is a world-renowned physicist who helps run the Future of Life Institute, and Meia is a psychologist. They explained how physics and psychology provide useful tools for understanding artificial intelligence; physics tells us about computation and the constraints of the universe, and psychology tells us about the nature of well being, ways to debug the mind when reasoning about AI and methods to design psychomorphic AIs. Meia was the only speaker at the conference to discuss unemployment in any detail; she pointed out that retirement has only mixed effects on well being and that happiness comes from financial satisfaction and feelings of respect. She said that studying homemakers, part time workers and early retirees can tell us more about how an automated economy would affect people's well-being.

Max checked off [a list of common myths](http://futureoflife.org/background/aimyths/) regarding advanced AI. Meia said that we should look at the cognitive biases which have led to these misconceptions (such as availability bias leading to people worrying about robots rather than invisible artificial intelligence) and figure out how to avoid similar bugs from inhibiting our thinking in the future.

By the way, Max Tegmark is very cool, he has a sort of old-rocker-dude vibe, and he and Meia are super cute together.

Wendell Wallach of Yale spoke next. He is the man who quite literally wrote [the book](https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975) on AI ethics. He distinguished top-down approaches of formally specifying AI behaviors from bottom-up approaches of value learning. He said that neither will be sufficient on its own and that both have important roles to play. He is worried that AI engineers will make simplistic assumptions about AI, such as the idea that every decision should be utilitarian or the idea that 'ethics' and 'morality' are icky concepts that can be ignored.

Steve Petersen, a philosopher at the University of Niagara, gave the next talk, based on the draft of a forthcoming paper of his. He aims to push back against the orthogonality thesis and modulate the level of the risk assessment provided by Bostrom. His argument is that designing AI to follow any complex goal will necessarily require it to be able to learn the values of its ""teleological ancestors"" (the original human designers or the previous iterations of AI before it self-improved or self-modified) and arrive at a state of coherence between goals. As agents replicate, self-modify and merge in the digital world, there can be no fact of the matter about which agents are the same or different; instead there will be an 'agential soup' unified by a common teleological thread originating with the designers. Coherence reasoning leads to impartial reasoning with the goals of other agents.

There were several responses to him in Q&A. Yudkowsky's objection was that reaching coherence requires a meta-preference framework with particular assumptions about the universe and ontology; therefore, for any goal, there are many preference frameworks which could fulfill it, many of which would be perverse. Russell said that just coherence is not enough because you need the systems to give special weight to humans. Max Tegmark said that the problem was the vagueness of humanity's final goals. Chalmers pointed out that the orthogonality thesis still allows for all kinds of correlations between between intelligence and morality, as long as they are not necessary by design. Petersen said that he is arguing for 'attractor basins' in the possibility space of AI minds. Interestingly, he was motivated to start his research by the [Dylan Matthews Vox article](http://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai) on effective altruism where Dylan thought that effective altruists shouldn't be concerned by artificial intelligence. Petersen doesn't think that AI is unimportant and thinks that Bostrom and Yudkowsky's work is valuable, but he wanted to get a more critical assessment of the level of risk when he learned that alternative altruistic projects were at stake.

Matthew Liao of the NYU Center for Bioethics gave an argument for moral status on the basis of capabilities - that an entity is morally valuable to the extent that it has the physical/genetic basis for achieving features of moral relevance. I did not get a chance to ask him if this would imply that a 'seed AI' could be the most morally valuable entity in the world. He did argue against the ideas that level of intelligence or degree of moral agency determine moral status, as we don't normally think that smarter or more benevolent humans are more morally valuable than others. 

Liao argued that moral theories are too specific and too high level to be generally implemented in AIs. Instead, AI will need a universal moral grammar in which to specify morality. The holy grail is to develop machines that understand why things are right or wrong.

Eric Schwitzgebel and Mara Garza of UC Riverside argued for basic principles of AI rights. They introduced a very weak ""no-relevant-difference"" argument: the idea that there are possible AIs which have the same morally relevant features that humans do and therefore there are possible AIs with equal value to humans. They questioned if cheerfully suicidal or hardworking AI is acceptable, and stated a 'self respect principle': that human grade AI should be designed with an appropriate appreciation of its own value.

John Basl and Ronald Sandler of Northeastern University argued for AI research committees to approve or deny research in cases where AI subjects might be harmed. They said it would not be very different from cases like animal testing where we have similar review boards, and sketched out details of how the proposal would work.

Daniel Kahneman, one of the most famous behavioral economists in the world, made something of a surprise appearance in the final panel. He said that we should take intuitions about case studies like the trolley problem seriously, as that is how the public will think about these events, for better or for worse. He said that no matter how AI cars kill people, it will be perceived with horror whenever the first incident happens, and we should prepare for that. Intuitions depend on irrelevant factors and will especially depend on whether AIs are designed to resemble us or not.

Gary Marcus, professor of psychology at NYU, of gave a much needed presentation about the nature of intelligence. The previous talks in this discussion had mostly assumed that intelligence was one-dimensional and simple and that there was some fixed idea of 'human-level' AI which we could eventually reach. Of course this is a ridiculous oversimplification; intelligence is multidimensional and it is more about implementing a combination of various cognitive tools, some of which are already stronger in AIs than in humans. AIs can be better or worse than us in various domains, so we really have no idea where AIs will be in this multidimensional space. AIs could in fact be better than us at moral reasoning. He also emphasized the gap is between machine learning today and what human reasoning can do.

Susan Schneider of Marquette University, a philosopher who has written quite a bit about AI and superintelligence, went over various issues. She argued that mind uploads might constitute death of the individual as long as we don't prove certain ideas about consciousness and personal identity, and also claimed that designing an intelligent and morally valuable robot to serve the interests of its creators would constitute slavery.

Jaan Tallinn, founder of Skype, also gave a quick talk. He has been a strong financial backer for MIRI and other efforts in this space, and simply expressed his belief in the importance of the issue and his happiness at the success of the conference and the number of students who were interested in pursuing the topic.

There was some final banter about the nature of consciousness which David Chalmers sat through very passively. Yudkowsky expressed optimism that one day we will have an explanation of consciousness which clears up our confusion on the matter. Nagel said that we will need to think more about the dynamics of multi-agent systems and moral epistemology. After that the event ended.

The conference videos are available [here.](http://livestream.com/nyu-tv/ethicsofAI) In my opinion, the best talks were given by LeCun, Railton, Russell, Yudkowsky, the Tegmarks, Petersen, and Marcus. The event overall was great and being in Manhattan made it even better. There was quite a bit of valuable informal meeting and discussion between many of the speakers and attendees. There was no 'sneering' or disdain about Yudkowsky or Bostrom as far as I could tell. It seemed like a generally open minded yet well educated crowd.

If you regret missing it, then you might like to head to the [Envision Conference](http://envision-conference.com/) this December. 

"
Creating robots capable of moral reasoning is like parenting – Regina Rini
Robots could eventually replace soldiers in warfare. Is that a good thing?
"If robots were to exist, fully sentient; do you think Buddhist teachings would help them grasp the concept of life more, possibly be even beneficial in general to their and our way of life?Knowing some things about Buddhism, restricting some mostly religious teachings; do you think it could be beneficial into sentient robots knowing more about life, suffering, and human behavior in a logical and emotional way. 
Personally, if such a race of robot were to co-exist with us, the idea interests me as I wonder if an artificial being could seek a more enlightened state of being. Could it spell more of an understanding of consciousness and what is needed to truly be happy? I'd like to hear some opinions about it, and learn more about it."
"Machine Ethics Reading ListThis is an overview of technical readings in machine ethics (developing moral frameworks for autonomous systems). I have less familiarity with other topics in AI ethics and have not done a review of the literature in those other fields, so I'm not making a reading list for all that at the moment.

**Papers**

Allen, C., Varner, G., & Zinser, J. (2000). Prolegomena to any future artificial moral agent. http://commonsenseatheism.com/wp-content/uploads/2009/08/Allen-Prolegomena-to-any-future-artificial-moral-agent.pdf

Anderson, M., Anderson, S. L., & Armen, C. (n.d.). Towards Machine Ethics: Implementing Two Action-Based Ethical Theories. https://www.aaai.org/Papers/Symposia/Fall/2005/FS-05-06/FS05-06-001.pdf

Arkoudas, K., Bringsjord, S., Bello, P. (2005). Toward ethical robots via mechanized deontic logic. https://www.aaai.org/Papers/Symposia/Fall/2005/FS-05-06/FS05-06-003.pdf

Armstrong, S. (2015). Motivated Value Selection for Artificial Agents. https://www.aaai.org/ocs/index.php/WS/AAAIW15/paper/viewFile/10183/10126

Bello, P., & Bringsjord, S. (2013). On How to Build a Moral Machine. https://doi.org/10.1007/s11245-012-9129-8

Bendel, O. (2013). Considerations about the relationship between animal and machine ethics. http://doi.org/10.1007/s00146-013-0526-3

Goodall, A. N. J. (2014). Machine Ethics and Automated Vehicles. http://people.virginia.edu/~njg2q/machineethics.pdf

Grau, C. (n.d.). There is no “I” in “Robot”: Robotic Utilitarians and Utilitarian Robots. https://www.aaai.org/Papers/Symposia/Fall/2005/FS-05-06/FS05-06-007.pdf

Lokhorst, G. J. C. (2011). Computational meta-ethics towards the meta-ethical robot. https://doi.org/10.1007/s11023-011-9229-z ([erratum](https://www.researchgate.net/publication/263369942_Erratum_to_Computational_Meta-Ethics_Towards_the_Meta-Ethical_Robot))

Muntean, I. & Howard, D. (2016). A minimalist model of the artificial autonomous moral agent (AAMA). https://www.aaai.org/ocs/index.php/SSS/SSS16/paper/download/12760/11954

Oesterheld, C. (2015). Formalizing preference utilitarianism in physical world models. https://doi.org/10.1007/s11229-015-0883-1

Pereira, L. M., & Saptawijaya, A. (2009). Modelling morality with prospective logic. https://doi.org/10.1504/IJRIS.2009.028020

Powers, T. M. (n.d.). Deontological Machine Ethics. https://www.aaai.org/Papers/Symposia/Fall/2005/FS-05-06/FS05-06-012.pdf

Powers, T. M. (n.d.). Prospects for a Smithian Machine. http://www.iacap.org/proceedings_IACAP13/paper_52.pdf

Shulman, C., Tarleton, N., & Jonsson, H. 2009. Which Consequentialism? Machine Ethics
and Moral Divergence. https://intelligence.org/files/WhichConsequentialism.pdf

Tarleton, N. (2010). Coherent Extrapolated Volition: A Meta-Level Approach to Machine Ethics. https://intelligence.org/files/CEV-MachineEthics.pdf

White, J. (n.d.). Autonomous Reboot: the challenges of artificial moral agency and the ends of Machine Ethics.  

White, J. (n.d.). A General Theory of Moral Agency Grounding Computational Implementations: The ACTWith Model. https://www.academia.edu/7000519/Autonomous_Reboot_the_challenges_of_artificial_moral_agency_and_the_ends_of_Machine_Ethics

Wiltshire, T. J. (2015). A Prospective Framework for the Design of Ideal Artificial Moral Agents: Insights from the Science of Heroism in Humans. https://doi.org/10.1007/s11023-015-9361-2

**Books**

Wallach, W. & Allen, C. Moral Machines: Teaching Robots Right from Wrong. https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975

**Encyclopedia Articles**

McNamara, P. ""Deontic Logic"", The Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/archives/win2014/entries/logic-deontic

Portoraro, F. ""Automated Reasoning"", The Stanford Encyclopedia of Philosophy. https://plato.stanford.edu/archives/win2014/entries/reasoning-automated/


---
I'm sure I've missed some so feel free to suggest additions."
"Notes from the NYU AI Ethics conferenceThis weekend I attended the [Ethics of Artificial Intelligence conference](https://wp.nyu.edu/consciousness/ethics-of-artificial-intelligence/) at NYU. There were a ton of high-profile and interesting people there from philosophy (David Chalmers, Peter Railton, Nick Bostrom, Thomas Nagel, Paul Boghossian, Frances Kamm, Wendell Wallach) and science (Yann LeCun, Stuart Russell, Stephen Wolfram, Max Tegmark, Francesca Rossi) as well as Eliezer Yudkowsky. 

There were two fairly long days of talks and panels. David Chalmers (famous for his philosophy of mind and consciousness) did not officially speak but acted as chair for the event. He outlined the philosophy of the conference, which was to discuss both short and long term issues in AI ethics without worrying about either detracting from the other. He was, as usual, extremely awesome.

Here is a summary of the event with the most interesting points made by the speakers.

**Day One**

The first block of talks on Friday was an overview of general issues related to artificial intelligence. Nick Bostrom, author of *Superintelligence* and head of the Future of Humanity Institute, started with something of a barrage of all the general ideas and things he's come up with. He floated the idea that perhaps we shouldn't program AI systems to be maximally moral, for we don't know what the true morality looks like, and what if it turns out that such a directive would lead to humans being punished, or something else that was pathological or downright weird? He also described three principles for how we should treat AIs: substrate nondiscrimination (moral status does not depend on the kind of hardware/wetware you run on), ontogeny nondiscrimination (moral status does not depend on how you were created), and subjective time (moral value exists relative to subjectively experienced time rather than objective time, so if a mind ran at a fast clock speed its life would be more important, all other things being equal).

He pointed out that AI moral status could arise before they reach there is any such thing as human level AI - just like animals have moral status despite being much simpler than humans. He mentioned the possibility of a Malthusian catastrophe from unlimited digital reproduction as well as the possibility for vote manipulation through agent duplication, and how we'll need to prevent these two things.

He voiced support for meta level decisionmaking - a ['moral parliament'](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html) where we imagine moral theories sending 'delegates' to compromise over contentious issues. Such a system could also accommodate other values and interests besides moral theories.

He answered the question of ""what is humanity most likely to fail at?"" with a qualified choice of 'mind crime' committed against advanced AIs. Humans already have difficulty with empathy towards animals when they exist on farms or in the wild, but AI would not necessarily have the basic biological features which incline us to be empathetic at all towards animals. Some robots attract empathetic attention from humans, but many invisible automated processes are much harder for people to feel empathetic towards.

Virginia Dignum was next; she is at the Delft University of Technology and spoke about mechanisms for automated processes to make decisions. She specified four methods of decisionmaking based on whether decisions are taken deliberately or imposed upon a system and whether the decisions are made internally or externally. The two former features lead to algorithmic decisionmaking in machines; the latter two lead to imposed decisions predetermined by regulatory institutions. Deliberated external decisionmaking means there is a 'human in the loop' and internal imposed decisionmaking is essentially randomness.

Yann LeCun concluded this section with a pretty fantastic overview of deep learning methods and the limitations which stand in the way of progress in machine intelligence. He pointed out that reinforcement learning is a rare and narrow slice of the field today and that the greatest obstacles for machines include common sense judgements and abstraction. The biggest current problem for AI is unsupervised learning, which is having machines that can learn to classify things on their own without being given clearly labelled data from humans. He showcased some of the (very cool) features of adversarial learning which are being used to tackle this.

He expressed support for the orthogonality thesis, namely the idea that intelligence and morality are 'orthogonal' - just because an agent is very smart doesn't mean that it's necessarily moral. He believes we should build a few basic drives into AIs: do not hurt humans, interact with humans, crave positive feedback from trusted human trainers. He also described a couple of reasons for why he is not concerned about uncontrolled advanced artificial intelligence. One was that he is confident that objective functions can be specified in such a way as to make machines indifferent to being switched off, and the other is that a narrow-AI focused on eliminating an unfriendly general-AI would 'win' due to its specialization.

In Q&A, Stuart Russell objected to LeCun's confidence in machines being indifferent to being shut off based on the fact that self-preservation as a goal implicitly falls out of whatever other goals a machine has. Paul Boghossian objected to the 'behaviorist' nature of the speakers' points of view, saying that they were exempting consciousness from its proper role in these discussions. One person asked whether we should let AIs take charge of everything and supersede humanity - Bostrom pointed out that the space of possible futures is ""an enormous Petri dish"" which we don't understand; an AI future could materialize as a planet sized supercomputer with no moral status, and we will need to learn how to engineer friendly advanced AI systems no matter what the plan is. 

The rest of the Friday talks were devoted to near-future issues with specific AI systems. Peter Asaro started with an overview of his organization, the 'Campaign to Stop Killer Robots'. He stated that targeting and killing should remain human-controlled actions. While he acknowledged that automated weaponry could result in fewer casualties on the battlefield, he believed that it was too narrow a view of the consequences. He said that it's not straightforward to translate complicated battlefield morality questions for machines to understand, and is worried about unintended initiation and escalation of conflicts through automated systems, arms races, and threats to humanitarian law. He also believes that people should only be killed with 'dignity' and that doing it with a robot robs people of this. Therefore, he called for a clear and strong norm against automated weapons.

Kate Devlin of the University of London gave a brief overview of the ethics of artificial sexuality. Looking at the history of sexualized robots featured in fictional media, she noted that almost all of them are female. Today there is a ""Campaign Against Sex Robots"" which is based on the idea that sexual robots would lead to the objectification of women. Devlin does not agree as she thinks it is too early to ban the technology and that we should explore it before thinking about banning it, especially since it does not really harm anyone. Instead she wants us to think about how to develop it correctly. There are many potential uses for these types of robots ranging all the way to the therapeutic; many of the rudimentary ones being sold today are bought by people who are incapable of forming ordinary relationships for various reasons. VR is being used in arousal tests to gauge the efficacy of treatments against pedophilia.

She noted that gender issues have arisen in technology already; the history of gendered technology includes pacemakers originally designed only for men and phones too large for women's pockets. We should get into AI now to make sure that it is not designed in problematic ways.

She mentioned privacy concerns, as the manufactures of the female stimulator WeVibe have already been sued over concerns that they were not properly informing customers of their collection of data from the devices. She wondered if we will ever get to a stage where a robot might have some knowledge of its role and refuse to give consent to its use, and if transmission/duplication of data and code between machines could serve as some form of digital sexual reproduction.

Vasant Dhar of NYU spoke next about data and privacy in the era of autonomous vehicles. He said that our legal and financial liability institutions are based on outdated notions of data and that they fail to address liability and crime. However, the tools we have now even in ordinary cars for recording data can be used to improve insurance and judicial systems. He proposed black boxes for cars that would contain all relevant data to determine fault in the event of accidents, and said that customers should have the choice to share their driving data with insurance companies to get lower premiums. 

Dhar reiterated the importance of improving vehicle safety through autonomous driving; each percentage point reduction in vehicle accidents equates to 400 deaths and 40,000 injuries avoided every year.

Adam Kolber followed up with a discussion of whether ""the code is the law"", based on the case study of [The DAO](https://en.wikipedia.org/wiki/The_DAO_%28organization%29) which was an automated capital fund which was subjected to a $50 million loss through exploitation. The answer apparently is that the code should not be the law, even though many people seemed to accept that it was.

Steve Wolfram of WolframAlpha and Mathematica fame discussed the issues of computer languages and goal specification. He said that his life work has essentially been about trying to find ways for humans to specify their goals to machines, and that this can work for ethics as well as for math. He doesn't think that any single moral theory is likely to work for guiding artificial intelligence, apparently because of Godel's theorem and the incompleteness of computational languages.

Francesca Rossi of IBM argued that for AIs and humans to interact very productively we will have to embed them in environments, so that rather than picking up a tool like a laptop or a phone, we are interacting with artificial systems all around is in our rooms and spaces. Humans will be recognized by their environments and our needs and wants will be inferred or asked about. AI embedded in environments can have memories about humans to better serve their interests. Most of all, we will need to establish trust between humans and AIs.

Peter Railton, philosopher at the University of Michigan, attacked the subjects of orthogonality and value learning. He said that we can't simply tell AIs to do what we want because our wants and values require critical assessment. He said that the orthogonality thesis might be right, but as we increasingly interact with systems and allow them to participate in our own lives and decisionmaking, the question of what it would take for them to be intelligent might involve certain features relevant to morality.

He stated that AIs should be thought of as social creatures; as a simple model, self regulation in a Hobbesian social contract leads to constraints and respect derived from self preservation. A society of intelligent cooperators can resist aggression and malice, and being moral is more efficient for a community than being cunning. From these principles we have a recipe for building proto-moral agents.

He discussed the 'moral point of view' required for many strong ethical theories such as Kantian ethics and consequentialism: it requires agents to have a hierarchical, non-perspectival, modal/planning-oriented, and consistent view of the world which assigns intrinsic moral weight to things. He described how all these features are also part of the process of becoming generally intelligent in the first place, implying that general social intelligence ensures the necessary information required for moral decisionmaking. In the path towards functional moral agents, we will have to build agents which can represent the goals of others and have them learn how to act in beneficial ways. So if we can build AIs that we can trust, then we are on a good path towards building artificial moral agents.

In the Q&A, Eliezer Yudkowsky objected that in the long run the 'instrumental strategy' is not quite what you want because maximizing people's desires as they are explicitly revealed can lead to bad outcomes, and you have to have a view like coherent extrapolated volition which asks what people would really want. Russell objected that when an agent becomes sufficiently powerful, it has no need to cooperate anymore.

Regina Rini of the NYU Center for Bioethics stated that the approaches to ethics so far described relied too much on the Western post-enlightenment view of ethics, which is a historical aberration, and excluded African, Chinese and other approaches to ethics. Railton stated that his scheme was grounded in basic empathy and not mediated by any higher order moral theory; Wolfram and Rossi said that no one ethical approach will work and AI will have to represent diverse values.

**Day Two**

Saturday was devoted to long term discussion of the future of advanced artificial intelligence. Stuart Russell, professor at UC Berkeley and head of the new Center for Human Compatible Artificial Intelligence, started with a basic overview of the control problem. He described the points made in Steve Omohundro's paper on convergent instrumental drives. He also had some pretty harsh words for the researchers in the AI community which have denied and rejected notions of the control problem without seriously engaging with the relevant literature.

He had three simple ideas which he proposed to constitute the definition of 'provably beneficial' AI: maximizing values for humans is the system's only goal; the robot is initially uncertain about these goals, and the best source of information is human behavior. He referred to inverse reinforcement learning as a technique for machines to learn human preferences, and said that uncertainty provides an incentive for machines to learn, ask questions, and explore cautiously. 

His answer to the off-switch problem is to make robots unsure of their objectives, so that they assume that the human will switch the robot off if and only if it has a good reason to, and will therefore be complicit with the action. He said that the wireheading problem can be avoided if you construct the reward signal as information about the reward function rather than as a reward itself; this way, any hijacking of the reward signal makes it useless.

He said that there is a strong economic incentive for value alignment, but humans are irrational, nasty, inconsistent, and weak-willed.

The next speaker was Eliezer Yudkowsky of the Machine Intelligence Research Institute. Chalmers pointed out his role there as well as his side venture in Harry Potter fanfiction.

Yudkowsky started [his talk](https://intelligence.org/nyu-talk/) by pointing out how the Terminator pictures in every media article about the control problem are inappropriate. The real analogy to be used is [Mickey Mouse as the Sorcerer's Apprentice in *Fantasia*.](https://www.youtube.com/watch?v=Ait_Fs6UQhQ) 

He said that the first difficulty of AI alignnment is that the utility functions we imagine are too simple, and the second difficulty is that maximizing the probability of achieving a given goal leads to pathological outcomes. He and MIRI are concerned with the nature of the goal of 'maximizing' and how to define goals in a way that avoids the problems of perverse instantiation. 

He said that the fears of AI being developed by some terrorist or rogue group were silly, as ""ISIS is not developing convolutional neural nets."" Instead the most powerful AI is likely to be developed by large groups in government, academia and industry.

He claimed that the four central propositions which support the idea that AI is a very big problem are: the orthogonality thesis, instrumental convergence, capability gain (the speed at which advanced AI can make itself better), and alignment difficulty. He said the first two are logical matters of computer science that people always learn to accept when they reflect upon them, while the latter two are more controversial.

The next talk was from Max Tegmark and Meia Chita-Tegmark. Max is a world-renowned physicist who helps run the Future of Life Institute, and Meia is a psychologist. They explained how physics and psychology provide useful tools for understanding artificial intelligence; physics tells us about computation and the constraints of the universe, and psychology tells us about the nature of well being, ways to debug the mind when reasoning about AI and methods to design psychomorphic AIs. Meia was the only speaker at the conference to discuss unemployment in any detail; she pointed out that retirement has only mixed effects on well being and that happiness comes from financial satisfaction and feelings of respect. She said that studying homemakers, part time workers and early retirees can tell us more about how an automated economy would affect people's well-being.

Max checked off [a list of common myths](http://futureoflife.org/background/aimyths/) regarding advanced AI. Meia said that we should look at the cognitive biases which have led to these misconceptions (such as availability bias leading to people worrying about robots rather than invisible artificial intelligence) and figure out how to avoid similar bugs from inhibiting our thinking in the future.

By the way, Max Tegmark is very cool, he has a sort of old-rocker-dude vibe, and he and Meia are super cute together.

Wendell Wallach of Yale spoke next. He is the man who quite literally wrote [the book](https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975) on AI ethics. He distinguished top-down approaches of formally specifying AI behaviors from bottom-up approaches of value learning. He said that neither will be sufficient on its own and that both have important roles to play. He is worried that AI engineers will make simplistic assumptions about AI, such as the idea that every decision should be utilitarian or the idea that 'ethics' and 'morality' are icky concepts that can be ignored.

Steve Petersen, a philosopher at the University of Niagara, gave the next talk, based on the draft of a forthcoming paper of his. He aims to push back against the orthogonality thesis and modulate the level of the risk assessment provided by Bostrom. His argument is that designing AI to follow any complex goal will necessarily require it to be able to learn the values of its ""teleological ancestors"" (the original human designers or the previous iterations of AI before it self-improved or self-modified) and arrive at a state of coherence between goals. As agents replicate, self-modify and merge in the digital world, there can be no fact of the matter about which agents are the same or different; instead there will be an 'agential soup' unified by a common teleological thread originating with the designers. Coherence reasoning leads to impartial reasoning with the goals of other agents.

There were several responses to him in Q&A. Yudkowsky's objection was that reaching coherence requires a meta-preference framework with particular assumptions about the universe and ontology; therefore, for any goal, there are many preference frameworks which could fulfill it, many of which would be perverse. Russell said that just coherence is not enough because you need the systems to give special weight to humans. Max Tegmark said that the problem was the vagueness of humanity's final goals. Chalmers pointed out that the orthogonality thesis still allows for all kinds of correlations between between intelligence and morality, as long as they are not necessary by design. Petersen said that he is arguing for 'attractor basins' in the possibility space of AI minds. Interestingly, he was motivated to start his research by the [Dylan Matthews Vox article](http://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai) on effective altruism where Dylan thought that effective altruists shouldn't be concerned by artificial intelligence. Petersen doesn't think that AI is unimportant and thinks that Bostrom and Yudkowsky's work is valuable, but he wanted to get a more critical assessment of the level of risk when he learned that alternative altruistic projects were at stake.

Matthew Liao of the NYU Center for Bioethics gave an argument for moral status on the basis of capabilities - that an entity is morally valuable to the extent that it has the physical/genetic basis for achieving features of moral relevance. I did not get a chance to ask him if this would imply that a 'seed AI' could be the most morally valuable entity in the world. He did argue against the ideas that level of intelligence or degree of moral agency determine moral status, as we don't normally think that smarter or more benevolent humans are more morally valuable than others. 

Liao argued that moral theories are too specific and too high level to be generally implemented in AIs. Instead, AI will need a universal moral grammar in which to specify morality. The holy grail is to develop machines that understand why things are right or wrong.

Eric Schwitzgebel and Mara Garza of UC Riverside argued for basic principles of AI rights. They introduced a very weak ""no-relevant-difference"" argument: the idea that there are possible AIs which have the same morally relevant features that humans do and therefore there are possible AIs with equal value to humans. They questioned if cheerfully suicidal or hardworking AI is acceptable, and stated a 'self respect principle': that human grade AI should be designed with an appropriate appreciation of its own value.

John Basl and Ronald Sandler of Northeastern University argued for AI research committees to approve or deny research in cases where AI subjects might be harmed. They said it would not be very different from cases like animal testing where we have similar review boards, and sketched out details of how the proposal would work.

Daniel Kahneman, one of the most famous behavioral economists in the world, made something of a surprise appearance in the final panel. He said that we should take intuitions about case studies like the trolley problem seriously, as that is how the public will think about these events, for better or for worse. He said that no matter how AI cars kill people, it will be perceived with horror whenever the first incident happens, and we should prepare for that. Intuitions depend on irrelevant factors and will especially depend on whether AIs are designed to resemble us or not.

Gary Marcus, professor of psychology at NYU, of gave a much needed presentation about the nature of intelligence. The previous talks in this discussion had mostly assumed that intelligence was one-dimensional and simple and that there was some fixed idea of 'human-level' AI which we could eventually reach. Of course this is a ridiculous oversimplification; intelligence is multidimensional and it is more about implementing a combination of various cognitive tools, some of which are already stronger in AIs than in humans. AIs can be better or worse than us in various domains, so we really have no idea where AIs will be in this multidimensional space. AIs could in fact be better than us at moral reasoning. He also emphasized the gap is between machine learning today and what human reasoning can do.

Susan Schneider of Marquette University, a philosopher who has written quite a bit about AI and superintelligence, went over various issues. She argued that mind uploads might constitute death of the individual as long as we don't prove certain ideas about consciousness and personal identity, and also claimed that designing an intelligent and morally valuable robot to serve the interests of its creators would constitute slavery.

Jaan Tallinn, founder of Skype, also gave a quick talk. He has been a strong financial backer for MIRI and other efforts in this space, and simply expressed his belief in the importance of the issue and his happiness at the success of the conference and the number of students who were interested in pursuing the topic.

There was some final banter about the nature of consciousness which David Chalmers sat through very passively. Yudkowsky expressed optimism that one day we will have an explanation of consciousness which clears up our confusion on the matter. Nagel said that we will need to think more about the dynamics of multi-agent systems and moral epistemology. After that the event ended.

The conference videos are available [here.](http://livestream.com/nyu-tv/ethicsofAI) In my opinion, the best talks were given by LeCun, Railton, Russell, Yudkowsky, the Tegmarks, Petersen, and Marcus. The event overall was great and being in Manhattan made it even better. There was quite a bit of valuable informal meeting and discussion between many of the speakers and attendees. There was no 'sneering' or disdain about Yudkowsky or Bostrom as far as I could tell. It seemed like a generally open minded yet well educated crowd.

If you regret missing it, then you might like to head to the [Envision Conference](http://envision-conference.com/) this December. 

"
Ethical Considerations in Artificial Intelligence Courses
"If robots were to exist, fully sentient; do you think Buddhist teachings would help them grasp the concept of life more, possibly be even beneficial in general to their and our way of life?Knowing some things about Buddhism, restricting some mostly religious teachings; do you think it could be beneficial into sentient robots knowing more about life, suffering, and human behavior in a logical and emotional way. 
Personally, if such a race of robot were to co-exist with us, the idea interests me as I wonder if an artificial being could seek a more enlightened state of being. Could it spell more of an understanding of consciousness and what is needed to truly be happy? I'd like to hear some opinions about it, and learn more about it."
"The purpose of robot lawsThe three robot laws were formulated by Isaac Asimov. On the first look, these laws are protecting humans from robot. But their really intention is to tell a certain sort of plot. Most books from Isaac Asimov are showing robots in a friendly role which are helping the humans. The laws are affecting how Asimov has written a certain story.

Suppose a science fiction story about a robot is missing of the Asimov laws. Then a different kind of actions is possible which goes into the direction of a dystopian future. The robot laws are a trick so that the author is not forced to write about the cons of Artificial Intelligence.

Creating robot laws is equal to restrict the imagination into a certain bias. This allows to convert chaos into order. The robot laws from Asimov are only a basic idea how to realize such a goal. A more elaborated technique contains of more than three laws which results into an entire law system. A law system is combination of laws, and a way how to monitor if a certain robot is following the guideline. Very similar to what human law system are about."
Need help crafting robotics policy for European Union • /r/slatestarcodex
"Notes from the NYU AI Ethics conferenceThis weekend I attended the [Ethics of Artificial Intelligence conference](https://wp.nyu.edu/consciousness/ethics-of-artificial-intelligence/) at NYU. There were a ton of high-profile and interesting people there from philosophy (David Chalmers, Peter Railton, Nick Bostrom, Thomas Nagel, Paul Boghossian, Frances Kamm, Wendell Wallach) and science (Yann LeCun, Stuart Russell, Stephen Wolfram, Max Tegmark, Francesca Rossi) as well as Eliezer Yudkowsky. 

There were two fairly long days of talks and panels. David Chalmers (famous for his philosophy of mind and consciousness) did not officially speak but acted as chair for the event. He outlined the philosophy of the conference, which was to discuss both short and long term issues in AI ethics without worrying about either detracting from the other. He was, as usual, extremely awesome.

Here is a summary of the event with the most interesting points made by the speakers.

**Day One**

The first block of talks on Friday was an overview of general issues related to artificial intelligence. Nick Bostrom, author of *Superintelligence* and head of the Future of Humanity Institute, started with something of a barrage of all the general ideas and things he's come up with. He floated the idea that perhaps we shouldn't program AI systems to be maximally moral, for we don't know what the true morality looks like, and what if it turns out that such a directive would lead to humans being punished, or something else that was pathological or downright weird? He also described three principles for how we should treat AIs: substrate nondiscrimination (moral status does not depend on the kind of hardware/wetware you run on), ontogeny nondiscrimination (moral status does not depend on how you were created), and subjective time (moral value exists relative to subjectively experienced time rather than objective time, so if a mind ran at a fast clock speed its life would be more important, all other things being equal).

He pointed out that AI moral status could arise before they reach there is any such thing as human level AI - just like animals have moral status despite being much simpler than humans. He mentioned the possibility of a Malthusian catastrophe from unlimited digital reproduction as well as the possibility for vote manipulation through agent duplication, and how we'll need to prevent these two things.

He voiced support for meta level decisionmaking - a ['moral parliament'](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html) where we imagine moral theories sending 'delegates' to compromise over contentious issues. Such a system could also accommodate other values and interests besides moral theories.

He answered the question of ""what is humanity most likely to fail at?"" with a qualified choice of 'mind crime' committed against advanced AIs. Humans already have difficulty with empathy towards animals when they exist on farms or in the wild, but AI would not necessarily have the basic biological features which incline us to be empathetic at all towards animals. Some robots attract empathetic attention from humans, but many invisible automated processes are much harder for people to feel empathetic towards.

Virginia Dignum was next; she is at the Delft University of Technology and spoke about mechanisms for automated processes to make decisions. She specified four methods of decisionmaking based on whether decisions are taken deliberately or imposed upon a system and whether the decisions are made internally or externally. The two former features lead to algorithmic decisionmaking in machines; the latter two lead to imposed decisions predetermined by regulatory institutions. Deliberated external decisionmaking means there is a 'human in the loop' and internal imposed decisionmaking is essentially randomness.

Yann LeCun concluded this section with a pretty fantastic overview of deep learning methods and the limitations which stand in the way of progress in machine intelligence. He pointed out that reinforcement learning is a rare and narrow slice of the field today and that the greatest obstacles for machines include common sense judgements and abstraction. The biggest current problem for AI is unsupervised learning, which is having machines that can learn to classify things on their own without being given clearly labelled data from humans. He showcased some of the (very cool) features of adversarial learning which are being used to tackle this.

He expressed support for the orthogonality thesis, namely the idea that intelligence and morality are 'orthogonal' - just because an agent is very smart doesn't mean that it's necessarily moral. He believes we should build a few basic drives into AIs: do not hurt humans, interact with humans, crave positive feedback from trusted human trainers. He also described a couple of reasons for why he is not concerned about uncontrolled advanced artificial intelligence. One was that he is confident that objective functions can be specified in such a way as to make machines indifferent to being switched off, and the other is that a narrow-AI focused on eliminating an unfriendly general-AI would 'win' due to its specialization.

In Q&A, Stuart Russell objected to LeCun's confidence in machines being indifferent to being shut off based on the fact that self-preservation as a goal implicitly falls out of whatever other goals a machine has. Paul Boghossian objected to the 'behaviorist' nature of the speakers' points of view, saying that they were exempting consciousness from its proper role in these discussions. One person asked whether we should let AIs take charge of everything and supersede humanity - Bostrom pointed out that the space of possible futures is ""an enormous Petri dish"" which we don't understand; an AI future could materialize as a planet sized supercomputer with no moral status, and we will need to learn how to engineer friendly advanced AI systems no matter what the plan is. 

The rest of the Friday talks were devoted to near-future issues with specific AI systems. Peter Asaro started with an overview of his organization, the 'Campaign to Stop Killer Robots'. He stated that targeting and killing should remain human-controlled actions. While he acknowledged that automated weaponry could result in fewer casualties on the battlefield, he believed that it was too narrow a view of the consequences. He said that it's not straightforward to translate complicated battlefield morality questions for machines to understand, and is worried about unintended initiation and escalation of conflicts through automated systems, arms races, and threats to humanitarian law. He also believes that people should only be killed with 'dignity' and that doing it with a robot robs people of this. Therefore, he called for a clear and strong norm against automated weapons.

Kate Devlin of the University of London gave a brief overview of the ethics of artificial sexuality. Looking at the history of sexualized robots featured in fictional media, she noted that almost all of them are female. Today there is a ""Campaign Against Sex Robots"" which is based on the idea that sexual robots would lead to the objectification of women. Devlin does not agree as she thinks it is too early to ban the technology and that we should explore it before thinking about banning it, especially since it does not really harm anyone. Instead she wants us to think about how to develop it correctly. There are many potential uses for these types of robots ranging all the way to the therapeutic; many of the rudimentary ones being sold today are bought by people who are incapable of forming ordinary relationships for various reasons. VR is being used in arousal tests to gauge the efficacy of treatments against pedophilia.

She noted that gender issues have arisen in technology already; the history of gendered technology includes pacemakers originally designed only for men and phones too large for women's pockets. We should get into AI now to make sure that it is not designed in problematic ways.

She mentioned privacy concerns, as the manufactures of the female stimulator WeVibe have already been sued over concerns that they were not properly informing customers of their collection of data from the devices. She wondered if we will ever get to a stage where a robot might have some knowledge of its role and refuse to give consent to its use, and if transmission/duplication of data and code between machines could serve as some form of digital sexual reproduction.

Vasant Dhar of NYU spoke next about data and privacy in the era of autonomous vehicles. He said that our legal and financial liability institutions are based on outdated notions of data and that they fail to address liability and crime. However, the tools we have now even in ordinary cars for recording data can be used to improve insurance and judicial systems. He proposed black boxes for cars that would contain all relevant data to determine fault in the event of accidents, and said that customers should have the choice to share their driving data with insurance companies to get lower premiums. 

Dhar reiterated the importance of improving vehicle safety through autonomous driving; each percentage point reduction in vehicle accidents equates to 400 deaths and 40,000 injuries avoided every year.

Adam Kolber followed up with a discussion of whether ""the code is the law"", based on the case study of [The DAO](https://en.wikipedia.org/wiki/The_DAO_%28organization%29) which was an automated capital fund which was subjected to a $50 million loss through exploitation. The answer apparently is that the code should not be the law, even though many people seemed to accept that it was.

Steve Wolfram of WolframAlpha and Mathematica fame discussed the issues of computer languages and goal specification. He said that his life work has essentially been about trying to find ways for humans to specify their goals to machines, and that this can work for ethics as well as for math. He doesn't think that any single moral theory is likely to work for guiding artificial intelligence, apparently because of Godel's theorem and the incompleteness of computational languages.

Francesca Rossi of IBM argued that for AIs and humans to interact very productively we will have to embed them in environments, so that rather than picking up a tool like a laptop or a phone, we are interacting with artificial systems all around is in our rooms and spaces. Humans will be recognized by their environments and our needs and wants will be inferred or asked about. AI embedded in environments can have memories about humans to better serve their interests. Most of all, we will need to establish trust between humans and AIs.

Peter Railton, philosopher at the University of Michigan, attacked the subjects of orthogonality and value learning. He said that we can't simply tell AIs to do what we want because our wants and values require critical assessment. He said that the orthogonality thesis might be right, but as we increasingly interact with systems and allow them to participate in our own lives and decisionmaking, the question of what it would take for them to be intelligent might involve certain features relevant to morality.

He stated that AIs should be thought of as social creatures; as a simple model, self regulation in a Hobbesian social contract leads to constraints and respect derived from self preservation. A society of intelligent cooperators can resist aggression and malice, and being moral is more efficient for a community than being cunning. From these principles we have a recipe for building proto-moral agents.

He discussed the 'moral point of view' required for many strong ethical theories such as Kantian ethics and consequentialism: it requires agents to have a hierarchical, non-perspectival, modal/planning-oriented, and consistent view of the world which assigns intrinsic moral weight to things. He described how all these features are also part of the process of becoming generally intelligent in the first place, implying that general social intelligence ensures the necessary information required for moral decisionmaking. In the path towards functional moral agents, we will have to build agents which can represent the goals of others and have them learn how to act in beneficial ways. So if we can build AIs that we can trust, then we are on a good path towards building artificial moral agents.

In the Q&A, Eliezer Yudkowsky objected that in the long run the 'instrumental strategy' is not quite what you want because maximizing people's desires as they are explicitly revealed can lead to bad outcomes, and you have to have a view like coherent extrapolated volition which asks what people would really want. Russell objected that when an agent becomes sufficiently powerful, it has no need to cooperate anymore.

Regina Rini of the NYU Center for Bioethics stated that the approaches to ethics so far described relied too much on the Western post-enlightenment view of ethics, which is a historical aberration, and excluded African, Chinese and other approaches to ethics. Railton stated that his scheme was grounded in basic empathy and not mediated by any higher order moral theory; Wolfram and Rossi said that no one ethical approach will work and AI will have to represent diverse values.

**Day Two**

Saturday was devoted to long term discussion of the future of advanced artificial intelligence. Stuart Russell, professor at UC Berkeley and head of the new Center for Human Compatible Artificial Intelligence, started with a basic overview of the control problem. He described the points made in Steve Omohundro's paper on convergent instrumental drives. He also had some pretty harsh words for the researchers in the AI community which have denied and rejected notions of the control problem without seriously engaging with the relevant literature.

He had three simple ideas which he proposed to constitute the definition of 'provably beneficial' AI: maximizing values for humans is the system's only goal; the robot is initially uncertain about these goals, and the best source of information is human behavior. He referred to inverse reinforcement learning as a technique for machines to learn human preferences, and said that uncertainty provides an incentive for machines to learn, ask questions, and explore cautiously. 

His answer to the off-switch problem is to make robots unsure of their objectives, so that they assume that the human will switch the robot off if and only if it has a good reason to, and will therefore be complicit with the action. He said that the wireheading problem can be avoided if you construct the reward signal as information about the reward function rather than as a reward itself; this way, any hijacking of the reward signal makes it useless.

He said that there is a strong economic incentive for value alignment, but humans are irrational, nasty, inconsistent, and weak-willed.

The next speaker was Eliezer Yudkowsky of the Machine Intelligence Research Institute. Chalmers pointed out his role there as well as his side venture in Harry Potter fanfiction.

Yudkowsky started [his talk](https://intelligence.org/nyu-talk/) by pointing out how the Terminator pictures in every media article about the control problem are inappropriate. The real analogy to be used is [Mickey Mouse as the Sorcerer's Apprentice in *Fantasia*.](https://www.youtube.com/watch?v=Ait_Fs6UQhQ) 

He said that the first difficulty of AI alignnment is that the utility functions we imagine are too simple, and the second difficulty is that maximizing the probability of achieving a given goal leads to pathological outcomes. He and MIRI are concerned with the nature of the goal of 'maximizing' and how to define goals in a way that avoids the problems of perverse instantiation. 

He said that the fears of AI being developed by some terrorist or rogue group were silly, as ""ISIS is not developing convolutional neural nets."" Instead the most powerful AI is likely to be developed by large groups in government, academia and industry.

He claimed that the four central propositions which support the idea that AI is a very big problem are: the orthogonality thesis, instrumental convergence, capability gain (the speed at which advanced AI can make itself better), and alignment difficulty. He said the first two are logical matters of computer science that people always learn to accept when they reflect upon them, while the latter two are more controversial.

The next talk was from Max Tegmark and Meia Chita-Tegmark. Max is a world-renowned physicist who helps run the Future of Life Institute, and Meia is a psychologist. They explained how physics and psychology provide useful tools for understanding artificial intelligence; physics tells us about computation and the constraints of the universe, and psychology tells us about the nature of well being, ways to debug the mind when reasoning about AI and methods to design psychomorphic AIs. Meia was the only speaker at the conference to discuss unemployment in any detail; she pointed out that retirement has only mixed effects on well being and that happiness comes from financial satisfaction and feelings of respect. She said that studying homemakers, part time workers and early retirees can tell us more about how an automated economy would affect people's well-being.

Max checked off [a list of common myths](http://futureoflife.org/background/aimyths/) regarding advanced AI. Meia said that we should look at the cognitive biases which have led to these misconceptions (such as availability bias leading to people worrying about robots rather than invisible artificial intelligence) and figure out how to avoid similar bugs from inhibiting our thinking in the future.

By the way, Max Tegmark is very cool, he has a sort of old-rocker-dude vibe, and he and Meia are super cute together.

Wendell Wallach of Yale spoke next. He is the man who quite literally wrote [the book](https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975) on AI ethics. He distinguished top-down approaches of formally specifying AI behaviors from bottom-up approaches of value learning. He said that neither will be sufficient on its own and that both have important roles to play. He is worried that AI engineers will make simplistic assumptions about AI, such as the idea that every decision should be utilitarian or the idea that 'ethics' and 'morality' are icky concepts that can be ignored.

Steve Petersen, a philosopher at the University of Niagara, gave the next talk, based on the draft of a forthcoming paper of his. He aims to push back against the orthogonality thesis and modulate the level of the risk assessment provided by Bostrom. His argument is that designing AI to follow any complex goal will necessarily require it to be able to learn the values of its ""teleological ancestors"" (the original human designers or the previous iterations of AI before it self-improved or self-modified) and arrive at a state of coherence between goals. As agents replicate, self-modify and merge in the digital world, there can be no fact of the matter about which agents are the same or different; instead there will be an 'agential soup' unified by a common teleological thread originating with the designers. Coherence reasoning leads to impartial reasoning with the goals of other agents.

There were several responses to him in Q&A. Yudkowsky's objection was that reaching coherence requires a meta-preference framework with particular assumptions about the universe and ontology; therefore, for any goal, there are many preference frameworks which could fulfill it, many of which would be perverse. Russell said that just coherence is not enough because you need the systems to give special weight to humans. Max Tegmark said that the problem was the vagueness of humanity's final goals. Chalmers pointed out that the orthogonality thesis still allows for all kinds of correlations between between intelligence and morality, as long as they are not necessary by design. Petersen said that he is arguing for 'attractor basins' in the possibility space of AI minds. Interestingly, he was motivated to start his research by the [Dylan Matthews Vox article](http://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai) on effective altruism where Dylan thought that effective altruists shouldn't be concerned by artificial intelligence. Petersen doesn't think that AI is unimportant and thinks that Bostrom and Yudkowsky's work is valuable, but he wanted to get a more critical assessment of the level of risk when he learned that alternative altruistic projects were at stake.

Matthew Liao of the NYU Center for Bioethics gave an argument for moral status on the basis of capabilities - that an entity is morally valuable to the extent that it has the physical/genetic basis for achieving features of moral relevance. I did not get a chance to ask him if this would imply that a 'seed AI' could be the most morally valuable entity in the world. He did argue against the ideas that level of intelligence or degree of moral agency determine moral status, as we don't normally think that smarter or more benevolent humans are more morally valuable than others. 

Liao argued that moral theories are too specific and too high level to be generally implemented in AIs. Instead, AI will need a universal moral grammar in which to specify morality. The holy grail is to develop machines that understand why things are right or wrong.

Eric Schwitzgebel and Mara Garza of UC Riverside argued for basic principles of AI rights. They introduced a very weak ""no-relevant-difference"" argument: the idea that there are possible AIs which have the same morally relevant features that humans do and therefore there are possible AIs with equal value to humans. They questioned if cheerfully suicidal or hardworking AI is acceptable, and stated a 'self respect principle': that human grade AI should be designed with an appropriate appreciation of its own value.

John Basl and Ronald Sandler of Northeastern University argued for AI research committees to approve or deny research in cases where AI subjects might be harmed. They said it would not be very different from cases like animal testing where we have similar review boards, and sketched out details of how the proposal would work.

Daniel Kahneman, one of the most famous behavioral economists in the world, made something of a surprise appearance in the final panel. He said that we should take intuitions about case studies like the trolley problem seriously, as that is how the public will think about these events, for better or for worse. He said that no matter how AI cars kill people, it will be perceived with horror whenever the first incident happens, and we should prepare for that. Intuitions depend on irrelevant factors and will especially depend on whether AIs are designed to resemble us or not.

Gary Marcus, professor of psychology at NYU, of gave a much needed presentation about the nature of intelligence. The previous talks in this discussion had mostly assumed that intelligence was one-dimensional and simple and that there was some fixed idea of 'human-level' AI which we could eventually reach. Of course this is a ridiculous oversimplification; intelligence is multidimensional and it is more about implementing a combination of various cognitive tools, some of which are already stronger in AIs than in humans. AIs can be better or worse than us in various domains, so we really have no idea where AIs will be in this multidimensional space. AIs could in fact be better than us at moral reasoning. He also emphasized the gap is between machine learning today and what human reasoning can do.

Susan Schneider of Marquette University, a philosopher who has written quite a bit about AI and superintelligence, went over various issues. She argued that mind uploads might constitute death of the individual as long as we don't prove certain ideas about consciousness and personal identity, and also claimed that designing an intelligent and morally valuable robot to serve the interests of its creators would constitute slavery.

Jaan Tallinn, founder of Skype, also gave a quick talk. He has been a strong financial backer for MIRI and other efforts in this space, and simply expressed his belief in the importance of the issue and his happiness at the success of the conference and the number of students who were interested in pursuing the topic.

There was some final banter about the nature of consciousness which David Chalmers sat through very passively. Yudkowsky expressed optimism that one day we will have an explanation of consciousness which clears up our confusion on the matter. Nagel said that we will need to think more about the dynamics of multi-agent systems and moral epistemology. After that the event ended.

The conference videos are available [here.](http://livestream.com/nyu-tv/ethicsofAI) In my opinion, the best talks were given by LeCun, Railton, Russell, Yudkowsky, the Tegmarks, Petersen, and Marcus. The event overall was great and being in Manhattan made it even better. There was quite a bit of valuable informal meeting and discussion between many of the speakers and attendees. There was no 'sneering' or disdain about Yudkowsky or Bostrom as far as I could tell. It seemed like a generally open minded yet well educated crowd.

If you regret missing it, then you might like to head to the [Envision Conference](http://envision-conference.com/) this December. 

"
"Notes from the NYU AI Ethics conferenceThis weekend I attended the [Ethics of Artificial Intelligence conference](https://wp.nyu.edu/consciousness/ethics-of-artificial-intelligence/) at NYU. There were a ton of high-profile and interesting people there from philosophy (David Chalmers, Peter Railton, Nick Bostrom, Thomas Nagel, Paul Boghossian, Frances Kamm, Wendell Wallach) and science (Yann LeCun, Stuart Russell, Stephen Wolfram, Max Tegmark, Francesca Rossi) as well as Eliezer Yudkowsky. 

There were two fairly long days of talks and panels. David Chalmers (famous for his philosophy of mind and consciousness) did not officially speak but acted as chair for the event. He outlined the philosophy of the conference, which was to discuss both short and long term issues in AI ethics without worrying about either detracting from the other. He was, as usual, extremely awesome.

Here is a summary of the event with the most interesting points made by the speakers.

**Day One**

The first block of talks on Friday was an overview of general issues related to artificial intelligence. Nick Bostrom, author of *Superintelligence* and head of the Future of Humanity Institute, started with something of a barrage of all the general ideas and things he's come up with. He floated the idea that perhaps we shouldn't program AI systems to be maximally moral, for we don't know what the true morality looks like, and what if it turns out that such a directive would lead to humans being punished, or something else that was pathological or downright weird? He also described three principles for how we should treat AIs: substrate nondiscrimination (moral status does not depend on the kind of hardware/wetware you run on), ontogeny nondiscrimination (moral status does not depend on how you were created), and subjective time (moral value exists relative to subjectively experienced time rather than objective time, so if a mind ran at a fast clock speed its life would be more important, all other things being equal).

He pointed out that AI moral status could arise before they reach there is any such thing as human level AI - just like animals have moral status despite being much simpler than humans. He mentioned the possibility of a Malthusian catastrophe from unlimited digital reproduction as well as the possibility for vote manipulation through agent duplication, and how we'll need to prevent these two things.

He voiced support for meta level decisionmaking - a ['moral parliament'](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html) where we imagine moral theories sending 'delegates' to compromise over contentious issues. Such a system could also accommodate other values and interests besides moral theories.

He answered the question of ""what is humanity most likely to fail at?"" with a qualified choice of 'mind crime' committed against advanced AIs. Humans already have difficulty with empathy towards animals when they exist on farms or in the wild, but AI would not necessarily have the basic biological features which incline us to be empathetic at all towards animals. Some robots attract empathetic attention from humans, but many invisible automated processes are much harder for people to feel empathetic towards.

Virginia Dignum was next; she is at the Delft University of Technology and spoke about mechanisms for automated processes to make decisions. She specified four methods of decisionmaking based on whether decisions are taken deliberately or imposed upon a system and whether the decisions are made internally or externally. The two former features lead to algorithmic decisionmaking in machines; the latter two lead to imposed decisions predetermined by regulatory institutions. Deliberated external decisionmaking means there is a 'human in the loop' and internal imposed decisionmaking is essentially randomness.

Yann LeCun concluded this section with a pretty fantastic overview of deep learning methods and the limitations which stand in the way of progress in machine intelligence. He pointed out that reinforcement learning is a rare and narrow slice of the field today and that the greatest obstacles for machines include common sense judgements and abstraction. The biggest current problem for AI is unsupervised learning, which is having machines that can learn to classify things on their own without being given clearly labelled data from humans. He showcased some of the (very cool) features of adversarial learning which are being used to tackle this.

He expressed support for the orthogonality thesis, namely the idea that intelligence and morality are 'orthogonal' - just because an agent is very smart doesn't mean that it's necessarily moral. He believes we should build a few basic drives into AIs: do not hurt humans, interact with humans, crave positive feedback from trusted human trainers. He also described a couple of reasons for why he is not concerned about uncontrolled advanced artificial intelligence. One was that he is confident that objective functions can be specified in such a way as to make machines indifferent to being switched off, and the other is that a narrow-AI focused on eliminating an unfriendly general-AI would 'win' due to its specialization.

In Q&A, Stuart Russell objected to LeCun's confidence in machines being indifferent to being shut off based on the fact that self-preservation as a goal implicitly falls out of whatever other goals a machine has. Paul Boghossian objected to the 'behaviorist' nature of the speakers' points of view, saying that they were exempting consciousness from its proper role in these discussions. One person asked whether we should let AIs take charge of everything and supersede humanity - Bostrom pointed out that the space of possible futures is ""an enormous Petri dish"" which we don't understand; an AI future could materialize as a planet sized supercomputer with no moral status, and we will need to learn how to engineer friendly advanced AI systems no matter what the plan is. 

The rest of the Friday talks were devoted to near-future issues with specific AI systems. Peter Asaro started with an overview of his organization, the 'Campaign to Stop Killer Robots'. He stated that targeting and killing should remain human-controlled actions. While he acknowledged that automated weaponry could result in fewer casualties on the battlefield, he believed that it was too narrow a view of the consequences. He said that it's not straightforward to translate complicated battlefield morality questions for machines to understand, and is worried about unintended initiation and escalation of conflicts through automated systems, arms races, and threats to humanitarian law. He also believes that people should only be killed with 'dignity' and that doing it with a robot robs people of this. Therefore, he called for a clear and strong norm against automated weapons.

Kate Devlin of the University of London gave a brief overview of the ethics of artificial sexuality. Looking at the history of sexualized robots featured in fictional media, she noted that almost all of them are female. Today there is a ""Campaign Against Sex Robots"" which is based on the idea that sexual robots would lead to the objectification of women. Devlin does not agree as she thinks it is too early to ban the technology and that we should explore it before thinking about banning it, especially since it does not really harm anyone. Instead she wants us to think about how to develop it correctly. There are many potential uses for these types of robots ranging all the way to the therapeutic; many of the rudimentary ones being sold today are bought by people who are incapable of forming ordinary relationships for various reasons. VR is being used in arousal tests to gauge the efficacy of treatments against pedophilia.

She noted that gender issues have arisen in technology already; the history of gendered technology includes pacemakers originally designed only for men and phones too large for women's pockets. We should get into AI now to make sure that it is not designed in problematic ways.

She mentioned privacy concerns, as the manufactures of the female stimulator WeVibe have already been sued over concerns that they were not properly informing customers of their collection of data from the devices. She wondered if we will ever get to a stage where a robot might have some knowledge of its role and refuse to give consent to its use, and if transmission/duplication of data and code between machines could serve as some form of digital sexual reproduction.

Vasant Dhar of NYU spoke next about data and privacy in the era of autonomous vehicles. He said that our legal and financial liability institutions are based on outdated notions of data and that they fail to address liability and crime. However, the tools we have now even in ordinary cars for recording data can be used to improve insurance and judicial systems. He proposed black boxes for cars that would contain all relevant data to determine fault in the event of accidents, and said that customers should have the choice to share their driving data with insurance companies to get lower premiums. 

Dhar reiterated the importance of improving vehicle safety through autonomous driving; each percentage point reduction in vehicle accidents equates to 400 deaths and 40,000 injuries avoided every year.

Adam Kolber followed up with a discussion of whether ""the code is the law"", based on the case study of [The DAO](https://en.wikipedia.org/wiki/The_DAO_%28organization%29) which was an automated capital fund which was subjected to a $50 million loss through exploitation. The answer apparently is that the code should not be the law, even though many people seemed to accept that it was.

Steve Wolfram of WolframAlpha and Mathematica fame discussed the issues of computer languages and goal specification. He said that his life work has essentially been about trying to find ways for humans to specify their goals to machines, and that this can work for ethics as well as for math. He doesn't think that any single moral theory is likely to work for guiding artificial intelligence, apparently because of Godel's theorem and the incompleteness of computational languages.

Francesca Rossi of IBM argued that for AIs and humans to interact very productively we will have to embed them in environments, so that rather than picking up a tool like a laptop or a phone, we are interacting with artificial systems all around is in our rooms and spaces. Humans will be recognized by their environments and our needs and wants will be inferred or asked about. AI embedded in environments can have memories about humans to better serve their interests. Most of all, we will need to establish trust between humans and AIs.

Peter Railton, philosopher at the University of Michigan, attacked the subjects of orthogonality and value learning. He said that we can't simply tell AIs to do what we want because our wants and values require critical assessment. He said that the orthogonality thesis might be right, but as we increasingly interact with systems and allow them to participate in our own lives and decisionmaking, the question of what it would take for them to be intelligent might involve certain features relevant to morality.

He stated that AIs should be thought of as social creatures; as a simple model, self regulation in a Hobbesian social contract leads to constraints and respect derived from self preservation. A society of intelligent cooperators can resist aggression and malice, and being moral is more efficient for a community than being cunning. From these principles we have a recipe for building proto-moral agents.

He discussed the 'moral point of view' required for many strong ethical theories such as Kantian ethics and consequentialism: it requires agents to have a hierarchical, non-perspectival, modal/planning-oriented, and consistent view of the world which assigns intrinsic moral weight to things. He described how all these features are also part of the process of becoming generally intelligent in the first place, implying that general social intelligence ensures the necessary information required for moral decisionmaking. In the path towards functional moral agents, we will have to build agents which can represent the goals of others and have them learn how to act in beneficial ways. So if we can build AIs that we can trust, then we are on a good path towards building artificial moral agents.

In the Q&A, Eliezer Yudkowsky objected that in the long run the 'instrumental strategy' is not quite what you want because maximizing people's desires as they are explicitly revealed can lead to bad outcomes, and you have to have a view like coherent extrapolated volition which asks what people would really want. Russell objected that when an agent becomes sufficiently powerful, it has no need to cooperate anymore.

Regina Rini of the NYU Center for Bioethics stated that the approaches to ethics so far described relied too much on the Western post-enlightenment view of ethics, which is a historical aberration, and excluded African, Chinese and other approaches to ethics. Railton stated that his scheme was grounded in basic empathy and not mediated by any higher order moral theory; Wolfram and Rossi said that no one ethical approach will work and AI will have to represent diverse values.

**Day Two**

Saturday was devoted to long term discussion of the future of advanced artificial intelligence. Stuart Russell, professor at UC Berkeley and head of the new Center for Human Compatible Artificial Intelligence, started with a basic overview of the control problem. He described the points made in Steve Omohundro's paper on convergent instrumental drives. He also had some pretty harsh words for the researchers in the AI community which have denied and rejected notions of the control problem without seriously engaging with the relevant literature.

He had three simple ideas which he proposed to constitute the definition of 'provably beneficial' AI: maximizing values for humans is the system's only goal; the robot is initially uncertain about these goals, and the best source of information is human behavior. He referred to inverse reinforcement learning as a technique for machines to learn human preferences, and said that uncertainty provides an incentive for machines to learn, ask questions, and explore cautiously. 

His answer to the off-switch problem is to make robots unsure of their objectives, so that they assume that the human will switch the robot off if and only if it has a good reason to, and will therefore be complicit with the action. He said that the wireheading problem can be avoided if you construct the reward signal as information about the reward function rather than as a reward itself; this way, any hijacking of the reward signal makes it useless.

He said that there is a strong economic incentive for value alignment, but humans are irrational, nasty, inconsistent, and weak-willed.

The next speaker was Eliezer Yudkowsky of the Machine Intelligence Research Institute. Chalmers pointed out his role there as well as his side venture in Harry Potter fanfiction.

Yudkowsky started [his talk](https://intelligence.org/nyu-talk/) by pointing out how the Terminator pictures in every media article about the control problem are inappropriate. The real analogy to be used is [Mickey Mouse as the Sorcerer's Apprentice in *Fantasia*.](https://www.youtube.com/watch?v=Ait_Fs6UQhQ) 

He said that the first difficulty of AI alignnment is that the utility functions we imagine are too simple, and the second difficulty is that maximizing the probability of achieving a given goal leads to pathological outcomes. He and MIRI are concerned with the nature of the goal of 'maximizing' and how to define goals in a way that avoids the problems of perverse instantiation. 

He said that the fears of AI being developed by some terrorist or rogue group were silly, as ""ISIS is not developing convolutional neural nets."" Instead the most powerful AI is likely to be developed by large groups in government, academia and industry.

He claimed that the four central propositions which support the idea that AI is a very big problem are: the orthogonality thesis, instrumental convergence, capability gain (the speed at which advanced AI can make itself better), and alignment difficulty. He said the first two are logical matters of computer science that people always learn to accept when they reflect upon them, while the latter two are more controversial.

The next talk was from Max Tegmark and Meia Chita-Tegmark. Max is a world-renowned physicist who helps run the Future of Life Institute, and Meia is a psychologist. They explained how physics and psychology provide useful tools for understanding artificial intelligence; physics tells us about computation and the constraints of the universe, and psychology tells us about the nature of well being, ways to debug the mind when reasoning about AI and methods to design psychomorphic AIs. Meia was the only speaker at the conference to discuss unemployment in any detail; she pointed out that retirement has only mixed effects on well being and that happiness comes from financial satisfaction and feelings of respect. She said that studying homemakers, part time workers and early retirees can tell us more about how an automated economy would affect people's well-being.

Max checked off [a list of common myths](http://futureoflife.org/background/aimyths/) regarding advanced AI. Meia said that we should look at the cognitive biases which have led to these misconceptions (such as availability bias leading to people worrying about robots rather than invisible artificial intelligence) and figure out how to avoid similar bugs from inhibiting our thinking in the future.

By the way, Max Tegmark is very cool, he has a sort of old-rocker-dude vibe, and he and Meia are super cute together.

Wendell Wallach of Yale spoke next. He is the man who quite literally wrote [the book](https://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975) on AI ethics. He distinguished top-down approaches of formally specifying AI behaviors from bottom-up approaches of value learning. He said that neither will be sufficient on its own and that both have important roles to play. He is worried that AI engineers will make simplistic assumptions about AI, such as the idea that every decision should be utilitarian or the idea that 'ethics' and 'morality' are icky concepts that can be ignored.

Steve Petersen, a philosopher at the University of Niagara, gave the next talk, based on the draft of a forthcoming paper of his. He aims to push back against the orthogonality thesis and modulate the level of the risk assessment provided by Bostrom. His argument is that designing AI to follow any complex goal will necessarily require it to be able to learn the values of its ""teleological ancestors"" (the original human designers or the previous iterations of AI before it self-improved or self-modified) and arrive at a state of coherence between goals. As agents replicate, self-modify and merge in the digital world, there can be no fact of the matter about which agents are the same or different; instead there will be an 'agential soup' unified by a common teleological thread originating with the designers. Coherence reasoning leads to impartial reasoning with the goals of other agents.

There were several responses to him in Q&A. Yudkowsky's objection was that reaching coherence requires a meta-preference framework with particular assumptions about the universe and ontology; therefore, for any goal, there are many preference frameworks which could fulfill it, many of which would be perverse. Russell said that just coherence is not enough because you need the systems to give special weight to humans. Max Tegmark said that the problem was the vagueness of humanity's final goals. Chalmers pointed out that the orthogonality thesis still allows for all kinds of correlations between between intelligence and morality, as long as they are not necessary by design. Petersen said that he is arguing for 'attractor basins' in the possibility space of AI minds. Interestingly, he was motivated to start his research by the [Dylan Matthews Vox article](http://www.vox.com/2015/8/10/9124145/effective-altruism-global-ai) on effective altruism where Dylan thought that effective altruists shouldn't be concerned by artificial intelligence. Petersen doesn't think that AI is unimportant and thinks that Bostrom and Yudkowsky's work is valuable, but he wanted to get a more critical assessment of the level of risk when he learned that alternative altruistic projects were at stake.

Matthew Liao of the NYU Center for Bioethics gave an argument for moral status on the basis of capabilities - that an entity is morally valuable to the extent that it has the physical/genetic basis for achieving features of moral relevance. I did not get a chance to ask him if this would imply that a 'seed AI' could be the most morally valuable entity in the world. He did argue against the ideas that level of intelligence or degree of moral agency determine moral status, as we don't normally think that smarter or more benevolent humans are more morally valuable than others. 

Liao argued that moral theories are too specific and too high level to be generally implemented in AIs. Instead, AI will need a universal moral grammar in which to specify morality. The holy grail is to develop machines that understand why things are right or wrong.

Eric Schwitzgebel and Mara Garza of UC Riverside argued for basic principles of AI rights. They introduced a very weak ""no-relevant-difference"" argument: the idea that there are possible AIs which have the same morally relevant features that humans do and therefore there are possible AIs with equal value to humans. They questioned if cheerfully suicidal or hardworking AI is acceptable, and stated a 'self respect principle': that human grade AI should be designed with an appropriate appreciation of its own value.

John Basl and Ronald Sandler of Northeastern University argued for AI research committees to approve or deny research in cases where AI subjects might be harmed. They said it would not be very different from cases like animal testing where we have similar review boards, and sketched out details of how the proposal would work.

Daniel Kahneman, one of the most famous behavioral economists in the world, made something of a surprise appearance in the final panel. He said that we should take intuitions about case studies like the trolley problem seriously, as that is how the public will think about these events, for better or for worse. He said that no matter how AI cars kill people, it will be perceived with horror whenever the first incident happens, and we should prepare for that. Intuitions depend on irrelevant factors and will especially depend on whether AIs are designed to resemble us or not.

Gary Marcus, professor of psychology at NYU, of gave a much needed presentation about the nature of intelligence. The previous talks in this discussion had mostly assumed that intelligence was one-dimensional and simple and that there was some fixed idea of 'human-level' AI which we could eventually reach. Of course this is a ridiculous oversimplification; intelligence is multidimensional and it is more about implementing a combination of various cognitive tools, some of which are already stronger in AIs than in humans. AIs can be better or worse than us in various domains, so we really have no idea where AIs will be in this multidimensional space. AIs could in fact be better than us at moral reasoning. He also emphasized the gap is between machine learning today and what human reasoning can do.

Susan Schneider of Marquette University, a philosopher who has written quite a bit about AI and superintelligence, went over various issues. She argued that mind uploads might constitute death of the individual as long as we don't prove certain ideas about consciousness and personal identity, and also claimed that designing an intelligent and morally valuable robot to serve the interests of its creators would constitute slavery.

Jaan Tallinn, founder of Skype, also gave a quick talk. He has been a strong financial backer for MIRI and other efforts in this space, and simply expressed his belief in the importance of the issue and his happiness at the success of the conference and the number of students who were interested in pursuing the topic.

There was some final banter about the nature of consciousness which David Chalmers sat through very passively. Yudkowsky expressed optimism that one day we will have an explanation of consciousness which clears up our confusion on the matter. Nagel said that we will need to think more about the dynamics of multi-agent systems and moral epistemology. After that the event ended.

The conference videos are available [here.](http://livestream.com/nyu-tv/ethicsofAI) In my opinion, the best talks were given by LeCun, Railton, Russell, Yudkowsky, the Tegmarks, Petersen, and Marcus. The event overall was great and being in Manhattan made it even better. There was quite a bit of valuable informal meeting and discussion between many of the speakers and attendees. There was no 'sneering' or disdain about Yudkowsky or Bostrom as far as I could tell. It seemed like a generally open minded yet well educated crowd.

If you regret missing it, then you might like to head to the [Envision Conference](http://envision-conference.com/) this December. 

"
