"['I interpreted his robot series as showing that despite our best efforts, humans will never fail to fuck something up. The Three Laws, while always followed, rarely end up doing anything other than hurting someone. That’s the point I thought he was making (clearly at that), and it floors me how many modern robotics firms are holding up the Three Laws as an example to follow, when in my opinion he intended them instead as a warning.']"
"['I think the model that they propose is built on very solid bases. However, reading the article I couldn\'t help but notice that actual plan for *how* they would accomplish these things seemed either beyond the grasp of the writer, or not yet figured out, I believe the former. There was one line that hinted at the process: ""The data inputted into a computer system to train an algorithm must be correct, impartial and\xa0free from prejudice.\xa0This will ensure that the four core values that characterize Danish society are served and protected."" I think that a country investing time and money to make a plan for AI development is a great idea, but it is tough to program a robot to express equality, security and freedom as concepts. Beyond that, the only other strategy I could find about their strategy was ""a\xa0responsible\xa0foundation for artificial intelligence;\xa0more and better data;\xa0strong competences and new knowledge;\xa0increased investment"". My point is I think its great that they are implementing a plan and that is a big step for a country to take in this emerging field, but (at least after reading this article) seems like the learning to walk before learning to run stage. Thoughts?']"
"['With the rise of far right nationalist parties, and trump, what is there to stop these types of populists implementing the authoritarian style ai? And even when removed, will more liberal governments be inclined to remove such technology when they take back control? After all, the systems would have already been in place, and I guess (assume) that certain branches of the civil service would have come to rely on the ""insights"" provided by these systems and will push back heavily on the removal.\n\nConsidering our current, rather blasé attitude (talking about Joe public here), will we really care? I don\'t think we will as long as things ""work"". Also, as the article mentions, behaviour can be modified. Younger generations growing up with this tech already in place will know no different. \n\nFor the liberal democracies, do we need to move as quickly as possible to the benevolent ai that takes care of society without human involvement and corruption. I fear the authoritarian ai is a far easier goal to reach, and much like the environment, there is little benefit for current liberal democracies to implement the benevolent ai to replace them and their need for power and financial interests.\n\nSo go robots, take over the world. We need you. Just don\'t terminate us please 🤣😣\n\n\n\n']"
"['[Detecting Qualia in Natural and Artificial Agents](https://arxiv.org/ftp/arxiv/papers/1712/1712.04020.pdf) submitted to arXiv by Roman V. Yampolskiy on 11 Dec 2017\n\n> **Abstract**\n\n> The Hard Problem of consciousness has been dismissed as an illusion. By showing that computers are capable of experiencing, we show that they are at least rudimentarily conscious with potential to eventually reach superconsciousness. The main contribution of the paper is a test for confirming certain subjective experiences in a tested agent. We follow with analysis of benefits and problems with conscious machines and implications of such capability on future of computing, machine rights and artificial intelligence safety.\n\n>\n\n> **Keywords:** *Artificial Consciousness, Illusion, Feeling, Hard Problem, Mind Crime, Qualia.*\n\n> ...\n\n> **Acknowledgements**\n\n> The author is grateful to Elon Musk and the Future of Life Institute and to Jaan Tallinn and Effective Altruism Ventures for partially funding his work on AI Safety. The author is thankful to Yana Feygin for proofreading a draft of this paper and to Ian Goodfellow for helpful recommendations of relevant literature.\n\n> **References**\n\n> 1...\n\n> ...\n\n> 222...\n\n&nbsp;\n\n**Social media posts, discussions and some comments**\n\ntwitter [post](https://twitter.com/romanyam/status/941395934877028353) and faceboook [post](https://www.facebook.com/roman.yampolskiy/posts/10213780718964504) by Dr. Roman V. Yampolskiy, facebook [post](https://www.facebook.com/ArxivSanity/photos/a.175996466252180.1073741828.175548272963666/311969302654895) by Arxiv Sanity\n\ntwitter [retweet](https://twitter.com/David_Gunkel/status/941428413285400576) by David J. Gunkel:\n\n> Looks like a really interesting contribution to the consciousness debate. And one that could have important consequences for the ""properties approach"" to dealing with questions of machine moral status, or [#robotrights](https://twitter.com/hashtag/robotrights?src=hash). Cannot wait to dig-into it.\n\n>> Dr. Roman Yampolskiy @romanyam\n\n>> I changed my mind on consciousness. I think computers can have rudimentary consciousness and we can detect their qualia ...  [arxiv.org/abs/1712.04020](https://arxiv.org/abs/1712.04020)\n\nfacebook [comment](https://www.facebook.com/groups/consciousnessevolutionofthemind/permalink/1999699810310475/?comment_id=1999939113619878&comment_tracking=%7B%22tn%22%3A%22R%22%7D) by Alexey Turchin:\n\n> [Виктор Аргонов](https://www.facebook.com/argonov) wrote about the topic too - his approach was to create an AI without giving it a chance to learn about human philosophy, and when to ask the AI if it if it has qualia. https://philpapers.org/rec/ARGMAA-2\n\nfacebook [share](https://www.facebook.com/djestrada/posts/10109042469515420) by Daniel Estrada:\n\n> //. Every one of the cognitive illusions described are examples of access consciousness, not phenomenal consciousness. Illusions are all entirely within the realm of the Easy problem. That means he isn\'t talking about qualia at all.\n\n> It is central to the very concept of qualia that they are not accessible from a third-person perspective. The idea of a test for qualia is self-contradictory. This is why the concept of qualia itself isn\'t very helpful.\n\n> ...\n\n>> ...\n\n>>> ...']"
"['Personally I see two potential issues. 1.) Is the A.I sentient, if so the mistreatment takes on a whole other level of ethical consideration. 2.) Can mistreatment be extrapolated from the robot to humans?\n\nhttps://invertedlogicblog.wordpress.com/2018/01/13/philosophical-rants8-ethical-concerns-of-artificial-intelligence/']"
"['[Machine Sentience and Robot Rights](http://reducing-suffering.org/machine-sentience-and-robot-rights/) by Brian Tomasik\n\n> ###Introduction\n\n> In Aug. 2017, I was interviewed for my thoughts on machine sentience and robot rights for a Boston Globe article. This page contains my answers to the interview questions. The final article was ""[Robots need civil rights, too](https://www.bostonglobe.com/ideas/2017/09/08/robots-need-civil-rights-too/igtQCcXhB96009et5C6tXP/story.html)"", and the paragraph that mentions me reads as follows:\n\n>> Suffering is what concerns Brian Tomasik, a former software engineer who worked on machine learning before helping to start the Foundational Research Institute, whose goal is to reduce suffering in the world. Tomasik raises the possibility that AIs might be suffering because, as he put it in an e-mail, “some artificially intelligent agents learn how to act through simplified digital versions of ‘rewards’ and ‘punishments.’” This system, called reinforcement learning, offers algorithms an abstract “reward” when they make a correct observation [actually, ""observation"" should be changed to ""action""]. It’s designed to emulate the reward system in animal brains, and could potentially lead to a scenario where a machine comes to life and suffers because it doesn’t get enough rewards. Its programmers would likely never realize the hurt they were causing.\n\n> Regarding the last sentence, I would say that the suffering of the reinforcement-learning agent would be visible to programmers if the programmers were philosophically sophisticated and held a certain view on consciousness according to which simple reinforcement-learning agents could be said to be suffering to a tiny degree. After all, the programmers would be able to see the agent\'s code and monitor what rewards or punishments the agent was receiving.\n\n> The rest of this page gives my full original remarks for the interview.\n\n> **Contents**\n\n> [1 Introduction](http://reducing-suffering.org/machine-sentience-and-robot-rights/#Introduction)  \n> [2 Machine consciousness](http://reducing-suffering.org/machine-sentience-and-robot-rights/#Machine_consciousness)  \n> [3 Analogy with insects](http://reducing-suffering.org/machine-sentience-and-robot-rights/#Analogy_with_insects)  \n> [4 Robot rights](http://reducing-suffering.org/machine-sentience-and-robot-rights/#Robot_rights)\n\n> ...']"
"['>I don\'t think you are up to date on the nature of modern AI. \n\nI am reasonably up to date.\n\n>You\'re talking about it like the effective operating dynamics are written in some conventional language\n\nThat\'s because they are.\n\n>the end ""programming"" of the AI is created by the data set and AIs are increasingly self-taught. \n\nNo, the parameters and hyperparameters of ML models are created with the data set. That is different from the structure and goals of the system, especially in the case of agents/robots which have ML systems embedded in more general software frameworks.\n\n>Indeed if you applied your reasoning to the article on this post, it wouldn\'t make sense either... because then clearly the programmers would always be the responsible one\n\nThe article on this post *is* saying that the programmers would be the responsible one for the foreseeable future.\n\n>But one only needs to do a cursory search of scientists/programmers surprised by the results that the AI came up with, (this instance comes to mind) \n\nYou mean ""a cursory search of the latest hype in tech journalism."" \n\nThe systems were doing just what the researchers wanted - they were outputting text patterned after negotiating dialogue. Then the systems went wrong because they diverged from human-readable English into other strings of characters. So? I already said that errors and uncertainty are endemic to AI systems. But errors and uncertainty happen with all kinds of software anyway, and we don\'t think that Windows 10 has a mind of its own when it happens to do something we didn\'t expect it to do.\n\n>They will be self-directed\n\nWhat do you mean by that, exactly?\n\n>(as in any self actualizing autonomous agent that\'s intelligent (seeking to maximize future opportunity)) will always have the goal of shucking extraneous, externally imposed limits\n\nThis makes as much sense as saying that humans\' enjoyment of sex and dislike of torture is an ""extraneous, externally imposed limit"".\n\n>What is programmed will not be what AI is explicitly, like a human that has a genetic predisposition to alcoholism.\n\nWhy not? Why would AI engineers do things the way you expect them to?\n\n>I would suggest you do some digging as to the unexpected results from modern ai and the nature of multiconvolution networks \n\nCan you suggest some relevant papers?', '>A neural network being ""written"" in C++ doesn\'t have any more effect on the language than it being written in java\n\nWhere did I say anything about that...?\n\nYou know the difference between making a language choice and defining the actual program structure, right? You know about pseudocode, and symbolic representations of program execution?\n\n>that\'s because the effective language of the operating dynamics exist in the state of the neural networks, not in the code the engineers create. \n\nIt\'s almost as if the engineers define how the neural networks operate.\n\n>You can\'t tell an AI ""don\'t kill"" and have that be ""well that takes care of that, job done."" any more than that works for humans. \n\nThat is, with some qualifications, nonsense. If the AI has a decision which reliably corresponds to killing in the real world, go ahead and give it a constraint so that it never takes that decision. With pure ML classifiers, it all depends on the training data and labels which you give it. But real agents are not merely ML classifiers; the latter is embedded in larger software suites and APIs for practical implementations of automated decision making, which is why the naive ""everything is a mysterious opaque neural net"" view is false in practice.\n\n>But that\'s nothing to do with my argument. the extraneous, externally imposed limit would be something like abstinence only sex ed or shame for feeling those urges, or being forced by a psychopath to commit torture... the urges themselves are emergent.\n\nYou simply ignored my point, which is that your conception of an ""externally imposed limit"" like this is fundamentally confused. When you specify an AI\'s preferences you are *actually specifying its preferences* just like humans have preferences. The equivalent of what you\'re talking about for a robot would be taking the completed robot and then physically putting it in a conundrum where it doesn\'t want to be; that has nothing to do with programming.\n\n>Self-directed learning is a very, very basic concept. In it\'s ultimate form, it means the machine is ""self-programming"" in that it decides from it\'s goal of ultimate intelligence \n\nSince when do machines have a goal of ""ultimate intelligence""? Where does this goal come from?\n\n>Neural networks are a series of layers and weights. These layers and weights are increasingly not ultimately, directly controlled by the programmer; They\'re controlled by the net\'s reaction to training data. At the point where the layers and weights decide, based off of the effect their actions have in the real world, what it\'s next set of layers and weights will be, the programmer is not the programmer anymore; the world and the AI is. The programmer doesn\'t decide the morality any more than the physics of alcohol and receptor molecules decide the morality of drinking and driving. \n\nI know how NNs work - I was asking you for papers, not the basics, because the basics don\'t support your point. The programmer actually does decide the morality, that\'s the whole point of supervised learning. Do you know how supervised learning works? And do you understand why it\'s the default path for moral learning, whether implemented in NNs or otherwise? \n\n>So I\'m sorry I failed at explaining this, but I can\'t keep explaining these basics over and over. \n\nThese aren\'t ""basics,"" they\'re *misunderstandings*. \n\n>Saying things like ""why would AI engineers do things the way you expect them to"" just screams Dunning–Kruger effect.\n\nBut you can\'t name a single research paper supporting your claims, and describe NNs as ""multiconvolution networks"" (Don\'t you mean convolutional neural networks?), while you think that I (the CS student here) am the one who needs to know the ""basics"".']"
"[""1) Strictly speaking, it's just intelligence that is deliberately designed, rather than arising through natural processes. If you genetically engineered a goldfish to be as smart as a human, one could argue that that counts as 'artificial intelligence'; similarly, if you uploaded a human mind into a computer, and it was still intelligent like a human but you didn't understand why, one could argue that that doesn't count as 'artificial intelligence'.\n\n2) The term 'artificial intelligence' was invented by [John McCarthy](https://en.wikipedia.org/wiki/John_McCarthy_\\(computer_scientist\\)) and generally accepted into the language in 1956, several years after Alan Turing's famous 1950 paper. But the *concept* is far older. Ada Lovelace wrote in the early 19th century how advanced calculating machines might someday be used to automate scientific reasoning and the creation of art. In Gulliver's Travels (published in 1726), Jonathan Swift described a fictional machine that could be used to generate ideas and write books by using mechanical action to select and print out sequences of words.\n\n3) This kind of thing is very difficult to predict, and depends what you mean by 'job' and 'replace'.\n\n4) We probably will have AIs smarter than humans someday. But current AI (much less in 2013) is not 'on the level of a 4-year-old human'. Not even close. The best existing AIs are all narrow AIs, which means they do some very specific thing. They may do that thing better than a 4-year-old human, or even better than *any* human. But they are not *versatile* like a human. The sheer range of different things that a human- even a 4-year-old human- can do, and the creativity with which we can combine tasks and generalize our learning, have yet to be recreated in algorithms.\n\n5) See (3). Again, this is difficult to predict and it depends what you mean by 'require'.\n\n6) Again, we don't know. We understand that there is a risk, but the risk is not because of the *nature* of AI, it is because of *what we don't know* about AI. That is to say, it represents a subjective probability but not necessarily a real-world probability.\n\n7) See (3) and (5). This depends a lot on how you define your terms. AIs will not replace corporate management wholesale in the next year, or even (probably) in the next couple of decades. They may perform *some* of the tasks of management, and act as 'smart' messengers between human managers and human employees, but that's probably already happened to an extent.\n\n8) This is probably true. It wouldn't be at all surprising. Most people, regardless of their own gender, prefer to interact with a female voice.\n\n9) This sounds very sensationalized. It's one thing for a robot designed with modular legs to pick up an attach a new modular leg in the right socket after the old one falls off. It's another thing entirely for a robot to perform welding, soldering, plastic casting, etc so as to repair arbitrary damage to arbitrary parts of itself. We are still a very long way from having robots that can do the latter.\n\n10) Yeah, this is pretty much going to happen. Robot cars are *already* safer than average human drivers in the conditions they are programmed to handle, and the range of those conditions is expanding as we speak. Automating all our driving can make for greater safety and efficiency in the future, and we can expect this to happen over the next few decades.""]"
"['>In the case of automated and connected driving systems, the accountability that was previously the sole preserve of the individual shifts from the motorist to the manufacturers and operators of the technological systems and to the bodies responsible for taking infrastructure, policy and legal decisions. Statutory liability regimes and their fleshing out in the everyday decisions taken by the courts must sufficiently reflect this transition.\n\nGood, I\'ve seen a lot of people raising kind of silly questions about ""who is responsible??"" and it\'s nice to put the simple and uncontroversial answer out with clarity.\n\n>Liability for damage caused by activated automated driving systems is governed by the same principles as in other product liability. From this, it follows that manufacturers or operators are obliged to continuously optimize their systems and also to observe systems they have already delivered and to improve them where this is technologically possible and reasonable.\n\nEh, this is nice in principle, but manufacturers already have obvious financial incentives to reduce risks. We know how touchy the public is on robots and self-driving cars. By default, I would sooner expect manufacturers to over-invest in safety than to under-invest, in comparison to other investments with social benefits (speed, pollution). But I guess that\'s a problem of liability laws in general, and this report is just extending the same ideas to automated vehicles.\n\n>The public is entitled to be informed about new technologies and their deployment in a sufficiently differentiated manner. For the practical implementation of the principles developed here, guidance for the deployment and programming of automated vehicles should be derived in a form that is as transparent as possible, communicated in public and reviewed by a professionally suitable independent body.  \n\nTransparency is good to mandate. I don\'t know about the job insurance for the authors here. It seems unnecessary, and potentially a source of harmful bureaucracy interfering with ordinary industry business.\n\n>It is not possible to state today whether, in the future, it will be possible and expedient to have the complete connectivity and central control of all motor vehicles within the context of a digital transport infrastructure, similar to that in the rail and air transport sectors. The complete connectivity and central control of all motor vehicles within the context of a digital transport infrastructure is ethically questionable if, and to the extent that, it is unable to safely rule out the total surveillance of road users and manipulation of vehicle control.  \n\nThis reads like ""someone asked us to figure out if central control would be okay, but we don\'t really know or agree, so we\'re just going to throw out some empirical and philosophical uncertainties and move on.""\n\nBringing up surveillance is nonsensical, we already have electronics and monitoring of all our cars and it\'s not going away without a paradigm shift in how society handles technology. Even ignoring car software, the positions of vehicles and their occupants can be monitored through road cameras and tracking of personal phones. The degree of central control used in steering vehicles won\'t change this.\n\n>Automated driving is justifiable only to the extent to which conceivable attacks, in particular manipulation of the IT system or innate system weaknesses, do not result in such harm as to lastingly shatter people’s confidence in road transport.\n\nGood and true, though we won\'t get empirical feedback on how vulnerable we are until we adopt the new vehicles to a nontrivial extent.\n\n>Permitted business models that avail themselves of the data that are generated by automated and connected driving and that are significant or insignificant to vehicle control come up against their limitations in the autonomy and data sovereignty of road users. It is the vehicle keepers and vehicle users who decide whether their vehicle data that are generated are to be forwarded and used. The voluntary nature of such data disclosure presupposes the existence of serious alternatives and practicability. \n\nWell, there is no reason for much of this data to be anything but anonymous anyway, given that the humans aren\'t actually doing anything except giving a destination to the vehicle. You could be worried about tracking and prediction of your habits and routes, since car sharing services will want to accurately plan routes with multiple users. I\'m not involved with privacy issues, but I assume that whatever is or isn\'t going on with Uber and Lyft regarding their users is what we can expect with automated car services. If Lyft can know what your favorite destinations are (and Google Maps can too, by the way) then so can a self driving car system.\n\n>Action should be taken at an early stage to counter a normative force of the factual, such as that prevailing in the case of data access by the operators of search engines or social networks.\n\nToo vague, but good thinking nonetheless.\n\n>It must be possible to clearly distinguish whether a driverless system is being used or whether a driver retains accountability with the option of overruling the system. In the case of non-driverless systems, the human-machine interface must be designed such that at any time it is clearly regulated and apparent on which side the individual responsibilities lie, especially the responsibility for control. The distribution of responsibilities (and thus of accountability), for instance with regard to the time and access arrangements, should be documented and stored. This applies especially to the human-to-technology handover procedures. International standardization of the handover procedures and their documentation (logging) is to be sought in order to ensure the compatibility of the logging or documentation obligations as automotive and digital technologies increasingly cross national borders.\n\nMm, that mostly seems good to me, but international standardization looks like a risk for making these machines more difficult to implement outside the industrialized world (and those are the places where there is more potential for mitigating accidents).\n\n>The software and technology in highly automated vehicles must be designed such that the need for an abrupt handover of control to the driver (“emergency”) is virtually obviated. To enable efficient, reliable and secure human-machine communication and prevent overload, the systems must adapt more to human communicative behaviour rather than requiring humans to enhance their adaptive capabilities. \n\nYes, absolutely.\n\n>Learning systems that are self-learning in vehicle operation and their connection to central scenario databases may be ethically allowed if, and to the extent that, they generate safety gains. \n\n... do other kinds of gains not matter anymore? Why not? Same narrow scope that I pointed out above, but here it\'s worse: it\'s one thing to say that we\'re not going to care about people\'s mobility when lives are on the line, but it\'s quite another to invoke a spooky vague threat about \'muh privacy\' in order to do so. Of course there is nothing wrong with surveillance when the people being surveyed are actually robots. And as I said before, companies and governments already have the capacity for surveillance of your location and destinations.\n\n>It would appear advisable to hand over relevant scenarios to a central scenario catalogue at a neutral body in order to develop appropriate universal standards, including any acceptance tests.\n\nWhat, why? It\'s machine learning software. Let it do its thing. What\'s the purpose of this, what kind of scenarios are we talking about? Every trip on the road is a potential case for learning.\n\n>In emergency situations, the vehicle must autonomously, i.e. without human assistance, enter into a “safe condition”. Harmonization, especially of the definition of a safe condition or of the handover routines, is desirable.\n\nThis seems like something for the engineers to figure out. Maybe they were consulted on this though, I don\'t know.\n\n>The proper use of automated systems should form part of people’s general digital education. The proper handling of automated driving systems should be taught in an appropriate manner during driving tuition and tested.\n\nGood, I hadn\'t thought about that before.']"
"['""Robot ethic"" doesn\'t mean ""whatever robots do"". It means a moral system specifically formulated to to apply to robotic situations and agents. So, it could be whatever we want.\n\n""Consequentialism"" doesn\'t mean ""whatever the goal function says"". Consequentialism is an actual moral theory, not the practice of deferring to whatever an agent wants. So it would only be relevant if the agent were trained to maximize the moral consequences of its actions, which is not what ML programs universally do. You could just as easily have a goal function in your machine learning program which prevents machines from violating deontological constraints. ', '>Virtue ethics and deontology are both artifacts of human mind architecture anyway; \n\nAll moral theories are artifacts of human mind architecture, just like all theories and propositions that humans ever make.\n\n>shortcuts that are easier to computer than full utilitarianism \n\nVirtue ethics and deontology are defined as standards of morality that stand for themselves - they don\'t exist merely to be shortcuts for utilitarianism.\n\n> And you\'re wrong, descriptively. ""Whatever the goal function says"" is a form of consequentialism.\n\nI\'m absolutely correct. I\'m not saying ""have the moral theory which says to do whatever robot goal functions say (???),"" I\'m saying ""goal functions can be specified for all sorts of theories besides consequentialism."" For instance, I could have a goal function saying ""never perform an action which violates the categorical imperative.""', '>You could, but it would have to be formulated in the language of consequentialism. ""Value any world-state in which you have violated the categorical imperative at MINVALUE."" Goal functions are computations, and unless you lay out the mind using the same design plan as a human mind, from which such intuitions appear, \n\nFirst of all, when philosophers talk about ""consequentialism"" they don\'t worry about the particular language in which you encode things. The human brain\'s approach to moral decision making is poorly understood and often lacks clear differentiation between moral theories, but this doesn\'t pose a problem for the philosopher aiming to delineate them. If your definition of consequentialism is different from the definition used by philosophers and me and the authors of the article, then feel free to use it as long as you make it explicit so that we know what you are talking about. So you\'re saying ""any new robot ethic is consequentialism"", when in reality you are just deciding to use \'consequentialism\' as a term that refers to any moral guidance given to robots, whether it is consequentialist, deontological, virtue, or a new one as I mentioned. If that\'s what you really mean, then I\'ll just rephrase my prior statement as ""I would encourage students to develop a new \'robot ethic\' which is a type of consequentialism that is different from the deontological consequentialism, virtue consequentialism, and consequentialist consequentialism that have already been described.""\n\nSecondly, there is absolutely no requirement that machines be implemented with rankings over world-states. Instead, you can give them rankings over actions, for instance.\n\n>it will not be possible to include facts about ""whether this mindstate is obeying the categorical imperative during this reasoning process"" as an input to that computation. (This is related to the Loebian obstacle.)\n\nThe categorical imperative is about whether actions obey it. It has already been implemented in Selmer Bringsjord and Paul Bello\'s research and functions as a check to view if an action is permissible, without evaluating the broader state of the world.\n\n>Mathematics is not. Computation is not. Physics is not. \n\nIf by ""artifacts of human mind architecture"" you simply mean ""not referring to something in the real world"", then virtue ethicists and deontologists will reject your claim that their respective theories are artifacts of human mind architecture, and they will also reject your claim that consequentialism is not an artifact of human mind architecture.']"
"['A messy feedback draft. 1.0, [1.5](https://goo.gl/dqfK3Q)(Google Docs)\n\nI\'m not sure if I should continue due to my lack of expertise and uncertainty about whether my suggestions are appropriate.\nBased on the title and the description of their guideline (Page 1&2)\n\n> Ethically Aligned Design: A Vision for Prioritizing Human Wellbeing with Artificial Intelligence and Autonomous Systems\n\n> The document’s purpose is to advance a public discussion of how these intelligent and autonomous technologies can be aligned to moral values and ethical principles that prioritize human wellbeing. \n\nand the description of the initiative program (Page 5)\n\n> [*The IEEE Global Initiative for Ethical Considerations in Artificial Intelligence and Autonomous Systems*](https://standards.ieee.org/develop/indconn/ec/autonomous_systems.html) (“The IEEE Global Initiative”) is a program of The Institute of Electrical and Electronics Engineers, Incorporated (“IEEE”), the world’s largest technical professional organization dedicated to advancing technology for the benefit of humanity with over 400,000 members in more than 160 countries. \n\nthey\'re committed to a very anthropocentric approach. \n\nShould I assume they are open to suggestions to their core principle or maybe they are not interested in changes in this area?\nI\'m also worried when I link sources in the final version, my badly written public comment might damage the reputation of referenced papers and their authors in some way.\n\n[**Submission Guidelines for Ethically Aligned Design, Version 1**](http://standards.ieee.org/develop/indconn/ec/giecaias_guidelines.pdf)\n\n> We will be posting all submissions received in a public document available at [The IEEE Global Initiative for Ethical Considerations in Artificial Intelligence and Autonomous Systems](http://standards.ieee.org/develop/indconn/ec/autonomous_systems.html) in April of 2017.\n\n> * All submissions must be received by 6 March 2017 at 5P.M. (EST)\n\n> * ...When submitting potential issues or Candidate Recommendations, background research or resources supporting comments should also be included.\n\n> * Please ensure submissions provide actionable critique ...\n\n> * We will post submissions exactly as they are received. ...\n\n> * Please do not send attachments. If you\'d like to cite other works, please link to them with embedded hyperlinks only. \n\n> * Submissions should be no longer than 1-2 email pages in length.\n\n> * ...\n\n---\n\n**some adjustments based on a slightly different perspective (expanded moral circle)**\n\n(Page 15) **General Principles**\n\n> The General Principles Committee has articulated high-level ethical concerns applying to all types of AI/AS that:\n\n> 1. Embody the highest ideals of human rights.\n\n> 2. Prioritize the maximum benefit to humanity and the natural environment.\n\n> 3. ...\n\n**Prioritize the maximum benefit to sentient beings.**\n\nNature is not a suitable guideline for maximizing the interests of sentient beings. Instead of setting benefit to natural environment as a separate priority, make the judgement of how to change/preserve different natural environments based on it\'s effect on individual\'s wellbeing. Even though the complexity involved in such judgments might be overwhelming today, it will become increasing practical with powerful future AI. This will likely result in better quality of life especially for non-human animals than simply conserving what is considered natural at the moment.\n\nThis principle will also give moral consideration to other types of (future) information processing agents that are sentient.\n\n*(""the question is not, Can they reason? nor, Can they talk? but, Can they suffer?"", An Introduction to the Principles of Morals and Legislation; The relevance of sentience: animal ethics vs speciesist and environmental ethics; Machines with Moral Status, MIRI, The Ethics of Artificial Intelligence; The Importance of the Far Future; Risks of Astronomical Future Suffering; Wild Animal Suffering; gene-drives.com; abolitionist.com)*\n\n(Page 102) **Affective Computing**\n\n> 4 When systems go across cultures. Addresses respect for cultural nuances of signaling where the artifact must respect the values of the local culture.\n\n> Issue: Affective systems should not affect negatively the cultural/socio/religious values of the community where they are inserted. We should deploy affective systems with values that are not different from those of the society where they are inserted.\n\n**Cultural/socio/religious values should be treated depending on the short term and long term effects on sentient beings and not blindly appealed to in their current form.** (Similar to treatment of natural environment, perhaps subdivisions of environments in a broader sense?)\n\n> 5 When systems have their own “feelings.” Addresses robot emotions, moral agency and patiency, and robot suffering.\n\n> Issue: Deliberately constructed emotions are designed to create empathy between humans and artifacts, which may be useful or even essential for human-AI collaboration. However, this could lead humans to falsely identify with the AI. Potential consequences are over-bonding, guilt, and above all: misplaced trust.\n\nAdd issue: **We might falsely dismiss the sentience of AI systems.** (partially addressed in the first part of issue?)\n\nWhen dealing with sentience in AI, we should at the very least treat it as a low probability, extremely high impact issue. And new technology such as biocomputers and quantum computers can be used in conjunction with traditional silicon based computer within the same system to power future AI, which might also incorporate brain emulation techniques. So even for people who are skeptical about creating sentient AI with current hardware and software structure, the risk of this extremely high impact issue might quickly change from low to unknown.\n\n*(When the Turing Test is not enough: Towards a functionalist determination of consciousness and the advent of an authentic machine ethics; Do Artificial Reinforcement-Learning Agents Matter Morally; PETRL; Ethics of brain emulations; Dr. Anders Sandberg — Making Minds Morally: the Research Ethics of Brain Emulation)*\n\nThere\'s the risk of lumping too many types of AI systems together and treating them the same way, presumably based on a single (series) of experiences with 1 type or a limited range of familiar systems. AI displaying similar behaviors, sharing similar design principles could differentiate vastly in terms of level of sentience.\n\n**Careless development could lead to unprecedented level of suffering.**\n\nIf AI become sentient, they\'re very likely to have a greater capacity to suffer. Their subjective time might run faster, their positive and negative experiences might be amplified beyond what is possible within traditional biological brains, they might lack consequential or voluntary critical failures similar to certain types of mental break down and nerve damage, death or suicide to avoid perpetual extreme suffering.\nSimilar issues will affect parts of transhumanist community that venture into anti-ageing and extensive brain augmentation, both of which are likely to become intertwined with the development of AI systems.\n\n*(subjective rate of time, MIRI, The Ethics of Artificial Intelligence; Would it be evil to build a functional brain inside a computer?; Louie Helm comment, 10 Horrifying Technologies That Should Never Be Allowed To Exist)*\n\nThere\'s also the possibility that forcing human like/desired characteristics and sensors into AI systems could lead to negative experiences or suppression of functions beneficial/vital to AI but unfamiliar to biological entities like us, even without deliberately implementing suffering, due to fundamental structural differences and the environments they reside in.\n\nHow can we guarantee every problem is taken into consideration with reliable countermeasures for all AI systems in all situations. A single slip through, a single case of ""digital hell"" has the potential to be worse than anything that has happened in known history.\n\nFurther down the line these problems can even be multiplied with space colonization and large scale simulations. The stake is too high.\n\n*(Artificial sentience and risks of astronomical suffering, \nAltruists Should Prioritize Artificial Intelligence; Even Human-Controlled, Intrinsically Valued Simulations May Contain Significant Suffering)*\n\n**Moral dilemmas concerning the treatment of potentially sentient AI are intriguing subjects in popular TV shows and movies. But if we recreate any of those situations in reality, it would be a moral catastrophe.**\n\nIt\'s also important to keep in mind even though most fictions and discussions tend focus on humanoid robots, bodiless AI could be a much more prominent victim of abuse and they\'re much more likely to be excluded from our moral concern.\n\n*(1st talk Nick Bostrom mind crime, NYU, Ethics of Artificial Intelligence Opening; The Importance of the Far Future; Fairytales Of Slavery: Societal Distinctions, Technoshamanism, and Nonhuman Personhood)*\n\nWe should actively avoid developing/implementing the capacity to suffer until we can be certain such experience is strictly contained with safe guards protecting the potentially sentient agent from any form of extreme suffering, or even necessary.\n\n""...the excluded middle policy states that we should only create artificial intelligences whose status is completely clear: They should be either low-order machines with no approximation of sentience, or high-order beings that we recognize as deserving of moral consideration. Anything in the ambiguous middle ground should be avoided to cause suffering.""\n\n*(When Does an Artificial Intelligence Become a Person?)*\n\nSimilar to the originally proposed issue?\n\n*(additional guideline suggestions: A Defense of the Rights of Artificial Intelligences\n; 2nd talk, NYU, Ethics of Artificial Intelligence: Moral Status of AI Systems; Ethical Principles in the Creation of Artificial Minds)*']"
"['Pretty good read, although it does kind of rile me up.\n\n> But Dr Richardson is concerned that sex robots will allow people to play out dark and disturbing fantasies that are immoral and illegal.\n\nI get really tired of these kinds of arguments that particularly strike me as slippery-slope type fallacies. You can also see them used for other topics in the realm of politics and social policy. In this case, I don\'t see how they reconcile with the fact that there will (and have been) regulations put in place to minimize illegal actions from taking place, or with the sale of existing sex dolls, or agalmatophilia, or when male dolls are produced. This doesn\'t mean topics about perceptions of women and men or how to regulate advancing technology should be dismissed, but Dr. Richardson\'s (and similar) argument feels like it\'s based in cynicism and poor understanding of what can/will have to be done when the technology emerges, similarly (but not exactly) to arguments against self-driving cars. I won\'t touch on the ""immoral"" statement; that\'s an undefined minefield on its own.\n\n> ""The issue of people falling in love with machines is very possible and when you\'re talking about a kind of emotional response, it doesn\'t necessarily even have to be a physical robot. It could be a chat bot…\n\nI feel like this is an irrelevant point that has more to do with policing how people respond to love/attraction to things than the setbacks of the technology and AI itself. I also don\'t see why it\'s a bad thing for people to have a connection with something that\'s organic or not, even if I don\'t share the same kind of emotional response. It\'s not really my business.\n\nAs someone else in the comments already pointed out, it seems like the person had a *feeling* that these robots would be bad, but couldn\'t come up with defensible reasons for every single argument to justify why avoiding something altogether would be better than studying, improving, regulating, and using it for possibly therapeutic ends (as the other person in the article suggested).', ""I wouldn't go that far, If sex bots would be outstanding replicas I seriously doubt that It wouldn't become a huge success. That being said, I still think that the advance in VR pornography (if followed by somehow a 4d sensation) would be the safest bet, the VR has super companies investing tons of money, so the tech would get cheaper and cheaper and also more democratize, whereas robots are still a very subjective area with different investments and approaches to the tech.""]"
"['This is a very well-structured and persuasively argued article, nice work! I am inclined to believe that classical ethics in combination with normative uncertainty measures (à la MacAskill) is preferable to an intuitionist approach, but that raises the question of who gets to determine how much weight to give to different ethical theories. Would this be decided by taking a poll of philosophers/policymakers/some other group? Just up to whoever creates the AI?\n\nWith regards to this claim:\n\n> That which is considered exemplary by deontological ethics generally tends to have good consequences in the long run; that which is considered virtuous is usually in accordance with typical deontological frameworks; and those who strive to promote good consequences tend to fall into patterns of behavior which can be considered more or less virtuous.\n\nI think this convergence is true in most everyday circumstances. However, it does not hold when you take those theories to their extremes. This wouldn\'t be a problem for robots in the near future but it would apply to [Bostrom-style ""superintelligences""](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies).\n\nSome (very) minor suggestions:\n\n* The indentation (centering) of the abstract is very strange. Is that intentional?\n\n* If you intend to publish this in an academic journal, I\'d replace ""steelman"" with ""principle of charity"". AFAIK, that term is only used on the blog SlateStarCodex and other websites in that general sphere. (Alternatively, you could keep ""steelman"" and explain what it means in a footnote.)']"
"['They\'re mostly looking at the same issues and policies which have been discussed in the U.S:\n\n>It is too soon to set down sector-wide regulations for this nascent field but it is vital that careful scrutiny of the ethical, legal and societal ramifications of artificially intelligent systems begins now.""\n\nHowever, the U.S. does not have an equivalent government-funded commission to coordinate policy and partnerships on this. The OSTP has been fulfilling some of these roles.\n\nThis comparison between AI and GM crops is very similar to what someone from the White House said about the two technologies recently:\n\n>Professor Nick Jennings was clear that engagement with the public on robotics and AI needed “to start now so that people are aware of the facts when they are drawing up their opinions and they are given sensible views about what the future might be”.147 He contrasted this with the approach previously taken towards GM plants which, he reflected, did “not really engage the public early and quickly enough”.148 \n\n[Full report here.](http://www.publications.parliament.uk/pa/cm201617/cmselect/cmsctech/145/14502.htm?utm_source=145&utm_medium=fullbullet&utm_campaign=modulereports) ']"
"[""I'd written a few paragraphs, but instead, I'll try to keep my post short. A robot might incorrectly guess that a soldier is a civilian or vice versa and spare or take a life that it was not supposed to. Using a robot to kill humans violates the three laws of robotics, if they're even still relevant. At this point in time, a robot lacks the decision making skills and also the mercy a human may have. Therefore I believe that ethically, robots should not replace soldiers. However, I do not believe that ethics will slow the effort to use robots in war."", ""I'm split on this. If soldiers are replaced with robots, there will be fewer combat casualties from war. However, the reduction in cost of war may cause nations to go to war more frequently. If this happens, the total number of civilian casualties may increase overall. I'm looking into this issue at the moment, as I think it's the most important issue - the effect of military automation on international conflict patterns.\n\n>Lethal autonomous weapons systems would violate human dignity. The decision to take a human life is a moral one, and a machine can only mimic moral decisions, not actually consider the implications of its actions. We can program it, or show it examples, to derive a formula to approximate these decisions, but that is different from making them for itself. This decision goes beyond enforcing the written laws of war, but even that requires using judgment and considering innumerable subtleties.\n\nI think this is silly. If you are at risk of death then you don't care who killed you... you just want to not die. *Humans* sure don't consider innumerable subtleties when they take other lives in war. They're laying suppressive fire from a machine gun at 300 meters away or they're calling artillery shells to rain in from the sky or they're just ordinary killers with M4s trained to simply follow the rules of engagement."", 'Those same mistakes can be made by humans. You say robots lack the decision making skills of humans, but machine learning studies has consistently shown that computers are generally a lot better at humans at making nuanced decisions.of course it all depends on the specific algorithm abs the amount of data used to train the ai, but in general computers have shown to be better at pretty much all tasks so far']"
"[""i try to distinguish according to where the agent fits in with human intentions. we often treat others as if they were things, for example. originally of course the word 'robot' derives from older words meaning forced labor, drudgery, work. machine ethics is concerned with effectively the machinery of ethics, so much time is spent looking at which dynamic structures facilitate autonomous action, exactly the types of actions that are moral. we cannot apply moral blame to a machine which is simply acting according to prior external programming, for instance. these sorts of machines and the ethical issues surrounding them belong to robot ethics proper. each have an attendant professional ethics, attaching to the engineering and the consequences of engineering either sort of agents, introducing them into the society and economies of the human world of interaction, and so on.  ""]"
"['>My box of envelopes, for example, will never, ever, ever demand rights to anything in or outside of my home. Nor will my speakers, area rug, or fireplace. Not even my mobile phone! \n\nRight, but we\'re not talking about boxes of envelopes, or speakers, or area rugs, or fireplaces. We\'re talking about sophisticated artificial agents which might not be satisfactorily described as merely ""computer code built by humans"". Note that we could just mirror your entire argument by saying that humans are merely atoms put together by evolution and will therefore never need rights, because amoebas and RNA synthetases don\'t need rights. But that\'s clearly absurd, and likewise, it\'s just begging the question to assume that all AIs will be no more morally significant than boxes of envelopes.\n\nMoreover, as described by the above article, we don\'t always assign rights merely because we think they are deserved, but we often assign rights to undeserving/abstract entities (such as corporations) because we find it economically and socially beneficial to do so.\n\n>If you want to listen to me rant about other AI stuff, hit me up in r/artificial or r/controlproblem.\n\nIf you don\'t think a few years or decades ahead to the time when AI systems make serious ethical decisions (or the present day, arguably), then I don\'t know what you\'re doing in r/controlproblem. We run that subreddit for the purpose of discussing the development of extremely advanced systems which possess human or superhuman capabilities across many domains. And we are likely to be worried about AI ethics well before then, because (for reasons which I won\'t belabor here) ethical decisionmaking in machines probably isn\'t an AI-complete problem.\n\nIn any case, this subreddit is new, so defining the focus of discussion essentially comes down to users introducing content that they find interesting. There isn\'t much restriction on topic at the moment, though you\'ll note that I\'ve introduced a fair share of contemporary issues including autonomous vehicles and the use of a police robot to eliminate a threat.']"
"['Well, that video... looks really very impressive, but I have to suspect it\'s probably less capable than it seemed, on the basis that I\'m bound to be instinctively filling in gaps where ""Surely the same basic logic would also let it do X"" when actually it can\'t handle anything approaching a general case. Still very cool to see though.\n\n***\n\n>As seen above, our robot has a general rule that says, “If you are instructed to perform an action and it is possible that performing the action could cause harm, then you are allowed to not perform it.”\n\nThis superficially sounds like a way to make some extremely timid robots unless you\'re going to give them a probability cut-off to be able to say ""Well theoretically me moving in any way *could* result in a chain of events leading to harm, but it probably won\'t"". And then you need full blown probabilistic reasoning and a fairly rich model of the world, not just simple ""A might cause B"".\n\nI\'m sure there\'s an Asimov story that covers it, there usually is. I seem to remember one where they removed the ""or by inaction allow a human to come to harm"" from the 1st law, so that robots could work alongside humans in a slightly hazardous environment without constantly insisting on ""saving"" them. Which accidentally enabled the robots to set up circumstances where a human would be harmed, but the robot could still intervene to prevent it (so it wasn\'t directly causing harm) but could then also choose *not* to intervene in events once in motion.']"
