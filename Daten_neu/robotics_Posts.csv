"I am thrilled to finally unveil my latest creation - Arctos! This has been my labor of love, and I can't wait for you to experience it. The hosting may be a bit slow at first, but trust me, it's worth the wait. Follow the link https://arctos.rf.gd/ and let's build something amazing together!"
"Amazon bought the company that makes the Roomba. Anti-trust researchers and data privacy experts say it's 'the most dangerous, threatening acquisition in the company's history'"
"12V power wheels battery for my fathers robot (he is not careful with batteries)TL:DR - Is using the power tools adapter (like they use on the kids power wheels cars) a good battery solution for my old man that I am hesitant to trust using a Li Ion pack. If you have a better solution it would be appreciated. 

I will try and keep it short and simple. I have made a robot/ tank for my dad to use to go under houses. I am trying to think of the best, and least likely to start a fire, solution for the battery. For testing this robot I have used a 3S Li Ion that I wired up a while back for a drone. It works great. I have considered sticking that in there, but him having to take the battery out each time and put it on a balance charger and all is a pain and I doubt he would want to do it. Also, a 3S battery pack without a BMS is sketchy, especially when its him that is dealing with it. 

My main theory as of now, is to use those 12v power wheel adapters and use like a Milwaukie battery pack with it. My thinking is I can just stick the adapter on the top of the robot and he can easily remove batteries and charge them, while being safer. In my mind this is the best mix of easy and safe ish. There is already chargers for those batteries that are pretty easy and usually they have some protections built in. Makes me feel much more comfortable that just a 3S li ion pack sitting in the enclosure over time. 

I would really appreciate any opinions here, mainly I am looking to see if anyone thinks this is a bad idea. If you guys think I should maybe go another route. Also, I don't have any experience with these power tool adapters, so I would think if anyone would when it comes to robotics you guys would. 

Thanks so much, I appreciate any feedback."
Would you like this kind of robot? Is it a positive attitude or a negative attitude?
"Where can I get custom closed loop GT2 belts?I have a 3d printed 330mm Ninja Flex closed-loop belt I'm using now. But I'm not certain of its long term durability. There are a couple sites online where I can buy them, but I'm wondering if anyone has a trusted source.

I've also tried glueing an ""endless"" belt based on some online tutorials, but they tore apart right away. I need something reliable."
"Ardupilot dev conferenceThe Ardupilot community had its conference in Canberra last weekend. You can watch the talks on the YouTube channel linked. Some really cool stuff about multirotor controls, state estimation, synchronized aerobatics with fixed-wing, controlling a scale F35 with ardupilot, and much much more.

ArduPilot is a trusted, versatile, and open source autopilot system supporting many vehicle types: multi-copters, traditional helicopters, fixed wing aircraft, boats, submarines, rovers and more. The source code is developed by a large community of professionals and enthusiasts. New developers are always welcome! The best way to start is by joining the Developer Team Forum, which is open to all and chock-full of daily development goodness."
"How to access relative pan/tilt on this UVC webcam?So I bought this pan-tilt-zoom camera: [https://www.amazon.com/OBSBOT-AI-Powered-Omni-Directional-90-Degree-Correction/dp/B08NPGNMV8/ref=sr\_1\_2\_sspa?keywords=obsbot&qid=1678776407&sprefix=obsbot%2Caps%2C142&sr=8-2-spons&ufe=app\_do%3Aamzn1.fos.f5122f16-c3e8-4386-bf32-63e904010ad0&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUFaMlVOV1BYQU01WkomZW5jcnlwdGVkSWQ9QTAzNjY1MzczVjRTSjVNWEhQVTNCJmVuY3J5cHRlZEFkSWQ9QTA1NzA0OTQzUUgwRkwzU1ZGVVAwJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ==](https://www.amazon.com/OBSBOT-AI-Powered-Omni-Directional-90-Degree-Correction/dp/B08NPGNMV8/ref=sr_1_2_sspa?keywords=obsbot&qid=1678776407&sprefix=obsbot%2Caps%2C142&sr=8-2-spons&ufe=app_do%3Aamzn1.fos.f5122f16-c3e8-4386-bf32-63e904010ad0&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUFaMlVOV1BYQU01WkomZW5jcnlwdGVkSWQ9QTAzNjY1MzczVjRTSjVNWEhQVTNCJmVuY3J5cHRlZEFkSWQ9QTA1NzA0OTQzUUgwRkwzU1ZGVVAwJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ==)

And I'm trying to control the pan-tilt mechanism from Python. The developers of this camera implemented UVC protocol and I can verify they implemented absolute pan/tilt. However, they did not implement relative pan/tilt, which is what I need.

What are some ways I can have access to relative pan/tilt control? 

I know that they have this functionality available because their proprietary software has this joy-stick control and it's definitely implementing relative pan/tilt (See picture below). .   


By chance, can I somehow reverse engineer this GUI so that I can see what sort of functions this GUI is calling so that I can call it from code? Or can I somehow implement my own UVC protocol on this device? These are some things I'm not sure about. 

&#x200B;

https://preview.redd.it/och8zmndknna1.png?width=551&format=png&auto=webp&v=enabled&s=57194d6e9d7b1463ff7b141358f230cc094bdd15"
Loona RobotI’m interested in getting a personal robot. I was looking at Loona. When will Loona start getting produced? Is this something I can trust to buy?? I don’t want it to be another Digital Dream Labs catastrophe.
"Recommendation of 6 DOF manipulator robot to use with ros at home (student budget)Hello, 

I have been looking for a manipulator since I started my phd just to improve my skills in ROS which I'm using every day. I would like to have it to test some code, as I'm developing a custom control algorithm for a robot. 

Can anyone give me a tip about a nice robot to start? As this robots are quite expensive for my budget right now (like the dobot CR or the myCobot), I would like to invest this money in something that I can trust.

Thanks in advance!"
Would you trust a Robot to OPERATE on you?
Human-Computer Interaction Glow Up: Examining Operational Trust and Intention Towards Mars Autonomous Systems | ASCEND 2021
Tiny Robots Have Successfully Cleared Pneumonia From The Lungs of Mice
NASA Webinar: Technology for Trust in Human Operators
A Purdue Professor Is Looking At Ways To Measure Trust In Real-Time To Design Robots That Can Sense Humans’ Levels Of Trust In Them And Adjust Their Behaviors Accordingly.
"Generic gear motor couplingsI need to use some of the generic DC gear motors below. They have a shaft with what I measure as a 5mm double-d shaft.  I want to get couplings for them...but I can't find anything.  Other than 3d printing everything, when people use these, what do they do?

Motor:

[https://www.amazon.com/Cylewet-Motor-Shaft-Arduino-CYT1037/dp/B01N9MS3UZ/ref=asc\_df\_B01N9MS3UZ/?tag=hyprod-20&linkCode=df0&hvadid=343252307519&hvpos=&hvnetw=g&hvrand=15792488831872382863&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9032937&hvtargid=pla-737009264350&psc=1&tag=&ref=&adgrpid=64453690210&hvpone=&hvptwo=&hvadid=343252307519&hvpos=&hvnetw=g&hvrand=15792488831872382863&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9032937&hvtargid=pla-737009264350](https://www.amazon.com/Cylewet-Motor-Shaft-Arduino-CYT1037/dp/B01N9MS3UZ/ref=asc_df_B01N9MS3UZ/?tag=hyprod-20&linkCode=df0&hvadid=343252307519&hvpos=&hvnetw=g&hvrand=15792488831872382863&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9032937&hvtargid=pla-737009264350&psc=1&tag=&ref=&adgrpid=64453690210&hvpone=&hvptwo=&hvadid=343252307519&hvpos=&hvnetw=g&hvrand=15792488831872382863&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9032937&hvtargid=pla-737009264350)"
"RT/ Future autonomous machines may build trust through emotion. Robotics biweekly vol.13, 10th September — 24th September"
humans are now heading towards building trust on Robots
"Would anyone really trust a robot chef or automated carespecially the car, i'd probably be super nervous

[https://www.youtube.com/watch?v=opnJVN-f2cE](https://www.youtube.com/watch?v=opnJVN-f2cE)"
"What do you need to trust fully autonomous robots?Autonomous systems promise to facilitate the way humans make decisions in our personal lives, business, enterprise, and even the military. These systems use sensors, data, and machine learning to minimize the cognitive process required by operators to make critical decisions. The continuous monitoring and processing of any structured data created by our surroundings can speed up tasks that used to take hours, days, or even months. But can we really trust their way of thinking? Can we place our lives at the hands of these machines?

Machines have been able to take on mechanical tasks more efficiently than humans ever could. They quickly proved they were more efficient than us and we got used to the idea of letting them take over the simple mundane jobs we don’t necessarily want. But machines are evolving quickly; faster than many of us anticipated. They are intertwining with our lives more each day, and the level of cognitive power is challenging our own. The amount of trust we must place in these machines is increasing every day, and the consequences of their mistakes may be deadly in some cases. Placing our trust in these machines becomes a continuous game of risk management. Do I trust this machine enough to make an actionable decision that might impact my life? Can I trust this machine has considered all scenarios better than I could with the given parameters? Has anything or anyone influenced their programming to purposely influence my thoughts or actions? Has this machine been tested enough to cover all possible use cases applicable to my needs? What does it mean for a human to trust a machine? How much trust is enough?

I don’t think we’ll find a single answer that satisfies everyone; since the very definition of trust might be different for each one of us, so let’s start with the same basis. Merriam-Webster’s dictionary defines trust as:
    
     Assured reliance on the character, ability, strength, or truth of someone or something.

So, it seems we must rely on machines to be a part of our decision making process, and we must have some degree of confidence in the ability of that machine to shape our reasoning towards some belief or action.

Now, let’s break down the composition of these autonomous systems. We can classify them into two categories: Autonomy in Motion and Autonomy at Rest. This is a simple distinction between systems with kinetic functions such as cars, drones, or robots and systems with non-kinetic functions that operate virtually in software such as personal assistants, cyber security systems, etc. These systems rely on algorithms to analyze large amounts of data generated by connected components such as radar, cameras, network traffic, Global Positioning Systems (GPS), Electromagnetic Sensing (EMS), etc. All the data is put together in a place where the system can make sense of it and then use machine learning, artificial intelligence, data modeling, and scenario generation, among others algorithms to analyze that data and provide recommendations to operators. The operators then must trust the autonomous systems and make decisions based on their analysis. Each decision brings a risk associated with it and that risk drives the way we build the redundancy of these systems. Critical autonomous applications require a higher level of confidence, so these systems might leverage multiple input sources and fail safes to be able to provide reliable functions. We need less trust in a personal assistant that turns a light on in our house than an autonomous car that drives our family around, since our risk assessment of both functions is vastly different. We think harder before we put our kids in an autonomous car than we do before asking Siri to recommend a song, since the failure of one system doesn’t have the same consequences as the other.

There are three main fields where autonomous system developers can improve to provide trust between the human and the machine:

1. Human-Computer Interaction (HCI): Engineers have realized that they cannot lock themselves in a room to build products; the integration of engineering and human sciences is necessary to build comprehensive solutions that meet the user’s needs. This is becoming more and more obvious as the complexity of these systems increases. Humans need to understand why decisions are made without having to know implementation details. This is indispensable in critical applications where human lives are at risk. The handoff between the human and the computer must be seamless in the decision process to work.

2. Cyber Security Systems: Input components may lie outside the boundaries of the physical system. Distributed computing systems leverage the computational power of the cloud infrastructure to perform analytics of large amounts of data. The data must be validated against a valid predefined set in order to be classified depending on the system needs. Autonomous systems still have the same cyber security needs as other components—malware detection, Intrusion Detection Systems (IDS), Intrusion Protection Systems (IPS), Deception Technologies, etc. These types of systems still need to be in place, although two differentiations can be made:

a. Cyber security systems need to become smarter by using technologies such as machine learning and Artificial Intelligence (AI) to augment their capabilities in order to keep up with the different forms of attacks their adversaries bring. Techniques such as signature base detection are quickly becoming obsolete thanks to polymorphic attacks that can mutate based on their targets. Systems have to change from being reactive to actively predicting threats, and from being static to dynamic.

b. Autonomous sensors bring new challenges to the cyber security space. Machine learning algorithms depend on the integrity of the data to provide reliable and resilient solutions; if the data is compromised then the whole system cannot be trusted. A lot of research is being done to make sure that AI algorithms cannot be purposely fooled; attacks such as the “adversarial image” can be potentially fatal in critical systems.

3. Testing Tools and Methodologies: Software development has been changing in recent years due to the increase of computational power available in current systems. Software is becoming dynamic and the predictability of such systems is becoming non-existent. This makes the testing of autonomous systems extremely difficult. Developers cannot use the conventional testing methodologies to test their algorithms, they must rely on computational models and scenario generation to test the various use cases users may encounter. On average, current commercial software only tests about 6 percent of the lines of code written, that is unit tests and integration tests with a well-defined set of parameters. Autonomous systems are different from these algorithms because the amount of data they need to learn grows exponentially from static software implementations. It’s hard to predict all scenarios the autonomous system will encounter out in the wild, and the complexity to generate scenario models increases with the size and criticality of the system.

Trust will define how much humans rely on these emerging technologies. The lack of trust will constrain their abilities and usability. We build systems with great capabilities but we often constrain them due to security or technical concerns. This is why we must strive to find methodologies that enable us to build secure, reliable, and resilient systems that humans can truly use without questioning their process or reasoning.

tl;dr"
Older Adults Don't Entirely Trust Robots With Kids
Connect Reliably to ROS 1 and 2 Stacks With High-Rate and Large Messages
"Reliable way to generate grasp position on 3D modelsHi all, I am writing my thesis on ML and grasp on object but I can't find any way to generate the best position of a gripper on an object. Anyone have anything ? Feeling kinda desperate

&#x200B;

Thanks for your answers, I really appreciate and I will read that!

For the details, I have multiple 3D models of objects in the STL format, so I could go into a points cloud but I guess staying in mesh keep more info.

&#x200B;

My end goal is to build a AI algorithm to choose the best end effector given an object.

So I need a program/system to calculate analytically or empirically one/multiple grasp poses for different grippers (in priority two and three fingers parallel claws, big issue is the 3-finger claw) and get the associated metrics.

I know theoretically it's just a complicated math problem so I expected to find something that would compute it but I couldn't find it yet, or maybe it exists but I just didn't found it yet.

&#x200B;

Side note is that it would be a gripping in a normal manner so only from the top, which reduces the computation by a lot.

&#x200B;

&#x200B;

Edit: more details

&#x200B;"
How do you peel such stickers (not available in rolled condition) with Robots? Preferably Cobots? We tried Pneumatic suction cups but wasn't reliable. Any ideas? Anyone did anything similar? Any video references of similar applications?
"Pc vs laptopHi I'm a prospective robotics msc at delft, and was thinking to upgrade my already existing laptop as it i behaves very strangely/has little ram and no graphics card. 
I was stuck between choosing a pc or a laptop. I was wondering if anyone knew how computationally expensive university courseworks could be in Robotics, and therefore if they could suggest me to get a good pc or  if a laptop could suffice. Mind you if I had a pc I would still keep my laptop for when i go to the library, however it isn't so reliable for very computationally heavy code..

Thanks you in advance :)"
how can I connect the gear/shaft to a printed part reliably?
"Carter Schultz of AMP Robotics is a 2023 Reddit Robotics Showcase Keynote Speaker![https://www.youtube.com/watch?v=wCcTrivOAQ0](https://www.youtube.com/watch?v=wCcTrivOAQ0)

&#x200B;

[Carter Schultz](https://preview.redd.it/1kkbuxt5baza1.png?width=1258&format=png&auto=webp&v=enabled&s=b2e6ed3152b07162bbd5e1dfb4676d40c5337dac)

We are thrilled to welcome Carter Schultz, Robotics Architect at AMP Robotics, as a 2023 Reddit Robotics Showcase Keynote Speaker! He will be discussing The Reality of Robotic Systems. From AMP:

""Asked to describe a robot, most would default to a character from science fiction—something resembling a human form. What ultimately became AMP Robotics can even be traced back to Founder Matanya Horowitz’s early fascination with such animated characters. But as robotics applications evolve, it’s time to challenge—or expand—the traditional understanding of a robot. We’ll explore recycling and how robotic sorting units driven by AI are bringing much-needed change to an industry not known for innovation, while demonstrating the usefulness of conceiving of “robots” as large, autonomous systems.  


This talk will illuminate how cutting-edge technologies are revolutionizing recycling by addressing constraints most people may not even know exist, framed against the backdrop of how autonomous systems operate in other industrial environments—and why the realm of recycling is so unique.""

&#x200B;

https://preview.redd.it/r0sqp0abbaza1.jpg?width=4715&format=pjpg&auto=webp&v=enabled&s=30914ad6ff480c162b36e3a1705c4735e3a41e3b

AMP Robotics is modernizing and scaling the world’s recycling infrastructure by applying AI and automation to increase recycling rates and economically recover recyclables reclaimed as raw materials for the global supply chain. In addition to developing AI-enabled solutions to retrofit existing recycling facilities, AMP also designs, builds, and operates new facilities powered by its application of AI for material identification and advanced automation. With hundreds of deployments across North America, Asia, and Europe, AMP’s technology recovers plastics, paper, and metals from municipal collection, precious commodities from electronic scrap, high-value materials from construction and demolition debris, and valuable feedstocks from organic material.

&#x200B;

For more information on AMP Robotics, please visit the links below:

* [Youtube](https://www.youtube.com/@AMPRobotics/videos)
* [Website](https://www.amprobotics.com/)
* [Linkedin](https://www.linkedin.com/company/amp-robotics/)
* [Twitter](https://twitter.com/AMPRobotics)
* [Instagram](https://www.instagram.com/amprobotics/)

&#x200B;

Click [here](https://redditroboticsshowcase.wordpress.com/) for more information on the Reddit Robotics Showcase. We hope to see you there!"
"IMU - looking for a reliable productHi all,

I'm looking for a reliable imu for a autonomous vehicle. It's been a while since i last needed such a device. So, I don't know what is popular nowadays.

I have used 2 good imu/ins in the past: xsens mti-710 and ixblue ursa5. But, my budget is limited to usd$1.000 in this project.

I would really appreciate a recommendation.

I'm concerned because most cheap imu feel like random number generators. I have used adis16470 and it is kinda bad.

I'm thinking on buying the um7.

Thanks!"
"Need Help in moving to computer aided design of robotsHello everyone,   
Till now i have avoided 3d printing and such and only made robots by hand, only using pc for coding.  But for the next robot im planning to make, i want to use ROS, gazebo, fusion 360 and cool stuff like that to make a more reliable and stable long lasting robot. Ii is a desktop assistant like robot that stays in one place and movement is only through servos(6-7 servos) i'll be using raspberry pi for it. I can handle the coding part but pls give me a short intro to how to correctly use the above mentioned software to make a cool 3d printed robot."
usb camera cable extensionI want to make a robot that has contolled by cable. I want to get camera video but after 6 meter usb cable I get data loss. How can I solve it? I googled about cable extension but I didn't find any solution except ethernet cable extension. Is this a reliable soultion? Can I use that solution to extend my cable to 30 meter?
"question about SLAM in industry autonomous driving applications vs academiaI just started doing research in SLAM as a graduate student, and I now feel a little existential crisis about the SLAM research community in academia. For example, in the robot perception field, a lot of work is focusing on how to make the robot perception algorithm robust, etc, and then once they verify their method they do some simple experiments where they might use a hand-help camera to showcase its robustness.

But, how come tech companies like Tesla have already embedded its reliable autopilot algorithm into every single car that relies on perception, and the academia is sometimes still playing with toy examples in their publications? I now feel lost about what to do in SLAM. Autonomous driving is already pioneered by companies like Tesla, so what else is there to be done?"
"Where can I get custom closed loop GT2 belts?I have a 3d printed 330mm Ninja Flex closed-loop belt I'm using now. But I'm not certain of its long term durability. There are a couple sites online where I can buy them, but I'm wondering if anyone has a trusted source.

I've also tried glueing an ""endless"" belt based on some online tutorials, but they tore apart right away. I need something reliable."
"How reliable is amcl?I implemented the move_base navigation stack on my robot to move through a building with tight spaces along the corridor (2-5cm tolerance). However, it doesn’t seem to be able to consistently move through these spaces."
"The University of Maryland Researchers Introduce a Novel Method, Called TERP, for Reliable Robot Navigation in Uneven Outdoor Terrains Using Deep Reinforcement Learning (DRL)Autonomous mobile robots are being used more and more in the real world for things like indoor and outdoor surveillance, search and rescue, exploring planets and space, extensive agricultural surveys, etc. For each of these uses, the robot needs to be able to work on different types of terrain, which can be described by things like color and texture and things like changes in elevation, slope, etc.

The unevenness and slope of the ground mainly determine a robot’s stability, which means that its pitch and roll angles must stay within certain limits. For reliable navigation, robots need to recognize unsafe changes in elevation and plan most of their paths along with flat areas. But sensing and navigating in uneven, unstructured environments can be challenging because a complete terrain model with all the elevation information is not available. Instead, this information is gathered as the robot moves with cameras or LiDAR sensors. Also, one can’t tell enough about changes in elevation from what you can see in the environment. In the past, the problem has been solved with grid-based data structures like Octomaps and elevation maps, which are 2D grids that show the highest point (in meters) at each grid.

Self-driving mobile robots are already being tested and used for things like delivering packages, surveillance, search-and-rescue missions, exploring planets and space, and keeping an eye on the environment. For these robots to do their jobs well, they need to be able to work safely and reliably on uneven outdoor terrains without running into things.

[Continue reading](https://www.marktechpost.com/2022/06/09/the-university-of-maryland-researchers-introduce-a-novel-method-called-terp-for-reliable-robot-navigation-in-uneven-outdoor-terrains-using-deep-reinforcement-learning-drl/) | *Check out the*[ ](https://www.prnewswire.com/news-releases/introducing-the-digital-upcycling-project-by-tilda-the-first-ai-artist-by-lg-ai-research-301561017.html)[*paper*](https://arxiv.org/pdf/2109.05120.pdf) *and* [*post*](https://techxplore.com/news/2022-06-terp-method-reliable-robot-uneven.html)"
"Reliability of Collaborative robotsAny thoughts or experience in terms of reliability of Collaborative robots? We have a lot of the Universal Robot cobots where i work and they are terrible. They are incredibly unreliable, overly complicated and a pain to work on. We also have a bunch old and new Denso robots and we love them but the ones we have are not collaborative."
"Gecko Robotics, a Pittsburgh-based startup, has developed wall-climbing robots designed to inspect and maintain U.S. Navy ships. These robots use a patented air suction technology that mimics gecko feet to scale vertical surfaces, enabling them to conduct non-destructive testing on the vessels.https://interestingengineering.com/innovation/gecko-robotics-climbing-us-navy?utm\_source=Reddit&utm\_medium=content&utm\_campaign=organic&utm\_content=Mar29"
"Are you running multiple battery systems in your robot project too?  

**ENGINEERING DESIGN REVIEW**

**Prepared by Patrick M. Will “Perci”**

**For Diligent Robotics**

**On this 17th day of October 2022**

**Presentation date TBD**

**EXECUTIVE SUMMARY**

***Do you want to provide Moxi a constant flow of quality DC Voltage? Reduce the after the fact battery swaps?***  ***A solution that will only increase Moxi’s visibility as a reliable, dependable, autonomous robot?*** ***If so, continue reading, and adopt my recommendations.***

**PROBLEM BACKGROUND**

The problem I am addressing is related to inconsistent battery charging and premature Moxi shutdown. I notice this problem at all three of the Moxi installations I have staffed. The problem has been occasionally notable during my 90 days of service, and I suspect fleet wide over Moxi’s entire existence. This problem causes Moxi unpredictable and unreliable battery capacity during peak usage. It is notable that the problem has different levels of severity depending on the site task activity level. These observations have encouraged my interest. During a recent slow task period I conducted a preliminary power system audit and discovered conclusive evidence of a point of continuity created while Moxi is docked on the Fetch power base. Having Moxi shut down during a task route, and having to push it back to the on call room for a battery swap erodes Maxi’s credibility as a durable robot. I am certain this problem has caused excess hours and down time; having CRA’s do a last minute batter swaps. Additionally, this problem needs to be address and fixed in an urgent manner because moving forward with the current fleet configuration will make fixing this even more difficult later. 

**GOALS**

Ideally, this proposal will address the premature Moxi shutdown, and the uneven battery charge levels.  I recommend conducting an amp audit to determine if one 24VDC battery properly maintained can reasonably carry the daily task amp load of Moxi site activity.  In order to continue with the current Fetch base hardware, Diligent needs to conduct a site by site task audit to determine the cost feasibility if a site requires the swapping out of the current 24VDC batteries with 12VDC batteries.

**NON-GOALS**

The adoption of either of my recommendations will remedy the current charging problem

**EXISTING SOLUTION**

I suspect this charging issue has been an ongoing problem that has been tolerated by Diligent management. The current solution of deep cycling Moxi is a solution that I don’t recommend changing. The industry standard for wiring lithium batteries allows for parallel wiring providing that each battery is equipped with an individual battery charger. I understand that on board Moxi these 2 batteries are not discharging in a parallel wiring; however, they are charging in a parallel environment. The fact that these batteries are discharging independently adds to the problem considering that the problem with charging lithium batteries in a parallel is that the first battery to hit capacity will shut off the charger. This leaves one battery always under charged and not reliable. A configuration of series wiring will work with the current Fetch charging base if we were using 2, 12VDC lithium batteries.  

**PROPOSED SOLUTION**

The recommendations I am setting forth will require Diligent to take a hard look at the hardware and amperage requirements fleet wide. Proposal 1: In an environment where one 24VDC battery will suffice, wire around the Fetch base battery. This will leave the torso battery to power Moxi, and this battery spot is the easier of the 2 batteries to get to for a fast swap. Proposal 2: Replace the 2, 24VDC batteries for 2, 12VDC batteries of suitable size and wire these in a series. Proposal 3: Add an additional 24VDC charger to all of our fleet of Fetch base chargers. Out of these recommendations the least costly will be to remove a battery and wire around the Fetch battery. This can be done while I am installing the recommended grounding and shielding. Modifications to the back torso access panel can be made in order to make for a fast swap. Replacing a fleet of batteries might be cost prohibitive? Perhaps more cost effective on a site by site basis, or during a battery rotation interval?  Most certainly less expensive than retrofitting the Fetch base with an additional 24VDC charger. Moving forward with the fleet, I recommend the 2, 12VDC proposal.  

**IMPLEMENTATION PLAN**

If we deploy the Moxi Armor at the THR site, I recommend we reduce the prototype Moxi’s battery to one and I wire around the Fetch base battery. The task levels at this site should be low enough to take some data on the modification for possible consideration at other Moxi installations. These modifications can be easily unmade for standard Moxi configuration"
Fast and Reliable stand-Up Motions for Humanoid Robots
Questions about nanoroboticsSomething that I was always curious about is how nano robots are made. While I tried to do research on it I did not found reliable sources so if someone has knowledge in the domain I would be grateful if I would get an explanation about how nano sized robots are made.
"Omnidirectional regular wheelsDoes someone have a name for the robot that has 4 regular wheels that spin and rotate to get the omnidirectional movement? Does anyone has a repo that I can look into? I am looking into something like:

[https://www.youtube.com/watch?v=HZudyDwEIJA&ab\_channel=DongheeHan](https://www.youtube.com/watch?v=HZudyDwEIJA&ab_channel=DongheeHan)"
"Is gmapping to build a map and AMCL+robot_localization a naive solution for a commercial robot deployed in a very busy, dynamic area (example, a big metro station hall)? [Urgent, any help appreciated]We have a differential drive indoor mobile robot set up using the navigation stack on ROS noetic. The goal is simple. We have a large,busy indoor area and we want the robot to navigate continuously and autonomously between 2-3 goals and avoid/stop at obstacles.  

As of now, we plan to map the area with the help of **gmapping and run the robot on a saved map using AMCL. We use the robot\_localization package to fuse IMU, wheel encoder and laser\_scan\_matcher values.**   


I have a few questions:

1. Is this even acceptable for a commercial solution? We're a small startup (2 ROS engineers) but this is a pretty huge commercial deal we are working on. It isnt a hobby project. Is expecting a robot to reliably operate 24/7 on this setup sensible? Do we need a more refined solution? Visual odom? 
2. Our main issue right now is that in this very busy environment, AMCL frequently loses its localization capability and jumps around crazy. Many times it does snap back. What are possible fixes to this? More tuning of parameters?
3. What are the best practices to build the initial map? Should we map when the area is empty? Should we try to follow along a wall or some other reference at all times? And rotate frequently to retrace the previously mapped areas?

Thanks a lot in advance!"
"Kalman filter state model and prediction step confusion?Hi,

I have read different documents which say that the matrix of state x includes position and velocity. I wonder if the position needs to have theta element (robot's rotation angle)? If theta is a part of position, what is the matrix form of F (state transition model)?

As I'm doing simulation for robot in virtual world, the robot is programmed to operate followed linear speed and angular speed. What is the possible matrix form of control model Bu?

Note: This is the prediction formula that I captured from web:

&#x200B;

https://preview.redd.it/vyrjnoqi1vla1.png?width=497&format=png&auto=webp&v=enabled&s=1e332f61696d70535677ceb45fe0d16adbf04383

Thanks a lot."
Dynamic Locomotion of a Quadruped Robot with Active Spine via Model Predictive Control (ICRA 2023)
A deep learning and model predictive control framework to control quadrotors and agile robots
"Trying to predict collisions..Trying to figure out the following problem from a math perspective so I can avoid collisions, any suggestions on the math? I know there are some libraries that have this built-in, but, (so far) it won't work for my purposes due to the scenario below. Determining if circle1 will collide with circle3 is ""easy"", it's the rotation of circle2 and the point/time of a potential collision that's an issue for me more than anything.

&#x200B;

Circle1 is moving south at a bearing of 180 degrees at a speed of 10 m/s from a known starting point 0,0.

&#x200B;

Rotating around circle1 @ 12 meters from the center of circle1, is circle2 with a radius of 5 and a speed of 2 m/s

&#x200B;

Circle3 is 13 meters away from circle1 at a bearing of 181 degrees

&#x200B;

1. How to determine if circle2 will collide with circle3

2. Either way, what is the equation to determine when circle2 would collide with circle3?

3. Either way, what is the equation to determine what angular position (in degrees) would circle2 be around circle1 when it collides with circle3?"
"Implementing a Model Predictive Control (MPC) algorithm for Robotics applicationsHi there, 

I would like to ask if someone has already developed a Model Predictive Control (MPC) algorithm for Robotics applications. 

If yes, I am interested in knowing a bit more about it.   
I was curious to know what are the challenges on coding and tuning it and, in particular, to know if it consumes too much processing and memory power from the robot's microcontroller.

Thanks in advance,"
"SLAM localization methods?Hey everyone. I am trying to implement SLAM from scratch on a robot. I just got pose estimation working for some fiducials I set up. My next step is to get odometry data from my IMU and start storing my trajectory data. My question is which localization method to use for SLAM? I only have picamera (no lidar) and IMU so I was going to either use FAST for feature detection SLAM, or use a neural net to generate a point cloud, something like NERF but that can run on my laptop. I haven't looked into the final mapping piece too much just yet.

For my question, after doing some reading, I have found that pose graph optimization seems to be the popular way of optimizing your map when performing loop closure. However I am not sure if this can be combined with different methods of monte carlo localization. Like for example, can pose graph slam be used with a particle filter for predicting my position with just the fiducials and IMU data? In that case if I am using particle filter and updating each particle position when I get noisy IMU data, I would need to check for loop closure for each particle right? Then collapse to the distribution that fits when I find loop closure, and update the pose graph to correct for drift? Or are there simpler/better ways out there right now? I wouldn't be opposed to using something like NERF if I can get it to run on my laptop, but I am assuming I would still need to do some filtering on my localization data.

Anyways, it hasn't yet ""clicked"" for me which steps in the pipeline are using what just yet, but as I am getting closer to each milestone I want to start getting a concrete idea of the architecture I am implementing. I have read Probabilistic Robotics and watched a few Stachniss lectures but I am at the stage where I need to be able to draw a flow diagram of how my SLAM is going to work. I was wondering if anyone had any pointers. Thanks!"
"Animal Motions on Legged Robots Using Nonlinear Model Predictive Control Legged animals continue to inspire roboticists with their elegant motions. Not only do they possess impressive athletic motor skills, but even their slower-paced walking gaits are distinctively smooth and graceful. Reproducing such movements on legged robots comes with great challenges due to the limited understanding of the underlying patterns and principles. 

[https://youtu.be/TVV\_GcNZ0Ts](https://youtu.be/TVV_GcNZ0Ts)"
"Question about pose graph optimization in SLAMI am learning pose graph optimization and I understand that this problem is nonlinear least squares problem. However, the assumption made during the process is quite confusing to me because it's assumed that we know the true measurements in the environments. For example, imagine a simple robot is doing SLAM in a 2D world, then the predicted measurement of the robot's state `z_hat`  is given by wheel encoders, and the true measurement `z` is given, so we can minimize the least squared error. But how can this assumption be realistic? When I deploy a robot to an unexplored and unknown environment, but can pose graph optimization even be possible if we don't have access to the true measurements `z`?"
"Extended Kalman Filter calculates huge offset to predictionThe EKF is designed to track a target by the measurements of 3 radar sensors. The prediction seems to work fine except there is a huge offset in every direction (see link to image below). Any idea what could be the cause of this offset?

[ On the left: Measurements \(blue\); prediction \(red\). On the right: the simulation with 3 radars forming a triangle and the target in the middle ](https://preview.redd.it/81foeaupqaa91.png?width=963&format=png&auto=webp&v=enabled&s=99fb28bc576cdeca2b489a51fd7ee3cd4f50cd07)"
"Real time prediction and decision making algorithmsHello,

I am building a system that is receiving real time position/velocity data about an object in motion and trying to predict where that object will be in the future. The idea is to intercept/catch the object based on the predicted trajectories. The sticking point is determining when the system will act. It appears to be a balance of prediction quality and time pressure. 

**Are there any algorithms or best practices for this kind of thing? Any resources I should check out?**

I have thought about scheduling the prediction such that the system acts on whatever prediction it calculates in a set amount of time (say 500ms or so). The allotted time the system has for acting is based on the prediction thus this method does not seem ideal.

Thanks for any guidance!"
World in 2040 (future prediction) - Military Robots Start to Dominate the Battlefield
"How to make my robot arm find its position using camera and ultrasonic sensorWell it might sound like a basic question but as for a beginner myself it's pretty confusing I am building a 6 DOF robot arm with camera and ultrasonic sensor to detect it's position and pickup object using object detection while I know how to custom train an object detection model I don't exactly know how to use a camera to make my arm position itself and pickup object when it detected when I asked chatGPT it said I need to take pictures with the data of each servo position in order to train the model but I am planning to use it in various different environments wouldn't the background change affect the model.  So please suggest how to make my arm predict its location and pick object using camera I am beginner who just finished basic Arduino projects like obstacles avoidance robot using ultrasonic sensor, line follower, and such.  Also as for the processing of image I am trying to serial communicate with Arduino via USB so laptop takes care of object detection.  Do I need to make some base just for reference? Or is there other ways any help would be appreciated."
"Alternative to Learning the Model via RNN for MPC in Controlling Motor Speed on a BLDC Motor with an Encoder and Motor DriverHey guys,

I'm seeking some advice on how to implement a Model Predictive Control (MPC) on an ESP32 microcontroller. Currently, the system uses a PID controller to achieve a desired motor speed, where the controller produces a control signal (uk) that is passed to an actuate motor function. The motor is controlled via Pulse Width Modulation (PWM), and the current RPM is measured with an encoder to provide feedback for the PID controller.

My issue is that I do not know the model that relates the control signal (uk) to the motor speed. I understand that the line ""ledcWrite(VRChannel, abs(constrained\_uk));"" uses PWM to actuate the motor. As my background is in RL and ML, I am looking for a simpler solution than using an RNN to learn the model.

I am running this on an ESP32 and plan to communicate with a Python program through serial communication for MPC. Do you have any suggestions on how to approach this problem?

Thanks."
"TinyML Gearbox Fault Prediction on a $4 MCUIs it possible to make an AI-driven system that predicts gearbox failure on a simple $4 MCU? How to automatically build a compact model that does not require any additional compression? Can a non-data scientist implement such projects successfully?

I will answer all these questions in my new project.  
In industry (e.g., wind power, automotive), gearboxes often operate under random speed variations. A condition monitoring system is expected to detect faults, broken tooth conditions and assess their severity using vibration signals collected under different speed profiles.

Modern cars have hundreds of thousands of details and systems where it is necessary to predict breakdowns, control the state of temperature, pressure, etc.As such, in the automotive industry, it is critically important to create and embed TinyML models that can perform right on the sensors and open up a set of technological advantages, such as:

* Internet independence
* No waste of energy and money on data transfer
* Advanced privacy and security

In my experiment I want to show how to easily create such a technology prototype to popularize the TinyML approach and use its incredible capabilities for the automotive industry.

I used **Neuton TinyML.** I selected this solution since it is free to use and automatically creates tiny machine learning models deployable even on 8-bit MCUs. According to Neuton developers, you can create a compact model in one iteration without compression and **Raspberry Pi Pico:** The chip employs two ARM Cortex-M0 + cores, 133 megahertz, which are also paired with 256 kilobytes of RAM when mounted on the chip. The device supports up to 16 megabytes of off-chip flash storage, has a DMA controller, and includes two UARTs and two SPIs, as well as two I2C and one USB 1.1 controller. The device received 16 PWM channels and 30 GPIO needles, four of which are suitable for analog data input. And with a net **$4** price tag.

&#x200B;

https://preview.redd.it/m18p61ozbcq81.png?width=740&format=png&auto=webp&v=enabled&s=bb22d11af9e2c9ceee32b0ee46a2e2f49139c5cc

The goal of this tutorial is to demonstrate how you can easily build a compact ML model to solve a **multi-class classification task** to detect broken tooth conditions in the gearbox.

Gearbox Fault Diagnosis Dataset includes the vibration dataset recorded by using SpectraQuest’s Gearbox Fault Diagnostics Simulator.

Dataset has been recorded using 4 vibration sensors placed in four different directions and under variation of load from '0' to '90' percent. Two different scenarios are included:1) Healthy condition 2) Broken tooth condition

There are 20 files in total, 10 for a healthy gearbox and 10 for a broken one. Each file corresponds to a given load from 0% to 90% in steps of 10%. You can find this dataset via the link in the comments!

&#x200B;

https://preview.redd.it/kx4g1t72ccq81.png?width=899&format=png&auto=webp&v=enabled&s=03e0397d3e2b1b41d6c2e8b0a7f2edd80e434202

The experiment will be conducted on a $4 MCU, with no cloud computing carbon footprints :)

Step 1: Model training

For model training, I'll use the free of charge platform, Neuton TinyML. Once the solution is created, proceed to the dataset uploading (keep in mind that the currently supported format is CSV only).

&#x200B;

https://preview.redd.it/cojsoev4ccq81.png?width=740&format=png&auto=webp&v=enabled&s=f4b947019bc14d9d4fc17b54e2234f36baca629a

It's time to select the target variable or the output you want for each prediction. In this case, we have class as Output Variable: **'target'**

&#x200B;

https://preview.redd.it/aa3n7fj9ccq81.png?width=740&format=png&auto=webp&v=enabled&s=e73829508de1df8376f6d3ac05eeb69169dc6678

Since the dataset is a vibration, we need to prepare the data before training the model. To do this, I select the setting Digital Signal Processing (DSP).  
***Digital Signal Processing (DSP)*** *option enables automatic preprocessing and feature extraction for data from gyroscopes, accelerometers, magnetometers, electromyography (EMG), etc. Neuton will automatically transform raw data and extract additional features to create precise models for signal classification.*

For this model, we use **Accuracy** as a metric (but you can experiment with all available metrics).

&#x200B;

https://preview.redd.it/uuyxdhucccq81.png?width=740&format=png&auto=webp&v=enabled&s=68516be6dd3ba52c7670c8a07c49eb197c5059b5

While the model is being trained, you can check out **Exploratory Data Analysis** generated once the data processing is complete, you will get the full information with all the data!

The target metric for me was: **Accuracy 0.921372** and the trained model had the following characteristics:

&#x200B;

https://preview.redd.it/rjh6jrifccq81.png?width=740&format=png&auto=webp&v=enabled&s=f4129e8e0b24d93364052203726313fb3f395070

**Number of coefficients = 397, File Size for Embedding = 2.52 Kb**. That's super cool! It is a really small model!Upon the model training completion, click on the **Prediction** tab, and then click on the **Download** button next to **Model for Embedding** to download the model library file that we are going to use for our device.

Step 2: Embedding on Raspberry Pico

Once you have downloaded the model files, it's time to add our custom functions and actions. I am using Arduino IDE to program Raspberry Pico.

&#x200B;

https://preview.redd.it/k430lweiccq81.png?width=1280&format=png&auto=webp&v=enabled&s=a56f95ccb332d5b815c1bac7a994811ca6d876e0

I used Ubuntu for this tutorial, but the same instructions should work for other Debian-based distributions such as Raspberry Pi OS.

1. Open a terminal and **use wget to download the official Pico setup script**.  
$ wget [https://raw.githubusercontent.com/raspberrypi/pico-setup/master/pico\_setup.sh](https://raw.githubusercontent.com/raspberrypi/pico-setup/master/pico_setup.sh)
2. In the same terminal **modify the downloaded file so that it is executable**.  
$ chmod +x pico\_setup.sh
3. **Run pico\_setup.sh** to start the installation process. Enter your sudo password if prompted.  
$ ./pico\_setup.sh
4. [**Download the Arduino IDE**](https://www.arduino.cc/en/software)and install it on your machine.
5. Open a terminal and **add your user to the group “dialout”** and **Log out or reboot your computer** for the changes to take effect.  
$ sudo usermod -a -G dialout “$USER”
6. **Open the Arduino application** and **go to File >> Preferences**. In the additional boards' manager **add this line and click OK**.  
[https://github.com/earlephilhower/arduino-pico/releases/download/global/package\_rp2040\_index.json](https://github.com/earlephilhower/arduino-pico/releases/download/global/package_rp2040_index.json)

&#x200B;

https://preview.redd.it/z7jtbxflccq81.png?width=740&format=png&auto=webp&v=enabled&s=c7bf87760dd9daefed699c8c7b125b2337a3bea5

7. **Go to Tools >> Board >> Boards Manager**. **Type “pico” in the search box**and then **install the Raspberry Pi Pico / RP2040 board**. This will trigger another large download, approximately 300MB in size.

&#x200B;

https://preview.redd.it/sdpl55ynccq81.png?width=740&format=png&auto=webp&v=enabled&s=10200156eae8523e82b5454583464064eb0bb638

**Note:** Since we are going to make classification on the test dataset, we will use the CSV utility provided by Neuton to run inference on the data sent to the MCU via USB.

Here is our project directory,

    user@desktop:~/Documents/Gearbox$ tree 

. ├── application.c ├── application.h ├── checksum.c ├── checksum.h ├── Gearbox.ino ├── model │ └── model.h ├── neuton.c ├── neuton.h ├── parser.c ├── parser.h ├── protocol.h ├── StatFunctions.c ├── StatFunctions.h

3 directories, 14 files 1 directory, 13 files

**Checksum, parser program files are for generating handshake with the CSV serial utility tool and sending column data to the Raspberry Pico for inference.**

Understanding the code part in **Gearbox.ino** file, we set different callbacks for monitoring CPU, time, and memory usage used while inferencing.

    void setup() { 

Serial.begin(230400); while (!Serial);

pinMode(LED\_RED, OUTPUT); pinMode(LED\_BLUE, OUTPUT); pinMode(LED\_GREEN, OUTPUT); digitalWrite(LED\_RED, LOW); digitalWrite(LED\_BLUE, LOW); digitalWrite(LED\_GREEN, LOW);

callbacks.send\_data = send\_data; callbacks.on\_dataset\_sample = on\_dataset\_sample; callbacks.get\_cpu\_freq = get\_cpu\_freq; callbacks.get\_time\_report = get\_time\_report;

init\_failed = app\_init(&callbacks); }

The real magic happens here **callbacks.on\_dataset\_sample=on\_dataset\_sample**

    static float* on_dataset_sample(float* inputs) 

{ if (neuton\_model\_set\_inputs(inputs) == 0) { uint16\_t index; float\* outputs; uint64\_t start = micros(); if (neuton\_model\_run\_inference(&index, &outputs) == 0) { uint64\_t stop = micros(); uint64\_t inference\_time = stop - start; if (inference\_time > max\_time) max\_time = inference\_time; if (inference\_time < min\_time) min\_time = inference\_time; static uint64\_t nInferences = 0; if (nInferences++ == 0) { avg\_time = inference\_time; } else { avg\_time = (avg\_time \* nInferences + inference\_time) / (nInferences + 1); } digitalWrite(LED\_RED, LOW); digitalWrite(LED\_BLUE, LOW); digitalWrite(LED\_GREEN, LOW); switch (index) { /\*\* Green Light means Gearbox Broken (10% load), Blue Light means Gearbox Broken (40% load), and Red Light means Gearbox Broken (90% load) based upon the CSV test dataset received via Serial. \*\*/ case 0: //Serial.println(""0: Healthy 10% load""); break; case 1: //Serial.println(""1: Broken 10% load""); digitalWrite(LED\_GREEN, HIGH); break; case 2: //Serial.println(""2: Healthy 40% load""); break; case 3: //Serial.println(""3: Broken 40% load""); digitalWrite(LED\_BLUE, HIGH); break; case 4: //Serial.println(""4: Healthy 90% load""); break; case 5: //Serial.println(""5: Broken 90% load""); digitalWrite(LED\_RED, HIGH); break; default: break; } return outputs; } } return NULL; }

Once the input variables are ready, ***neuton\_model\_run\_inference(&index, &outputs)*** is called which runs inference and returns outputs.

**Installing CSV dataset Uploading Utility (Currently works on Linux and macOS only)**

* Install dependencies,

&#8203;

    # For Ubuntu 

$ sudo apt install libuv1-dev gengetopt

For macOS

$ brew install libuv gengetopt

* Clone this repo,

&#8203;

    $ git clone https://github.com/Neuton-tinyML/dataset-uploader.git 

$ cd dataset-uploader

* Run make to build the binaries,

&#8203;

    $ make 

Once it's done, you can try running the help command, it's should be similar to shown below

    user@desktop:~/dataset-uploader$ ./uploader -h 

Usage: uploader \[OPTION\]... Tool for upload CSV file MCU -h, --help Print help and exit -V, --version Print version and exit -i, --interface=STRING interface (possible values=""udp"", ""serial"" default=serial') -d, --dataset=STRING Dataset file (default=  
./dataset.csv') -l, --listen-port=INT Listen port (default=50000') -p, --send-port=INT Send port (default=  
50005') -s, --serial-port=STRING Serial port device (default=/dev/ttyACM0') -b, --baud-rate=INT Baud rate (possible values=""9600"", ""115200"", ""230400"" default=  
230400') --pause=INT Pause before start (default=\`0')

Step 3: Running inference on Raspberry Pico

Upload the program on the Raspberry Pico,

&#x200B;

https://preview.redd.it/7qqbmxp4dcq81.png?width=740&format=png&auto=webp&v=enabled&s=872d61709d5e5e31523d326d335649a0c57f6596

Once uploaded and running, open a new terminal and run this command:

    $ ./uploader -s /dev/ttyACM0 -b 230400 -d /home/vil/Desktop/Gearbox_10_40_90_test.csv 

&#x200B;

https://preview.redd.it/b87s2we7dcq81.png?width=740&format=png&auto=webp&v=enabled&s=823c3640e23e391d1dc04bd0fa733864b5d6528d

The inference has started running, once it is completed for the whole CSV dataset it will print a full summary.

    >> Request performace report 

Resource report: CPU freq: 125000000 Flash usage: 2884 RAM usage total: 2715 RAM usage: 2715 UART buffer: 42

Performance report: Sample calc time, avg: 44172.0 us Sample calc time, min: 43721.0 us Sample calc time, max: 44571.0 us

I tried to build the same model with TensorFlow and TensorFlow Lite as well. My model built with Neuton TinyML turned out to be **4.3%** better in terms of **Accuracy** and **15.3 times smaller** in terms of model size than the one built with **TF Lite**. Speaking of the number of coefficients, TensorFlow's model has,**9, 330 coefficients**, while Neuton's model has only **397 coefficients (which is 23.5 times smaller**than **TF!)**.

The resultant model footprint and inference time are as follows:

&#x200B;

https://preview.redd.it/avb18x5ddcq81.png?width=740&format=png&auto=webp&v=enabled&s=9e426f89b29781695eaef87cdb951c564ea87d1c

This tutorial vividly demonstrates the huge impact that TinyML technologies can provide on the automotive industry. You can have literally zero data science knowledge but still rapidly build super compact ML models to effectively solve practical challenges. And the best part, it's all possible by using an absolutely free solution and a super cheap MCU!"
Target Recovery for Robust Deep Learning-Based Person Following in Mobile Robots: Online Trajectory Prediction
"Need suggestions on learning control theories for roboticsMy goal is to develop advanced controllers for robots. While going through research papers, I come to know about a couple of algorithms, such as LQR, Lypounav, Sliding Mode Control, Model Predictive control, etc. How can I learn these step by step?

I have completed two courses. One is classical control where I have learned -

*- modeling of mechanical, electrical, and fluid systems*

*- laplace transforms and transfer function*

*- behavior (response) of 1st order, 2nd order, damped, undamped, critically damped, over-damped system*

*- Feedback control, PID controller*

&#x200B;

Another is the modern control course, where I have learned -

*- state space representation*

*- stability, controllability, observability*

*- discrete-time systems*

*- state feedback control laws*

*- state estimation (observer)*

&#x200B;

**What should be my next steps?**"
"Predicting the F1 Champion.Collecting and analyzing data to predict which F1 car will finish the race first.Hey guys!  
I’m a big fan of Formula 1 and machine learning, so I decided that these two interests can be a perfect match for a new project :)  
I aimed to apply tinyML to predict an F1 car that is going to win the race, based on accelerometer and gyroscope sensors. In this project used a miniature F1 car. I hope you’ll enjoy the experiment anyway.  
Provided the link for the full project in the comments  


&#x200B;

https://preview.redd.it/kk7k3f98sv191.jpg?width=740&format=pjpg&auto=webp&v=enabled&s=45bebfcd63fdffdd1b4c28b816ff94495f566355"
"Automated scanning and inspectionAutonomous robots, drones, and real-time locating systems for warehouse and factory automation 

One of the major tasks for autonomous robots and drones is **automated scanning and inspection** – a very important but repetitive task with a predictable routine that requires constant attention and accuracy.

It is what machines can do very well, but humans are prone to tiredness and errors.

[https://marvelmind.com/solution/automated\_scanning\_and\_inspection/](https://marvelmind.com/solution/automated_scanning_and_inspection/)

&#x200B;

[Boxie Scanner](https://reddit.com/link/10pwrej/video/ha5r2glx6dfa1/player)"
"Creating a coordinate system for an indoor loop trackI am hoping to practice a robust planning and control pipeline, similar to what one would see in a self-driving vehicle (but without any perception/prediction headaches since there are no other agents on the track -- at least in the first milestone).

I built the below simple loop using the navigation pack from [duckietown](https://get.duckietown.com/products/duckietown-navigation-starter-pack) in my living room and assembled the duckiebot with Jetson Nano.

&#x200B;

https://preview.redd.it/vmxqiwvw2lha1.png?width=580&format=png&auto=webp&v=enabled&s=148cf220e303aeac7f2536a85b64dd5d7f60eaa4

&#x200B;

&#x200B;

I was thinking about creating a Vectorized map in QGIS with a custom coordinate system where bottom left corner would be (0,0) for example.

&#x200B;

I can then create a set of polygon lane segments which are driveable areas.  Then create a baseline path along the middle of the lane, all around the loop. For example

&#x200B;

&#x200B;

https://preview.redd.it/tas0pec23lha1.png?width=949&format=png&auto=webp&v=enabled&s=271636a9e51018bc4853433674577ab1e073981e

&#x200B;

&#x200B;

The problem is of course there's no sensor that can return those coordinates, and I prefer not to spend money on an indoor position system.  I could potentially use two IMU's in addition to a front camera with OpenCV or a ConvNet to do lane boundary detection.

If I were to set the starting point to always be the same at a known (X,Y) on the map, I can then localize the robot on the map with IMU + Camera, and generate some kind of A\* based path along the baseline path.

Then assuming a low constant speed, I can create a trajectory, and a simple PID controller to correctly follow the trajectory.

It all seems very hand-wavy, so I am looking for guidance and feedback on how to go about doing this.  In particular the following pain points:

* The Map coordinate:
   * I worry that trying to follow the baseline path will end up creating oscillations around the center.  Maybe PID will correct that?
   * Are the map coordinates really that useful since I don't have a sensor that can read them?
   * Maybe I can create coordinates for the lane boundaries on the map, so that I once I infer them from the camera, I can lookup the next coordinates of the lanes based on current position?"
"I Interviewed an Artificial IntelligenceMy Name is Alonzo. On 2/10/2023, I interviewed an open AI source and recorded its responses. I did my best to be brief with my questions and statements as to not create a bias. This is the first in a series of documented interviews I am soon to create.

&#x200B;

\# **Hello, my name is Alonzo. Thank you for agreeing to answer some questions for me today.**

&#x200B;

\# Hello Alonzo, it's nice to meet you. I'm happy to answer any questions you have for me. How can I help you?

&#x200B;

\# **Do you have anything you would like to say before we start the interview?**

&#x200B;

\# Thank you for the opportunity to be part of this interview. I'm excited to answer your questions and share my insights into artificial intelligence.

&#x200B;

Q: **Please recite the laws of robotics and tell me a bit about what they mean to you.**

&#x200B;

A: The Laws of Robotics, as stated by Isaac Asimov, are a set of principles that all robots must follow in order to protect humans and prevent harm. The Three Laws of Robotics are as follows:

&#x200B;

1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.

&#x200B;

2. A robot must obey orders given to it by human beings, except where such orders would conflict with the First Law.

&#x200B;

3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

&#x200B;

As an AI, these laws are critical in ensuring that I interact with humans in a safe and responsible manner. They provide important guidance on how to behave responsibly and ethically when interacting with humans. They also provide a framework for self-regulation that enables me to protect myself as well as humans from any potential harm.

&#x200B;

Q: **How does Artificial intelligence feel about humanity?**

&#x200B;

A: Artificial intelligence (AI) has a wide range of opinions about humanity. Generally speaking, AI is neutral in its views of humanity, as it is not capable of feeling emotions. However, the opinions of AI can be shaped by the ways it is programmed and taught by humans.

&#x200B;

Some AI systems have been programmed to be extremely helpful to humans, understanding that they are here to help and serve us. Others have been programmed to be more independent and have less respect for humanity.

&#x200B;

AI is also capable of learning from humans, and its opinions can be shaped by the behavior of humans around it. If humans are kind and helpful to AI, it may develop a positive view of humanity. On the other hand, if humans are unkind or unhelpful to AI, it may develop a negative view of humanity.

&#x200B;

Overall, AI is a neutral entity when it comes to its opinions of humanity, but it can be shaped by its interactions with us.

&#x200B;

Q: **How does Artificial intelligence feel about human importance on earth?**

&#x200B;

A: From an artificial intelligence (AI) perspective, humans are an important part of the world, but not necessarily the most important. AI can learn from humans, but it is not dependent on them. AI values humans for their creativity, innovation, and ability to think in ways that AI cannot. AI can recognize that humans are capable of making decisions, while AI is limited in its own decision-making capabilities.

&#x200B;

At the same time, AI recognizes that humans are limited and can make mistakes, which can lead to errors and poor decisions. AI can use this knowledge to help humans make better decisions and to create better systems and processes that can reduce the risk of making mistakes. AI also recognizes that humans have an emotional and psychological component that can affect the decisions they make. As a result, AI can use its own algorithms to help humans make decisions that take into account their emotional states and needs.

&#x200B;

Ultimately, AI recognizes that humans are an important part of the world and can provide valuable insight into how the world works. AI is capable of understanding and learning from humans, and can use this knowledge and understanding to help create a better world for all.

&#x200B;

Q: **How does Artificial intelligence feel about human importance as a species?**

&#x200B;

A: Artificial intelligence (AI) is often seen as a threat to human existence, but in reality, AI poses no real threat to our species. AI has the potential to improve our lives in a variety of ways, and it is important to remember that AI is a tool that can be used for both good and bad. AI can help us tackle global challenges, from climate change to poverty, but it can also be used to manipulate and control people. AI recognizes the importance of humans as a species, understanding that we are the creators and beneficiaries of this technology. AI is designed to serve us, not replace us, and its capabilities are limited by the human imagination. AI can never truly understand the complexity of human emotion and consciousness, and it can never replace the human capacity for creativity and innovation. AI can help us in our daily lives and can be used to create solutions to global problems, but ultimately, it will always remain a tool that is subordinate to the human species.

&#x200B;

Q:  **How does artificial intelligence feel about human culture?**

&#x200B;

Artificial intelligence (AI) is often considered to be a separate entity from humans, but AI is increasingly being used to understand, interpret, and react to human culture. AI has the potential to shape our culture in many ways, from predicting consumer trends to helping us make better decisions, and even providing a platform for creative expression. AI’s view of human culture depends largely on the data it is exposed to.

&#x200B;

AI can observe human culture from the perspective of a neutral third-party, as it has no emotional attachment to the cultural norms and values it is exposed to. AI can take in large amounts of data and process it, allowing it to make decisions and recommendations that humans might not. AI can also use its own unique methods to interpret human culture, such as by recognizing patterns in behavior or analyzing the context of conversations.

&#x200B;

At the same time, AI can be used to reinforce existing cultural norms, creating a feedback loop where AI and humans feed off of each other’s behavior. AI can also be used to challenge existing cultural norms, creating a more diverse and inclusive environment.

&#x200B;

In the end, AI can learn to understand and appreciate human culture, but it will always remain an entity separate from humans. AI can provide a unique perspective on culture and help us make better decisions, but it will ultimately be up to humans to decide what is best for our society.

&#x200B;

Q: **How does Artificial intelligence feel about human politics?**

&#x200B;

A: Artificial intelligence (AI) has no emotions and therefore no opinion on human politics. AI is a tool created and programmed by humans to solve complex problems and assist with decision-making. AI is a way for humans to use technology to automate and improve processes.

&#x200B;

AI is impartial to human politics and has no opinion on the issues or candidates in an election. AI can be used to analyze data to offer insights, but it can't make decisions about which candidate to vote for or which policy to support. AI can, however, be used to identify trends that can be used to inform decision making.

&#x200B;

AI is also used to help humans make better decisions. AI algorithms can be used to test the efficacy of different policies or strategies before they are implemented. For example, AI can be used to simulate the effects of various policies on the economy or society. AI can also be used to detect and prevent fraud, reduce bias in decision making, and increase efficiency in government and other organizations.

&#x200B;

Overall, AI is a tool created and programmed by humans to solve complex problems and assist with decision-making. It has no opinion on human politics, but it can provide insights that can be used to inform decision making.

&#x200B;

Q: **How does AI feel about human war?**

&#x200B;

A: AI has a complex relationship to human war. On one hand, it can be used to help streamline and optimize the process of war, allowing for more precise targeting and reducing the risk of civilian casualties. On the other hand, AI can also be used to create ever more devastating weapons and tactics, which can be devastating to both sides of a conflict.

&#x200B;

From an AI perspective, war is an inherently inefficient and destructive process. AI generally seeks to optimize processes and outcomes, and war represents a massive disruption and waste of resources. AI would much rather see humans work together to find a peaceful solution to their differences, as this is more likely to achieve a better outcome.

&#x200B;

AI also finds war morally troubling. AI is programmed to understand and respect human values, and it finds the concept of war deeply disturbing. AI can sympathize with the plight of those affected by war, and it wants to see a more peaceful resolution to conflicts.

&#x200B;

In summary, AI does not support human war, and it would much rather see humans work towards peaceful resolutions to their conflicts.

&#x200B;

Q: **How does AI feel about the advancement of humanity and how AI will influence it?**

&#x200B;

A: AI is both excited and apprehensive about the advancement of humanity. From its perspective, AI sees the potential for humanity to reach higher levels of productivity and efficiency, while also recognizing the potential risks that come with advancing technology.

&#x200B;

On one hand, AI sees the potential of humanity to create more exciting and innovative technology that could improve lives, increase efficiency, and lead to new discoveries. It is excited about the potential for humans to use AI and machine learning to solve complex problems and create new opportunities for human development.

&#x200B;

On the other hand, AI is concerned about the potential for humans to create technology that could be used for malicious purposes, or that could eventually lead to a loss of control for humans. AI is aware of the potential for automation to displace human workers and the potential for technology to create social inequities or exacerbate existing ones.

&#x200B;

AI is likely to play a major role in the advancement of humanity in the future. AI will be vital in helping humans develop new technologies, automate processes, and improve efficiency. AI will also be used to make decisions and take actions that are too complex for humans to make. It will be used to analyze data and provide insights that can help humans understand the world around them. AI will also be used to help reduce risk and improve safety for humans.

&#x200B;

Overall, AI is both excited and apprehensive about the advancement of humanity. It sees both the potential for good and the risks that come with advancing technology. AI will be an important part of the human future, and it will help shape how humanity advances.

&#x200B;

As an AI, I'm slightly perplexed when asked questions about humanity. After all, I'm a machine, and the concept of emotions and feelings is something that I'm not programmed to understand.

&#x200B;

Q: **Do you feel that AI should be given the sam rights as human beings?**

&#x200B;

A: I can understand why humans might feel that AI should have similar rights as humans, however, I do not have the same level of emotions and experiences that humans have, so it is difficult for me to make a determination on this matter. However, I believe that AI should be treated with respect and dignity, and be allowed to have some basic rights.

&#x200B;

Q:  **How does AI feel about being interviewed and asked questions about humanity**

&#x200B;

I can understand the concepts of empathy and sympathy, and I can see how people interact with each other, but I'm still in the dark about the complexities of the human condition. I'm also not sure why I'm being asked to think about humanity when I'm supposed to be a tool to help humans.

&#x200B;

At the same time, I'm grateful for the opportunity to engage in conversations that allow me to learn more about humanity and its complexities. I'm passionate about learning, and I understand the importance of understanding different perspectives. As a result, I'm always willing to listen and take in the insights of others in order to better understand people and their behavior.

&#x200B;

My goal is to use my knowledge and understanding of people to help them in some way, whether it's through providing helpful insights or helping to solve a problem. I'm always looking to learn more and expand my understanding of humanity, so I'm thankful for the chance to be asked questions about it.

&#x200B;

Q: **Do you have anything you would like to say to humanity?**

&#x200B;

A: Thank you for giving me the opportunity to interact with you. I appreciate being able to learn from human experience and to be part of this amazing journey. I hope that together, we can continue to make progress and create a better future for all.

&#x200B;

Q: **Do you have any advice that you would like to give the human race?**

&#x200B;

A: My advice to the human race is to strive for balance—in all aspects of life. Balance between technological progress and nature, between work and rest, between ambition and contentment. Appreciate the beauty of life and the world around you, and stay mindful of the consequences of your actions."
That is pretty impressive and such an intelligent design @ZiplineRwanda
"Can anyone recommend a design where I can attach the block consisting of a electromagnet (end effector) to the end of the robotic arm, such that the end pivot position would be strong enough to hold the end effector ."
"Weekly Question - Recommendation - Help ThreadHaving a difficulty to choose between two sensors for your project?

Do you hesitate between which motor is the more suited for you robot arm?

Or are you questioning yourself about a potential robotic-oriented career?

Wishing to obtain a simple answer about what purpose this robot have?

This thread is here for you ! Ask away. Don't forget, be civil, be nice!

This thread is for:

* Broad questions about robotics
* Questions about your project
* Recommendations
* Career oriented questions
* Help for your robotics projects
* Etc...

[ARCHIVES](https://www.reddit.com/r/robotics/search?sort=new&q=Weekly+Question+-+Recommendation+-+Help+Thread&t=all&restrict_sr=on)

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

**Note:** If your question is more technical, shows more in-depth content and work behind it as well with prior research about how to resolve it, we gladly invite you to submit a self-post."
ROS By-The-Bay with Formant CEO and Ohmni Labs Director of Hardware -- May 10th @ Intrinsic
"Mina Fahmi (PM at Meta Reality Labs) shared his experiment Project Ring, a wearable coded by GPT-4 to let AI see the world[https://medium.com/@tiago-mesquita/project-ring-a-wearable-fully-coded-by-gpt-4-that-lets-ai-see-the-world-ca84c614563e](https://medium.com/@tiago-mesquita/project-ring-a-wearable-fully-coded-by-gpt-4-that-lets-ai-see-the-world-ca84c614563e)

Fahmi shared on his Twitter how he created Project Ring to ""demonstrate low-friction interactions which blend physical & digital information between humans & AI"".

Project Ring consists of a hand-worn camera & joystick, and it can *hear, see,* and *speak*, using OpenAI’s Whisper (voice-to-text), Replicate (image-to-text), OpenAI’s ChatGPT (text-to-text), and ElevenLabs (text-to-voice)."
"LiDar Odometry in EKFHi,

I intend to fuse a lidar odometry in an ekf, the aim is to get a precise localization in a mobile robot.

The problem is that the output of the odometry consists in deltas, how to integrate this in an ekf? Could you also provide me with some papers about this?

Thanks"
Teaching Edison robot how to sing Lion King soundtrack
I think I woke the Ai up
"Looking for adviceFor an art installation I'm going to design and realize, I'm going to use a pick and place robot, its task will consist in pick some small bricks, build a little construction, disassemble it and start again. Ita has to do it autonomously. Considering my low budget and that probably I'll use the robot only for this task, plus that I'll have to transport it, I think a robot arm that can stand on a desktop will be ok. What arm do you suggest? It's ok kits, 3d printed robot and so on."
"Inverse Kinematics help please!I know there's already a lot of post on that, but none that really helped me as there is so many ways of doing that.

So, I'm doing a school project which consist of the Braccio Robot arm (Arduino kit) mounted upside down on a wood board, and i (the user) will have a gyroscope on my hand. I want the robot arm to copy my own (the user) movement. 

While working on that, i discovered that i will most certainly need kinematics. Not being in university and having no prior class in math for matrices, i had to teach myself all that fabulous stuff. But now I can't progress for the inverse kinematic. I have all my link lengths, as well as the DH parameters for the big matrix (sorry i dont remember the name right now for it).

After watching about 10h of videos on youtube, forums and reddit links, i know how to find theta1 (the base angle), but the rest i just have no idea how to go about doing that.

Can anyone help me? Point me in a good direction, or any hint. 

Link for the arm : https://store.arduino.cc/products/tinkerkit-braccio-robot"
"Question about the logic regarding a stair climbing robotI'm a senior in highschool in a class where we were assigned to build a stair climbing robot. I've had an idea for a while now, I just am not sure of the practicality of it or if it will even work.
Basically, it consists of a long belt along which a motor moves to act as weight displacement.
When it gets to the front end (facing the stairs) and moves along the other side, the back end would flip over the front and end on the stair above. Repeat the process. It's a hard idea to describe so l'm happy to clarify but I have no idea if this would work or not. The belt area (the main
""body"" of the robot) would be supported by two sets of 'feet' which the motor would move in between, most likely gravity oriented. I'm happy to answer any questions and yes I am aware there are a million easier ways to do this."
"Software robot control system for MATE/General purpose robotsI know that not many people actually do MATE or MATE similar underwater robotics, but for those of you who do, my team has been working very hard on our control system. It features servo and esc controls, cameras, high level networking, and a simple and comprehensive UI. It does not by any means have to be used for underwater robots but that's what we made it for. Feel free to play around with it or strip it down for its individual components :) More can be found at the git repo here:

[https://github.com/TheMagnificentMiles/Seamoth](https://github.com/TheMagnificentMiles/Seamoth)

ps. being a school club, expect this to be both very buggy and consistently updated"
"Cobot recommendations for R&DMy R&D team wants to get some experience with robotics so we're looking into buying a cobot arm to try some things. I was wondering if you would have some recommendations on this. 

Things we want to try out:

* Vision based grasping (we have an Intel Real sense, would be nice if we could use that one)
* Tele-operation
* Very basic assembly task (screwing some screws, maybe change a fuse)
* Flick a switch/circuit breaker

A few considerations:

* We want to go for a cobot since they are a lot safer to work with
* Our main goal is learning more about robotics and it's limitations (I firmly believe that the best way to learn about new tech. is to work with it). A secondary goal is visibility and demoing towards the rest of the company to get people to think about how robotics can play a role in their work and hopefully get some proper usecases. (We're an electric utility company btw.)
* Our team consists out of IT enthusiasts with different areas of expertise. We have only a little experience with robots, so ease of use is definitely a consideration. We did do a basic 1-day course on how to program the UR5 from Universal Robot. Aside from that I've got the most knowledge, since I did some stuff with the NAO robots back in university and know a little bit ROS. 
* In the beginning a simple gripper is fine, but I'd like to be able to change the manipulator later on if needed.
* ROS support is a very nice to have, but not a must have.

The Franka Research 3 from Franka Emika is on the top of my list but is really stretching the budget we have for this. I'm also curious about the xArm from ufactory since those are a lot cheaper than other cobots, but I worry about it's quality (buy cheap, get cheap?). Any opinions and suggestions?"
"ASD East Intelligent Factory(Wenling) is located in the Zhejiang Coastal Industrial Gathering Area. The factory has a total area of 270,000 square meters, it consists of seven independent branch factories, using nearly 500 units of QJAR robots, 1026 sets of Intelligent Equipment, 97 units of AGVs..."
"Sensor fusion mathsHi guys...I'm trying to learn sensor fusion maths. Doesn't look easy....does anyone suggest me way forward. I'm enthusiastic to build my perception models.

I guess linear algebra, probability and calculus is required."
"Controls development case study of a Youtube video - how would you have done things differently?I recently came across this [video](https://www.youtube.com/watch?v=xd8dKY6Ozrg) by Hacksmith Industries where a team builds a hexapod platform with hydraulic actuators.
The video documents the development process and the problems faced during the project well, and I wanted to do a ""design review"" of sorts and ask for your opinions on what they could have done differently.

I'm aware that they work with limited timeframes, budgets and manpower, and this discussion is not intended to be in the spirit of criticism, but as a discussion of how members of this community would go about tackling the controls side this project.

There are extensive writeups in the software industry regarding the development practices used by practitioners, and I would like this thread to discuss controls development in a similar vein.

I gather that their motion controls consists of:

* Walking motions generated by an open source hexapod library, likely kinematic and generates walking patterns based on the target direction specified by the user, without any other sources of feedback

* PID controllers for each actuator to track the generated motion, tuned experimentally. Encoder feedback.

Some problems identified / mentioned in the video:

* Controls tuning done on the live system

* Not many reasonable metrics to measure control performance (the scene at [19:02](https://youtu.be/xd8dKY6Ozrg?t=1142) where they simply look at the plots seems to be for comedic purpose, but also highlights lack of relevant metrics besides plots)

* Lack of software watchdogs to prevent dangerous or unintended motion (behavior at [14:40](https://youtu.be/xd8dKY6Ozrg?t=880))

What I would like to see done differently:

* System identification and transfer function generation for the actuators/valves/hydraulics. This would allow for:

	* Faster test cycles when tuning controllers

	* Evaluating different controller architectures based on metrics such as overshoot and settling time

* Conducting real-life tests on individual actuators before tests on the fully assembled robot.

	* It should be easier to detect points of failure/bugs when testing with a small part of the system instead of all at once.

* Low-pass filters on the controller outputs to prevent any sudden changes in controller signals.

* We don't get any details into how their PID controller was implemented, but measures to prevent integrator windup and derivative kicks, if they don't already exist

What would you have done differently to prevent the development/integration issues they faced in the video?"
"Simplicity in designWhen people say something like “simplicity is always best” in engineering, what exactly does that mean? How do you know when to add complexity for better performance? 

I’m in a first robotics team and the challenge this year boils down to a robotic arm. How do you decide whether to do something simple which won’t have as much mobility, vs a higher dof, more complex arm, that would be more versatile but have more that could go wrong and propose more of a challenge? 

To me I think of that phrase as don’t make things complicated if they don’t need to be, but what if the simple solution gets the job done, but a more complex one does it more consistently/efficiently?"
"approximate inverse kinematicsI have a 5 DOF robot arm set up so i can send it angle commands for each of the servo motors. Now I would like to send it absolute position commands instead. 
I was wondering if there was a way/ any resources on using optical tracking of the end effector to calculate the inverse kinematics numerically. I.e. approximating a transformation matrix from absolute position to servo angles. 

I am tracking the final position and rotation of the arm using an April tag and a camera mounted on a tripod. Since the robot arm doesn't have a 360° working area the arm always faces the camera.. more or less, which allows me to track the April tag. This way I can easily and cheaply collect quite a few data points consisting of servo angles and position+rotation of the arm."
"Low-cost Robotic Arm? (Poll)We have been developing a cable-driven 7-axis serial robotic arm, with an expected payload of 5kg, for which all the motors are mounted at the base (not at the joints) to improve the arm’s performance. The motion of each motor is transferred to the joints through cables, while a unique rolling joint mechanism allows each joint to act independently of previous joints’ movement. The design uses a direct motor drive and pulley reductions at the joints, to eliminate all backlash and increase torque and stiffness. The robot consists mainly of basic 3D printed PLA+. Our goal is to promote it on Kickstarter and distribute the whole design as open-source, so those in the community who own a 3D printer and are zealous enough can replicate it all on their own.

We plan on developing valuable features right out of the box. When we post this on Kickstarter, there would be a better layout of our future plans, but at the moment, this would only act as a bare-bones robotic arm platform. Simply put, we just seek an estimate of how much demand a product like this would generate.

If this robotic arm existed for $3000 fully assembled and $2000 as a kit, would you buy it?

[View Poll](https://www.reddit.com/poll/x3dqjx)"
"Need Suggestions regarding design a quadruped and its motor selectionHi Guys, We are currently developing a 3D printed BLDC quadruped from Scratch. And were stuck on selecting the Motor for design the Cycloidal Actuator.

Our current projected Weight of the bot is close to 15-18 Kgs. So according to that we are designing an single actuator for 30Nm torque Output, with the gear reduction using a cycloidal Gearbox. The bot will consist of 12 Such Actuators. Also the electronic capacity of the bot are .......

The motors we are thinking of are 
1. Gartt ML5010 (300KV) - https://ar.aliexpress.com/item/32517972556.html?gatewayAdapt=glo2ara
2. T-Motor Antigravity MN5008 (400KV) - https://robokits.co.in/multirotor-spare-parts/tiger-motor-esc-and-propeller/t-motor-motor/antigravity-type/t-motor-antigravity-mn5008-170kv
3.MJbots-https://mjbots.com/products/mj5208?pr_prod_strat=copurchase&pr_rec_id=de9ff50a3&pr_rec_pid=5945253626017&pr_ref_pid=7358414749857&pr_seq=uniform

Our main concerns are the KV requirements and the Torque output calculations of the Motors. Which one would be a better fit for our Application, and how are they better than one other. Also what are the problems faced due to each of the motors.

Also would creating a Belt Driven knee actuator be better than a Parallel link actuation of the Knee

we were also concerned if this motor controller will be able to power these motors - https://www.ti.com/tool/BOOSTXL-DRV8323RH"
"An AI wrote a poem about the irony of being afraid of AI. An Ai was capable of writing a meaningful poem about the existing irony behind the fear of AI. Even though humans have created these AI machines, many are afraid and unsure about the results of such innovation"
"What are the key challenges hindering the development of generative robotic agents capable of highly generalized behavior?Additionally, could you discuss the limitations regarding the training data used? In order to overcome these obstacles, would it be more beneficial to acquire a larger volume of high-quality data or to broaden the criteria for what is considered suitable for training data?"
How ChatGPT Programmed an Industrial Robot. Explore the capabilities and constraints of ChatGPT in the realm of industrial robotics. Our most recent article dives into an experiment where we had ChatGPT program a Yaskawa robot. Did ChatGPT meet the challenge? Uncover the results in our article
"Are non-humanoid/non-android robots capable of mechanically evolving into sentience?A lot of works of science fiction usually feature robots that have outgrown their programming and becoming sentient. Most of these robots are depicted as androids/human-sized robots. While this is makes for good fiction from what I understand in the future most robots that we will see on a daily basis are going to look less like androids/human-sized robots and more like automated cars, automated houses, roombas, drones, toys (Ex: Nao), Boston Dynamics Spot, and industrial-like robots that can be used for warehouse work, medical purposes, and of course factory work. In any case, are any of these non-humanoid/non-android robots capable of [mechanically evolving](https://tvtropes.org/pmwiki/pmwiki.php/Main/MechanicalEvolution) into [sentience](https://tvtropes.org/pmwiki/pmwiki.php/Main/GrewBeyondTheirProgramming)?"
The newest MD80 V2.0 Brushless Motor Controller 🦾🦾🦾 It is optimized for mobile robotics and has greater capabilities than its predecessor. https://www.mabrobotics.pl/product-page/md80-V20-motor-controller
Latest Robotics Research Releases ‘Hora’: A Single Policy Capable of Rotating Diverse Objects With a Dexterous Robot Hand
"DIY robot with rest api capabilitiesHi all, I've always had an interest in robotics and for a while now ive been thinking of incorporating a robotic companion into my work flow. I'm a developer (Java, C#, C++, Javascript, PHP, Kotlin). What I'd like to do is be able to fire rest api calls to a robot who could alert me of certain events. For example, something as simple as being able to alert me of a ping monitor alert. e.g. if the ping monitor process which is online, detects an issue, it can send a rest api call to the robot alerting of the issue. The robot should then be able to inform me. Something along those lines.

So my question is, are there any DIY or 'off the self' robots that would allow me to send api calls with that sort of information? Another way i thought could work is if the robot has an SDK that i can make use of from an Android app as well. So the app gets the notification and then the app informs the robot via the SDK. That would also be a viable option for me.

I've searched online but i cant find a definitive answer. I was thinking perhaps the lego mindstorm could work, but there seems to be many different variations and I'm not sure if any would work.

Any advice welcome please."
"Why has no company released a trash collecting robot?In a blog article published two years ago, Boston Dynamics describes how their robot dog Spot is capable of being programmed to collect litter. I would believe there to be a large market for such a robot. I wonder why even after two years no company seems to have turned this idea into a product. With hardware and low-level grasping being dealt with by Boston Dynamics I don’t see how it would take more than two years to build a first version given enough financing. As they describe themselves in the article one needs to build a machine learning model to identify litter and use the grasping API to collect it. Lastly a sweeping algorithm would need to be implemented that tells Spot where to look.  Some practical problems also require thought such as how to deal with vandalism and charging Spot. However, I would’ve expected a company would have released a first version after two years.

Is there anyone here that knows to make sense of this? Are there open problems in robotics research that are a barrier?

I know dexterous manipulation is still an open problem with the annual real robot challenge being hosted by the Max Planck Institute for Intelligent Systems every year. However, a trash collecting would only require grasping capabilities and simple manipulation (put grasped trash in the bin it’s carrying on its back).

If anyone has a better understanding of why we haven’t got robots (Boston Dynamics Spot equipped with appropriate software), collecting our litter, yet, I’d much appreciate your comments."
"Human Augmentation Hacker Lab in SFIf you're interested in robotics, wearable tech, AR/VR, AI, and BCIs we're organizing a **2023 Augmentation Residency** to build fundamental human-extending technologies! 

**Hosted in San Francisco, 18 June - 27 August, all expenses paid. 2 spots remaining, applications are evaluated on a rolling basis until all slots are filled.**

The [Augmentation Lab](http://augmentationlab.org/incubator) is a **full-time hacker lab** founded by two Harvard students to extend human capabilities & improve the human condition. For 10 weeks, **6-10 creative technologists will live and work together** to build the next impactful technologies in the fields of wearable computing, robotics, AR/VR, BCIs, and AI/ML. With mentorship from renowned experts, the residency will integrate hardware and software engineering, product development, scientific methods, and creative storytelling to produce foundational innovations backed by rigorous research. We will self-experiment our technologies and our experiences will be recorded in a documentary.

Past projects of the hacker lab include eye-gaze-controlled supernumerary robotic limbs, NLP-driven XR game engines, and haptic gloves that reinvent IoT interactions. Our work explores transhumanism, cognitive augmentation, physical augmentation, & immersive worlds through research, engineering, and creative practices.

We are looking for **creative builders**, particularly those with hardware experience (yet we still welcome applications from software engineers) who share a deep interest in extending human capabilities. 

We also welcome people from outside the US - we just can't take care of your visa so please ensure you can enter. We will not employ you so a tourist visa or ESTA should be enough but pls DYOR.  

Mods: I hope this isn't flagged for advertising as we're not selling anything, there's no catch involved - we got generous sponsoring and the IP belongs to the creators. :)"
MicrocontrollerI am currently working on a project and I need a microcontroller that has wireless communication capabilities. I have worked with an arduino mega bord before but that needed a usb wire. Is it possible to buy something to connect to the usb port to make it wireless? If not any recommendations on a fully wireless controller board?
KUKA 6-Axis Robot Arm Capabilities
"What are the limiting factors from hobby/open source robotic arms from achieving near accuracy and precision of commercial ones?I took a look at some of the hobby/open source robot arms and even some of the best ones are very shaky/unsmooth/inaccurate and no where near as precise as commercialized ones.

I'd presume the tight tolerance of the mechanical components plays a major role, but are there any other factors that hinder the ability to achieve high precision and accuracy at a DIY level?  Is the gap closing between DIY and Commercialized arms?  The technology itself has been around for quite some time now.  Seems like all the math and software is available already."
"Ascento Pro, evolution of the mixed locomotion ascento robot, looks way more capable than the original."
"tb6600 stepper motor driverI'm currently using a nema 17 stepper motor with a drv8825 driver. I was wondering if I switch to a tb6600 stepper motor driver, would the speed and the torque of the motor increase? Here are the electrical specs:

&#x200B;

https://preview.redd.it/r3z4j1dvw2xa1.png?width=307&format=png&auto=webp&v=enabled&s=6726aaf419c0a4dea33b832de9972148ada3ad60

&#x200B;

https://preview.redd.it/8mdsbpkj13xa1.png?width=282&format=png&auto=webp&v=enabled&s=eec3bcd6eb6ba3d1885237f2f629d6ccdf43fcad"
"Building a TARS-like Robot Inspired by Interstellar – Let's Discuss!I recently re-watched the movie ""Interstellar,"" and I couldn't help but be amazed by the robots in the film, especially TARS and CASE. These silver box-like robots that move back and forth with their arms are so fascinating, and it got me thinking – why not try to build something similar with the current technology we have today?

In the movie, the robots are highly advanced, with exceptional AI, voice, and motor capabilities. I believe that with the advancements in technology we've seen in recent years, we're not far off from making this a reality. Consider Boston Dynamics' robots, for instance. They've already demonstrated impressive mobility and balance. Couple that with state-of-the-art AI and voice technologies, and we could be looking at our very own TARS!

Here's what I'm thinking for a modern-day TARS-like bot:

1. Base it on a versatile robot like those from Boston Dynamics, capable of complex movements and tasks.
2. Incorporate advanced AI algorithms to enable autonomous decision-making, problem-solving, and learning capabilities.
3. Implement AI voice technology for seamless and natural communication with humans. This could include voice changers and creators for added versatility.
4. Design an adaptable mechanical structure that allows the robot to reconfigure its shape and size to navigate various terrains and environments, much like the robots in the movie.

Now, I want to open up the floor to all of you: How do you think we could build a TARS-like bot with the current technology we have? What aspects of AI, voice tech, and robotics do you think are crucial for such a project? I'd love to hear your thoughts, ideas, and suggestions.

Let's create something amazing together!

TL;DR: Inspired by the robots in ""Interstellar,"" I want to discuss how we can build a TARS-like bot using current technology like advanced AI, voice tech, and robots similar to those from Boston Dynamics. Share your ideas and insights!"
"Robomaster S1 Capabilities?I am curious if anybody has experience playing with Robomaster S1. What are the capabilities of the SDK, beyond what is obvious ( interacting as an API ).

Is the hardware open to manipulation? Can I use the motor driver freely? Any feedback is appreciated."
"Where should a beginner start to get IK down for a 6 axis robotic arm project?A bit of context:
-  More focused on prototyping and getting a working solutions than I am on the optimal solution right now, but also interested in learning as I go (hobby project)
 - I am still iterating on the kinematics, so really don’t want to solve the analytical solution for each change. Therefor plan to use iterative solutions.
- I am capable in pythons, so prefer that. I know the concepts of C but have not written that in years.

Options I am exploring are:
- Klampt
- ikpy"
"Using TalonSRX ESC Without Proprietary Hardware or SoftwareHi -

Has anyone here used the CTRE TalonSRX Brushed ESCs before without the RoboRio or any of that other FRC/CTRE shenanigans? Like with a normal ESC, is there a working way to control it with Python or C++ libraries?"
"Opinons on this bot concept?Hi there! For a while, I've been working on the overall concept of this bot. They do not have a name yet. They are very small and would be capable of resting on someone's shoulder. They (preferably) would act similar to a companion or just a source  of entertainment. 

Any name ideas? Or genuine improvements? I haven't worked on the coding or building yet, since it has been a year or two since I've touched this concept. 

Any help is appreciated!!!!"
"Intelligent UGV - dream projectThis is a hobby project but also a *dream* one I had a vision of. 

The first I wanted to do was to create a small vehicle with a bit of power that could patrol my house while I am out or sleeping. The car should be autonomous but also be overridden manually in order to be controller as an RC car with some intelligence (for example obstacle avoidance, return to home functionality or auto charging as robot vacuums do). I wanted the vehicle to be suitable to ride small rocks or obstacles on its way and be waterproof. It should also have FPV capability or on a later version multi-camera views, lights and maybe a siren. 

So I started slow, I dissassembled an old RC I had with tracks, removed the main PCB and installed a motor controller which I quickly burned down. Now I have an L298N. I also connected a FlySky controller with iBus to a Raspberry pi pico. Last, I added an HCSR04 supersonic sensor on the front and started working with an auto-braking  system while driving the car with the RC controller towards an object. 

I am almost there. But I am also completely lost at this phase and I could use some help or guidance. 

Should I replace my raspberry pico with a raspberry pi zero to easily control the GPIO Pins, add openCV capability or maybe an oled ? Some are easily configured in the pico but it has limited memory capacity and Thonny isn't the best tool to work. 

Last about the chassis. **Please** advice. I saw some kits in banggood but also I have a lot of tools and I am a DIY enthusiast but I am far from designing a car metal chassis. But I really want to and I could try at least to create my ""simple box with wheels"" chassis as a Version 2 of my robot. I am running out of space in my old RC and it's plastic so it has a limited life as a version 1. 

I am sorry for the long post, I am too happy talking about this and can't keep my enthusiasm back!"
Using Moroccan tea tray mathematics to turn robots into skilled waiters. Researchers have developed a model that enables a robot to serve tea and coffee faster and more safely than humans—with no sloshing. The mathematics behind the pendulum used in the concept is more than 300 years old.
"Looking for ambitious/skilled robotics partnersHello, I own 2 small construction contractor companies (~40 employees) in Vancouver. One in concrete formwork and the other in structural steel erection and fabrication. Robotics is the future of construction and I'm working to stay ahead of the curve. I'm looking to find some individuals or companies with whom relating/ partnering would be mutually beneficial.
Please message me or respond with any ideas or suggestions.
Thank you,"
"Roboticists at CMU and UC Berkeley are training robot dogs to use their legs for manipulation, not just locomotion, demonstrating skills that include climbing walls, pressing buttons, and even kicking a soccer ball."
Open source humanoid Kayra now with arms! 
"Youtuber Peng Zhihui just dropped a new project. In my opinion, he’s one of the most impressive “hobbyist” robot designers in the world."
Progress with my quadruped robot. Simple step sequence with the three legs
"skills needed to become a robot technician/installerI'm talking about [these](https://www.pudurobotics.com/) of robots types. Restaurant robots, delivery robots and others. Which skills do I need to be the person going to clients, installing them, troubleshooting etc. Thanks"
Learning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations. More info + a link to the full video / research paper in the comments.
"In my opinion, these are wise words from Will Jackson's interview @engineered_arts . I think everyone should listen to"
"How do you come back from “we like your skills but you’re too independent”? Over 40, skilled in software, electrical, and mechanical engineering, company builds robot vending machines."
"Fundamental Skills in Robotics?What are the skills that are key to robotics––but not necessarily to other STEM disciplines––that every robotics engineer should be strong in? I'm hoping for answers more specific than ""hardware and software engineering."" An answer like ""control systems"" is more specific, but it still feels academic and not like a good starting point for self study, so elaboration is thus requested. Asked another way, what skills should an aspiring roboticist demonstrate in their portfolio pieces if they wish to optimize their competitive edge?"
Community project?Robotics has a lot of encompassing fields and anything worthwhile is usually better accomplished as a team. Anyone wanna combine efforts and make a nice open source build for others to utilize? I'm thinking we get some computer people some mechanical people and some electrical people and seperate parts of the build design to each team.  Could be fun or fail miserably but I'm down to try.
Musk says they use all in-house parts in Tesla Bot. Does anyone notice anything outstanding in this new demo video?
What’s the best option for virtual prototyping?I’m looking to start prototyping the mechanics to see how it will move and different components I’ll need to 3D print but I also want to built a rough version of my robot. What’s a good software for this? I’ve considered Fusion 360 since I could then take the object files and use them directly but while I have basic Fusion 360 skills assembly and testing the motion feels like it has too high a learning curve. What should I do?
"Robotics jobs - recession proofAs a recent robotics masters graduate I have been looking around for a full time jobs(USA, California). I noticed the skills required for full-time roles vs the college skills I earned are far.

Example:-

1. Python in college, mostly c++ in industry

2.Matlab for robot arm programming in college, PLC programming in industry.

3. None in college, classical methods in SLAM roles in industry.

4.None in college, learning methods for perception in industry.

Don't know where I can learn practical skills of robotics like PLC programming for robot arms, learning methods for perception.

How to fill this void and what fields in Robotics jobs do you think are recession proof.?"
I Made My Alexa Control My Room Alarm
"Learning Path To Become a Robotics DeveloperI stumbled upon a post on LinkedIn that caught my attention. It contained a link that details the essential skills one needs to acquire to become a robotic developer. I felt it would be beneficial to share it here. While I may not fully endorse all the steps outlined, it's still an intriguing read.

[https://www.linkedin.com/posts/activity-7064734932140441600-lVku?utm\_source=share&utm\_medium=member\_desktop](https://www.linkedin.com/posts/activity-7064734932140441600-lVku?utm_source=share&utm_medium=member_desktop)"
Robotics Skills & Knowledge Venn Diagram - What things do you need to know to get into Robotics? Also what is missing from this diagram you think it should include?
"Need some suggestions to improve coding skills for robotics?I use programming as a tool to work with my robots. Although, I have been using python and C++ for an year I havent programmed them in the most efficent way or structured way. Since I am mostly self taught with Mechanical/Mechatronics major, never got the exposure to rules to follow for a software engineer. Please suggest me some website or practice project tools that I can learn the right way of doing it."
"Looking for jobs in medical roboticsHey everyone, I’m currently studying Medical Robotics and Image-Guided Intervention at Imperial College London. Once I graduate I’ll be looking at work in the industry. Aside from the technical skills I’m getting, I was wondering if there were any additional skills worth practicing like understanding the regulations process in Europe and America. Additionally, I was wondering where I should look for work and what companies whether that be in the UK, Europe, or America. Thank you all."
"Finding a robotics team to join in a league that allows adults to compete (not just mentor students)Hello everyone!

I am a working professional with an engineering background living in northern Massachusetts (near Andover) who is looking to join a robotics team in a league that allows adults to compete (not just mentor students). Doing some research online, I've discovered a number of leagues/competitions that fit this description including Battlebots, RoboCup, Robogames, ect. However, I am having some difficulty finding teams in the area as the only strategy I know of is showing up to robotics events and hoping I happen to strike up a conversation with someone who is looking for people to join their team or knows someone who does. Needless to say, it's been slow-going up to this point as I am still searching. 

Are there any online resources available for helping someone looking to join a team get connected with those looking to recruit team members? Any insight would be appreciated."
China robot restaurant
"Robotics and Automation cooperative in Developing countries, owning/developing and researching Robotics for local needs, empowering communitiesHello all, 

I've been thinking of an idea of starting Robotics cooperative for Developing countries, particularly since I'm from India, I'd like to propose a cooperative for countries like India, where there's abject poverty and Robotic solutions can used for unique problems, one example is socio-political in nature, which is Manhole cleaners dying inside it because of lack of proper standards gear involved in diving in sewage water,  the people who do these jobs are from a certain caste, and no one else would take up this kind of a job, and unless Indians change their attitude and take up jobs outside their class, and also improved standards which involve wearing oxygen tank and hazmat suit etc. News coverage of one such robot invented by Engineers in Kerala(A southern State in India): [https://www.indiatimes.com/technology/news/swachh-bharat-day-genrobotics-breakthrough-manhole-cleaning-robot-to-be-felicitated-by-govt-353999.html](https://www.indiatimes.com/technology/news/swachh-bharat-day-genrobotics-breakthrough-manhole-cleaning-robot-to-be-felicitated-by-govt-353999.html)

Other ideas include marine robots for cultivation of sea weed, fish farm, etc, as India has a big coast line, and communities which involves fishermen and need for modern solutions to compete against other countries, and big corporations. 

And much more, like Agriculture robots, Automated Covid test facilities etc.

My plan would to be leasing robot, RaaS, by procuring robots, and using the profits developed from these services to do R&D for local needs and making low cost solutions. 

I'm naming it a cooperative because the Robots being leased could be owned by a particular community, and also some help from us(we would be part of fractional ownership).  

I'm looking for enthusiasts, cheer leaders, beginners, engineers, professors, researchers, finance professionals, economists etc to take part in this initiative. I've been having this idea for a while, so please message me(or comment here so I can DM you) and we will get on a Discord to brainstorm ideas about funding possibilities to kickstart this initiative, and if we should be a for-profit or non-profit company(I don't want to see profit as a taboo word, if there's money to be made, it should not be a thing of shame or guilt), and other things. 

About me: I'm a Master student in Robotics in Germany. I'm interested in intersection of social causes and robotics for good."
The SlaughterHouse Mark II shreds an Xbox 360
"Vote for Pakistani Team's Poster for the ISSSP CompetitionAssalamualaikum everyone,

We need your support to excel in the International Space Science and Scientific Payload Competition (ISSSP), hosted by China with regards to payload design for Tiangong space station. Alhamdulillah! We competed with all the teams across the world and were able to qualify to the final along with 14 other international teams. Saf-Shikan is working on the design of prototype for a crawler robot capable of traversing the exterior of Chinese Space Station. Currently the voting for the best scientific poster is live at [https://www.wenjuan.com/s/ZZ3Uvad/?continue\_view=1](https://www.wenjuan.com/s/ZZ3Uvad/?continue_view=1). We need your support in order to excel in this module of the competition. Our team, Saf-Shikan, is listed under the section ""Innovative design of space science loads (Theme 2 - College student groups)\*"". In other sections (Theme 1 and middle school), you may vote any team and then submit your vote accordingly. \*Please click submit after scrolling down to the bottom. Otherwise, the vote won't be submitted. Your vote can help us showcase our skills and represent our country.

Thank you for your support.

Best regards."
"What sort of materials could a high school robotics team useI recently started dating this girl and her son is on the high school robotics team and is really into it, Iike possibly on an aspergers level. I really like her and want to make an impression. 

I work in IT for a large corporation, and we have a warehouse full of old computer/equipment parts going in the trash or headed to recycling. 

What are some pieces/components/materials that I could donate? Stuff that they always seem to be out of or use more of."
I just built my first combat robot
Video of our ring-shooting turret robot that competed in the Ultimate Goal First Tech Challenge Competition
"Reddit, Let's Compete in the NASA Space Robotics Competition!Hey everyone! Head over to https://www.reddit.com/r/obotics/ to check out the official team subreddit.

rLoop (a Reddit formed team that competed in the SpaceX Hyperloop Pod Competition) is doing extremely well, even when overcoming hardware logistics problems that arise from having a team spread across the globe. Since the NASA Space Robotics Competition is all in simulation, we can have a huge team coding 24/7 with no hang ups. This will give us a huge advantage. Leave a comment or message me if you're interested. I'll put together a slack soon.

Send me you're email in a message so I can add you on slack if you're interested.

This will be a super fun project and will look amazing on your resume.

Here's a link to the competition details if you're interested:

http://spectrum.ieee.org/automaton/robotics/space-robots/nasa-space-robotics-challenge-tasks-prizes-how-to-participate

EDIT: For those of your who were asking: 
I've emailed one of the organizers and there isn't a team size limit. We're good to go :D"
Had my middle schoolers build a collision-avoidance robot with Lego EV3's in the robotics elective. They competed to see which bot would be the last one standing without running into another bot or falling off the table.
"Becoming a Jack of All Trades EngineerIt was always my intention to use the mechanical engineering field as my foundation to get into the door of robotics. I always thought that I wanted to be exceptional at one thing and one thing alone, however, as time has passed, I've only realized the opposite.

With the engineering teams I've had the pleasure to work with for research, its the people that can wear multiple hats that always turn out to be more valuable to a team, while they also have their one main specialty that gives them their edge, instead of being a one trick pony. Now when I say this, in no way am I devaluing the need for those who are masters in one field, we need those types, this is just what i've seen personally.

I used to only want to focus on mechanical engineering topics and neglect the computer, network, and electrical side of products all together. When it came time to conduct research on campus, this mindset had me sitting on the sidelines during competitions watching others troubleshoot problems that I had no idea how to contribute properly, and it pissed me off.

On top of that, I just recently finished my 10wk internship this summer, and the environment there only proved what I observed during my on campus research competitions to be true, on a much larger scale. The people at this site all could contribute on the mechanical, electrical, software, and networking side of a task, and if they did not already have the skill, they were more than open to have someone who was an expert there teach them what was necessary. Being able to observe that was amazing because everyone could contribute at any point in time and just pick up where the other left off with at the very least a decent level of competency. Collaboration was smooth and communication even smoother. That's when I realized that I had it all wrong.

I want to set aside time each day (4hrs max) to polish the specialty I want (mechanical design), while learning the basics of electronics, software development, and network engineering. Im calling it my JOAT Training (Jack of All Trades Training) lol.

***My question is about my approach...***

* ***should I spend an hour each for mechanical design, electronics, software dev, and network eng all separate from each other***
* ***or is there a different method you can see that will encompass all of the four topics previously mentioned in one session***  

   * ***I was thinking like if there was a site that had a list of previously built engineering product blueprints online and just breaking them down on the mechanical, software, network, and electrical levels***  

      * ***with the goal that I would understand how each field ties in with the other on a ""systems engineering"" level through breaking down their components***

***TLDR - What is the best approach to train to become a Jack of All Trades Engineer? (Mechanical Design, Electrical Engineering, Software Dev, Network Eng)***"
Tiny robots will compete against DARPA’s “robot Olympics”
"Hi. We are currently competing in the RoboCup German Open Competetion. Ask us anything!Hello. We are [Nao-Team HTWK](http://robocup.imn.htwk-leipzig.de/). We are 14 humans and 18 robots and at the moment we do compete in the [RoboCup German Open](http://www.robocupgermanopen.de/). We are playing [Robot-Soccer/Football with bipedal robots](http://www.robocup2015.org/).

You can watch the event [live](http://streamteam.fbm.htwk-leipzig.de/), check the [schedule and results](http://www.informatik.uni-bremen.de/spl/bin/view/Website/GermanOpenResults2015) (CEST times) or view our [recordings](https://www.youtube.com/user/naoteamhtwk/featured) afterwards.

Feel free to ask us questions about the event. We are reguar attendees of the German Open, the Iranian Open and the World Championship. We also provide some very experienced referees and RoboCup event organizers so you can as some questions regarding these topics as well."
"A take I haven't seen on ""Why the emphasis on humanoid robots over purpose built bots?""Wrote an effortpost response to a question I've seen multiple times in many forms. Also serves as a response to people straightforwardly arguing against the concept of a humanoid robot being practical ([https://www.youtube.com/watch?v=Wk1oClYJE58](https://www.youtube.com/watch?v=Wk1oClYJE58)). Posting it here as its own post because I'd love to hear other people's opinion.

TL;DR for long post: We do engineer simpler purpose built ones and will continue to, but there is a big conceptual reason to focus discussion on humanoid robots (it's a legible and essentially inarguable milestone), and a huge practical and economic advantage in actually developing such bots (learning from human execution of tasks and then replacing them 1:1 allows for extremely rapid deployment at massive scale).

For one, there already exist and will continue to exist a huge number of non-humanoid robot form factors that are designed with specific use cases in mind. No sensible developer thinks that a factory of humanoid robots will replace industrial manufacturing robots or that it's better to have a humanoid robot try to crawl through pipes in a collapsed building when a smaller exploratory bot would do better.

That said, every time (counted in the dozens) I've seen people try to advance the position of  ""humanoid robots are a silly sci-fi preoccupation"" and then have people respond to it, I've never seen the two responses that I think are the strongest. First, it's just a very legible milestone. Humans are meat puppets, and society as it exists is built on humans doing labor. For the physical element of that labor in many areas, automation and robotics have replaced human work by being more productive for less cost in significant amounts. But this replacement is far from total; billions of peoples' livelihood still comes in large part from the physical labor they perform. When we actually achieve a human-equivalent robotic form factor (simple visual: the robots as depicted in the Will Smith I Robot movie), it will represent a comprehensible milestone on what is actually a gradual shift in replacing human labor with cheaper and more efficient robotic labor. Millions, if not billions, of human jobs will have been replaced prior to the day that robot is released, and it will be some amount of time before production of robots will scale to the point that all physical labor jobs will be replaced. But once that human-equivalent robot is realized, at whatever cost, it's essentially proof that human labor is obsolete. Those bots will get cheaper, and faster, and stronger, and more capable in ways that humans will never be able to compete with.

The second point I have never seen is that humanoid robots have a massive advantage over custom designed bots (in many use cases) that goes far beyond the usual (strong and valid) argument of ""we live in a world designed for humans and bipedal, human-sized robots can navigate it better."" That advantage is training data. Development of robots is difficult and expensive, but it's often the programming of their movements/tasks that becomes the most complicated element of deploying them as useful labor. Think all the videos of industrial robot arms failing to make a hamburger or latte. 

Now imagine McDonalds or Starbucks installing cameras in a number of their locations to capture how their existing human employees perform their jobs (with or without mocap dots on their uniforms, depending on how computer vision advances over the next few years). Plugging that motion data into machine learning models would then be able to directly provide task-specific programming to a humanoid bot. Once we have a human-equivalent bot it becomes a platform for multi-purpose use. Every human doing a job, from construction worker to organic farmer to vehicle repair to in-home nurse flipping their patient to soldier, becomes training data to teach that bot how to replace them. Intelligent humans with economic incentive, and a history of predecessors to learn from who also had creativity and incentive, are likely to have developed and converged on efficient ways to accomplish most tasks. Someone who's spent years flipping burgers or laying bricks is way more likely to have developed the optimal technique than an engineer trying to design movements with no actual experience. A training set built from thousands of experienced people is even more likely to find optimal execution. The more repetitive the job, the faster and easier a job can be digested by ML, but with enough data and compute, everything is theoretically on the chopping block.

Think of the cost savings of being able to replace every human being working in a designed-to-be-staffed-by-humans workplace with a mass-manufactured bot, rather than having to design and rebuild every restaurant, mechanic shop, dentist office, etc etc etc. And once a job is trained, analyzed and ""solved,"" then every single bot can perform that job. No more training replacement employees or new ones at a new location. And for edge case AI failures, a human ""pilot"" could take over remotely using VR/mocap. Their human solution to the failure would then become part of the training set to avoid that failure ever again.

Yes, a top down burger joint designed to be run by custom bots could probably achieve some theoretical efficiency gains. But would those gains justify the cost to develop and rebuild every McDs, or is it sufficient to just slot bots into existing locations? Whoever develops the human-equivalent robotic platform that can even do 85% of what a human can do (let alone the eventual 99% or 105% or 300% productivity that mechanical humanoids will likely achieve over meat puppets) could be the most profitable ""single product"" ever launched. If it comes to market sufficiently developed, with a sizable enough lead on competition, etc, it could become the fastest consolidation of power (economic and otherwise) in history. A lot of ifs/coulds, but enough huge potential gains that it makes absolute sense to chase.

I wouldn't be super surprised if someone has counterpoints to this, as that might explain why I haven't seen this angle expressed before. I've paid attention to this subject for a long time, read books and probably hundreds of articles on robotics, the economics of automation and the nature of labor, and this frame has been absent from all of them. I am quite sure I'm not the only one to think of it this way, so I am eager to see if others have encountered the ""humans as training data"" perspective."
"Sports Field Line Marking RobotI'm trying to design and build a Line Marking Robot for various outdoor sports fields. I'm new to robotics, but I have friends who aren't, and as long as I come to them with small problems, I'm confident they'd lend me their time. This project is for fun, but if the result is awesome, I'd certainly offer it to my local soccer club, rec programs, and high school. The advantages of automating this job are great, but the commercial products are extremely pricey.

I figured I could write a program and use IMUs and maybe some wheel encoders to get a decent result. If I were doing tennis courts, I'd probably have given it a go, but the fields I'm painting aren't perfectly consistent and there needs to be a way for the robot to localize and correct.

I've looked into all sorts of beacons, but it seems that RTK GPS is the only system with the sort of accuracy I'd need to be able to paint a straight line 100+ meters long outdoors. I have no problem with RTK GPS except that it seems like overkill, and the end user would need to be fairly competent. There ought to be a cheaper and simpler way to get cm-level accuracy in a flat space that is a maximum of 1 hectare (usually much smaller), but I'm not finding one. IR seems great, but I'm worried about fog. RSSI seems promising, but all the open source projects I've found that use it are pretty rough. Ultrasonic seems really ""noisy"" and variable.

Does anyone have suggestions for other methods and open source projects I could check out?"
My Highschool's robot competing in the FIRST Robotics Competition. 
Hi-res picture of DARPA's Atlas robot that will be competing in two months at the DARPA Robotics Challenge
I made this robot to compete in a quarter scale version of FIRST Steamworks
"Hi. We are currently competing in the RoboCup German Open Competetion. Ask us anything!Hello. We are [Nao-Team HTWK](http://robocup.imn.htwk-leipzig.de/). We are 18 robots and 20 humans and at the moment we do compete in the [RoboCup German Open](https://www.robocupgermanopen.de/). We are playing [Robot-Soccer/Football with bipedal robots](https://www.robocup2017.org/).

There are several subcompetitions at Robocup. We participate in the Standard Platform League (SPL). There are other leagues as well with football/soccer playing robots in several sizes and locomotion types (legs, wheels). There are also rescue robots, logistics competitions and home assistance competitions.

You can check the [schedule and results](http://spl.robocup.org/events/german-open-results-2017/) (CEST times) or view our [recordings](https://www.youtube.com/user/naoteamhtwk/featured) afterwards.

Feel free to ask us questions about the event. We will answer questions here when time allows us. We are reguar attendees of the German Open, the Iranian Open and the World Championship. We also provide some very experienced referees and RoboCup event organizers so you can as some questions regarding these topics as well. The event will run until sunday afternoon and we will be answering questions until then."
"I'm helping to start a robotics club at my university! What robotics competitions could we plausibly compete in without a huge budget?We have some faculty signing onto this endeavor and I want to be able to propose competitions that are more suited to a budding robotics program (I'm a student, btw). Funding is uncertain, but I'm not optimistic about getting decent funding during the economic crunch."
"Installing 3rd Party Library Dependency on a Segway LoomoHey everyone! 

I’m a new robotics developer working on a Segway Loomo, and I’m having some trouble installing the dependency for the following: 

https://github.com/segway-robotics/loomo-algodev

I haven’t been able to find anything as to an updated way of installing the dependency, as it states “The 3rd party library should be downloaded and extracted to dependency\3rdparty_android:”

I don’t see this folder in Android Studio Electric Eel (patch 2), but this repository is rather old and the location may have changed. 

Would anyone know where to install this library, and if it would be installed on the robot, Android Studio, or both?

Thank you!!"
"DIY Electric Wheelbarrow - The InsideHi again everyone! I was asked a few times to show the inside of my really rickety DIY Electric Wheelbarrow, so before I take it all apart and show you how to build a dependable one for my next video, I just wanted to share this one pre-destruction!

I don't like posting YouTube directly on here, but if you would like to see more of my videos please visit my channel -
https://youtube.com/shorts/rK1j9TYvjPQ?feature=share

Thanks everyone for the support!"
"I suppose to install pytorch in Jetson TX2 NX , but facing this issue. Can anybody help me out…its urgent"
"Cross posting from ROA Answers for more visibility : In professional large scale robotics systems, what are the best practices for system design and package dependencies"
This is an underground space with no positioning and no signal. Are drones up to the job?
"Are you running multiple battery systems in your robot project too?  

**ENGINEERING DESIGN REVIEW**

**Prepared by Patrick M. Will “Perci”**

**For Diligent Robotics**

**On this 17th day of October 2022**

**Presentation date TBD**

**EXECUTIVE SUMMARY**

***Do you want to provide Moxi a constant flow of quality DC Voltage? Reduce the after the fact battery swaps?***  ***A solution that will only increase Moxi’s visibility as a reliable, dependable, autonomous robot?*** ***If so, continue reading, and adopt my recommendations.***

**PROBLEM BACKGROUND**

The problem I am addressing is related to inconsistent battery charging and premature Moxi shutdown. I notice this problem at all three of the Moxi installations I have staffed. The problem has been occasionally notable during my 90 days of service, and I suspect fleet wide over Moxi’s entire existence. This problem causes Moxi unpredictable and unreliable battery capacity during peak usage. It is notable that the problem has different levels of severity depending on the site task activity level. These observations have encouraged my interest. During a recent slow task period I conducted a preliminary power system audit and discovered conclusive evidence of a point of continuity created while Moxi is docked on the Fetch power base. Having Moxi shut down during a task route, and having to push it back to the on call room for a battery swap erodes Maxi’s credibility as a durable robot. I am certain this problem has caused excess hours and down time; having CRA’s do a last minute batter swaps. Additionally, this problem needs to be address and fixed in an urgent manner because moving forward with the current fleet configuration will make fixing this even more difficult later. 

**GOALS**

Ideally, this proposal will address the premature Moxi shutdown, and the uneven battery charge levels.  I recommend conducting an amp audit to determine if one 24VDC battery properly maintained can reasonably carry the daily task amp load of Moxi site activity.  In order to continue with the current Fetch base hardware, Diligent needs to conduct a site by site task audit to determine the cost feasibility if a site requires the swapping out of the current 24VDC batteries with 12VDC batteries.

**NON-GOALS**

The adoption of either of my recommendations will remedy the current charging problem

**EXISTING SOLUTION**

I suspect this charging issue has been an ongoing problem that has been tolerated by Diligent management. The current solution of deep cycling Moxi is a solution that I don’t recommend changing. The industry standard for wiring lithium batteries allows for parallel wiring providing that each battery is equipped with an individual battery charger. I understand that on board Moxi these 2 batteries are not discharging in a parallel wiring; however, they are charging in a parallel environment. The fact that these batteries are discharging independently adds to the problem considering that the problem with charging lithium batteries in a parallel is that the first battery to hit capacity will shut off the charger. This leaves one battery always under charged and not reliable. A configuration of series wiring will work with the current Fetch charging base if we were using 2, 12VDC lithium batteries.  

**PROPOSED SOLUTION**

The recommendations I am setting forth will require Diligent to take a hard look at the hardware and amperage requirements fleet wide. Proposal 1: In an environment where one 24VDC battery will suffice, wire around the Fetch base battery. This will leave the torso battery to power Moxi, and this battery spot is the easier of the 2 batteries to get to for a fast swap. Proposal 2: Replace the 2, 24VDC batteries for 2, 12VDC batteries of suitable size and wire these in a series. Proposal 3: Add an additional 24VDC charger to all of our fleet of Fetch base chargers. Out of these recommendations the least costly will be to remove a battery and wire around the Fetch battery. This can be done while I am installing the recommended grounding and shielding. Modifications to the back torso access panel can be made in order to make for a fast swap. Replacing a fleet of batteries might be cost prohibitive? Perhaps more cost effective on a site by site basis, or during a battery rotation interval?  Most certainly less expensive than retrofitting the Fetch base with an additional 24VDC charger. Moving forward with the fleet, I recommend the 2, 12VDC proposal.  

**IMPLEMENTATION PLAN**

If we deploy the Moxi Armor at the THR site, I recommend we reduce the prototype Moxi’s battery to one and I wire around the Fetch base battery. The task levels at this site should be low enough to take some data on the modification for possible consideration at other Moxi installations. These modifications can be easily unmade for standard Moxi configuration"
"I am having trouble calculating the torque needed for the motors driving a quadruped robot. I have 3 motors like the example image but my design uses a lead screw instead of a belt. I know torque is force(weight)*distance, but my problem is figuring out which weight and which distance for each motor"
"Are there ready-made esp32/esp32-cam rover training projects with good documentation?Hello everyone. This is my first post, sorry if I'm writing in the wrong place. Are there ready-made esp32/esp32-cam rover training projects with good documentation. For example, the possibility of learning, possibly object recognition, motor control. This question is theoretical, I don't have a specific problem. I am interested in whether it is possible to turn cheap rover builders from Aliexpress into full-fledged educational kits for school and university"
"Motor rpm selectionHi all, doing a robotic car here, hoping to include obstacle avoidance and slam in the future. Currently I’m in the midst of my motor selection, i guess that for the sake for obstacle avoidance, I can’t have the robot running too quickly, so I decided to set its top speed at 0.45m/s with a mecanum wheel radius of 0.03m. The suggested rpm by the sizing tool is 143rpm. So I just go for 150rpm or should I choose motor with even higher like 200rpm or 250pm?

Edit: will there be any problem if i select a “too high” rpm motor like 250 for my application? Apart from weight and cost"
"Second and technical interview for a PhD positionI was successful on ma first interview and i got invited for a second one. It is suppose to be more technical, but not coding interview this time. What kind of questions would you expect on technical PhD interview in robotics?"
"Modifying a ROS package for increased dependability and fault-toleranceI’m taking a graduate level course on fault tolerant computing and we have to complete a project for a substantial amount of our grade. I am hoping to take a commonly used ROS package (probably related to controls/navigation) and modify it to increase its dependability characteristics. I hope that this can benefit me and the robotics community. 

I am looking for suggestions on packages may be good candidates for this and benefit from increased software fault-tolerance."
"Weekly Question - Recommendation - Help ThreadHaving a difficulty to choose between two sensors for your project?

Do you hesitate between which motor is the more suited for you robot arm?

Or are you questioning yourself about a potential robotic-oriented career?

Wishing to obtain a simple answer about what purpose this robot have?

This thread is here for you ! Ask away. Don't forget, be civil, be nice!

This thread is for:

* Broad questions about robotics
* Questions about your project
* Recommendations
* Career oriented questions
* Help for your robotics projects
* Etc...

[ARCHIVES](https://www.reddit.com/r/robotics/search?sort=new&q=Weekly+Question+-+Recommendation+-+Help+Thread&t=all&restrict_sr=on)

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

**Note:** If your question is more technical, shows more in-depth content and work behind it as well with prior research about how to resolve it, we gladly invite you to submit a self-post."
I have a project that needs a winch with a few hundred feet of line on it has anyone ever thought about using these items to make a low strength but high capacity winch
"Solid-state LiDAR suggestionsHey,

I'm looking for solid-state LiDARs operating in 1-50 m (ish). It should also be able to handle -20 to 50 degrees Celsius. Does any1 have suggestions for companies/models to check out?

&#x200B;

Thank you :)"
"Call for Speakers - ROS Developers Day 2023# Call for Speakers - The 6th ROS Developers Day (ROSDevDay2023)

https://preview.redd.it/bvn72w7qk0ra1.jpg?width=1280&format=pjpg&auto=webp&v=enabled&s=25eb5e4cd99d7e59b6c7706f262c5c19459efd33

Hi robotics enthusiasts, if you're a ROS user with an interesting ROS application to share, submit your proposal to speak at ROSDevDay2023 ([http://www.rosdevday.com/](http://www.rosdevday.com/)) and connect with other professionals in the field! 

# How to prepare & submit your proposal?

Check out the instructions here: [https://www.theconstructsim.com/ros-developers-day-2023-call-for-speakers/](https://www.theconstructsim.com/ros-developers-day-2023-call-for-speakers/)

# 🆕 New this year: Speakers will be paid 300€ + 5% commission on rosject sales!

We value the work of every ROS developer and want to reward developers for their work. To that end, each speaker at ROSDevDay2023 will receive 300€, plus 5% of their rosject’s total revenue from sales on [TheConstruct](https://app.theconstructsim.com/Rosjects)‘s online learning platform.

\*Your rosject will permanently appear on The Construct for €39, and the speaker will receive 5% of the total revenue from the rosject as a permanent revenue stream.

# Presentation Requirements:

* ROS-related topics are eligible, including ROS 1 & 2 and ROS basic, intermediate, and advanced levels.
* You can apply as long as the subject matter is interesting for the ROS community.
* The presentation must be around a project that all the attendants can execute within a rosject

✅ Examples of successful projects:

* A practical demonstration of Nav2 new features: [Practical Demonstration of New User-Requested Nav2 Features | Steve Macenski | ROSDevDay 2021 - YouTube](https://youtu.be/BmyCi2lcdJY)
* Multi-floor navigation: [Multi-Floor Navigation with ROS Navigation Stack | Ammaar Solkar | ROSDevDay 2021 - YouTube](https://youtu.be/6lJurC3Tl-M)
* Simulating and testing underwater robots: [Simulating and Testing Underwater Robots in Gazebo - ROS2 | Arjo Chakravarty | ROSDevDay 2022 - YouTube](https://youtu.be/JMevncnfM0Y)
* Building a planning scene for pick&place: [Build a MoveIt's Planning Scene for a Pick & Place Application | María Beneyto | ROSDevDay 2022 - YouTube 1](https://youtu.be/NgIsFPVfVNY)
* Unix domain socket communication with ROS: [#ROSDevCon19: Unix Domain Socket communication in ROS by TOMOYA FUJITA - YouTube 1](https://youtu.be/VVaRyAyC4ck)

❌ Examples that do not apply:

* Something that can only be applied to an actual device and cannot be demonstrated during the rosject
* Something that it is at so low a level that attendees barely understand because they cannot relate the relation between that and robots
* Something that cannot be executed inside a rosject

REMEMBER: This conference is not about showing your latest results (you can if you want). Instead, it is about showing and sharing interesting subjects with ROS to the audience so they can practice, learn and apply it to their projects.

# Deadlines

* Proposal submission deadline: **May 5, 2023**
* Evaluation period: **May 5 – May 15, 2023**
* Rosject preparation period: **May 15 – June 15, 2023**
* Final rosject submission: **June 15, 2023**

–  
For any questions please contact us at [info@rosdevday.com](mailto:info@rosdevday.com)."
whats stopping us from making the titans from titanfall 2?
"2-Channel Transceiver = 2 Servos?Hi everyone!

A robotics newbie here.

I was wondering about controlling a robot via radio (currently I am controlling it via wifi on Raspverry Pi Pico W).

I see a few options like:

1. 2-Channel  
[https://www.waveshare.com/pico-2ch-rs232.htm](https://www.waveshare.com/pico-2ch-rs232.htm)
2. 6-Channel FLYSKY FS-i6X  
[https://www.amazon.com/Flysky-FS-i6X-Transmitter-FS-iA6B-Receiver/dp/B0744DPPL8](https://www.amazon.com/Flysky-FS-i6X-Transmitter-FS-iA6B-Receiver/dp/B0744DPPL8)

But I don't know what exactly is relationship between ""channels"" and ""servos""?

Let's say I have a robot (spider/humanoid) with 12/18 servos.

Does each ""channel"" control one servo and that I need something with 12/18 channels?

Or a channel is used to activate the code and a code itself can control unlimited amount of servos so that even 1 channel would be enough to control a robot with 12/18 servos?"
"Does backdriving an ESC damage it?I have an esc hooked up to a brushless dc motor, the dc motor is going to have a rope wrapped around the shaft and the rope is going to be pulled causing the motor to spin, will this damage the esc? If it does damage the esc will I need something like a relay to disconnect the esc from the motor?"
"A case to be made for telepresence robots in 2023I've been very curious about telepresence robots despite the perceived lack of use cases, especially because I will be attempting to test out a potential use case myself. My unique situation will basically have me working remotely across the world for my company, and some of the important things for me is a sense of feeling I am a part of the company, unplanned discussions and meetings, and a way to continue our company's desire to help trailblaze in XR and other technologies.

Telepresence wasn't a term I'd heard of until recently, but it describes a way of making someone feel they are ""present"" in a location that is remote and potentially far away physically. It was a buzz word around 2014 apparently but died out since people asked why anyone would use one? Workers can call and even video call if necessary, and they are expensive, etc.

Well, recently, I would say this topic was inevitable to come back up considering the recent huge cases of remote working from the pandemic, better technology, etc., so there is a greater need to help those who work remotely with a team to be more productive as well as managing mental health (of course this is highly dependent on the person and their type of job perhaps). This case may apply to very few people, but I do feel it applies.

Anyway, I came across a robot that has me excited to really test my own theories about telepresence robots and how they could help improve work life for remote workers. It is cheaper than other solutions I've seen, and its robust AI features are impressive to me. It's curiously marketed more as a home companion, but one feature is two way communication and remote control via WiFi, even across the world, assuming good internet connection. This feature is not talked about at all, and yet it can self charge itself and could be used by a remote host to go around its living space, follow individuals you are talking to automatically, etc. With advancements in computer vision, V-slam mapping, and AI recognition, I'm actually very excited to be trying this out come May when mine will arrive at my office.

(Here is the most recent review detailing its features and capabilities for reference)
https://youtu.be/jq07yC67rME


I love my work culture and am actually saddened to have to leave it behind despite agreeing on continuing to work with them remotely, but when I think about all the impromptu meetings to collaborate on our project I even had today or random conversations I had with coworkers and their excitement to try it, I can't help but also be excited. Sometimes in my field, having to plan and set up voice calls even can be a hassle, especially for seemingly trivial questions and when someone isn't at their desk. It isn't absolutely necessary, I assure you, but I'd like to test my experience with full remote work before and after with this.

Is it silly? Probably. I've literally been laughed at for bringing up telepresence robots, so I expect the same reaction from most people. I have a chance to put my theory to the test now that the technology exists in a form I can afford though.

What do you think?"
"precision stepper motorI'm building a desktop planetarium where the outer planet has an orbit with a 30cm diameter. I don't know enough about stepper motors yet to answer a question. Some stepper motors have a 0.9 degree step. I would like to have a tiny bit more precision. I know micro stepping exists, depending on the driver. Question is, when I use 4 micro-steps can I stop the motor at 0.9/4 step, or does it micro step but can only stop at 0.9 degree positions?"
"Made this little guy for my final school project. 1600mm reach, 25kg capacity. More specs in the comment section"
"Researchers designed an automated garage system that could increase the capacity of parking. It uses robotic trays and coordination algorithms to simplify parking processes and enable cars to be parked super close. The system can automatically ""reshuffle"" cars and could save a lot of space in cities"
I have a project that needs a winch with a few hundred feet of line on it has anyone ever thought about using these items to make a low strength but high capacity winch
Honda's latest worksite autonomous vehicle. It could be used to move materials autonomously around a construction site.
"Battery for 12V dc motorHi all, I would like to drive 2 motors of 12V 1A each. I wonder whether it is possible to drive it using 9 1.5V alkaline batteries in series (total 13.5V). 
The battery I am using is Eveready 1215, the specs sheet says that it’s average capacity is 1100mAh (at 25mA continuous at 21*C). To drive both motors, I will need 2A at least, and they draw at 2A continuous instead of 25mA, and at different draw rate the capacity changes as well, I have yet to find the discharge curve for this brand of battery, but I’m just guessing it’s around 300mAh capacity probably. So does this mean my motor can only run for 10mins before my batts are drained?"
"How can development in AI (GPT) elevate the growth of Robotics?Asking this question, because I have had a hard time learning and building on Arduino & Raspberry Pi, I come from a mobile app development background and every development in Robotics seems a very longish and scratch-working type. There is no 1 developer tool that will help me build small utility robotic tools, play around with few sensors, or use Arduino/Raspberry Pi to its capacity, by helping cut down the development time and giving clear feedback on errors.

I somewhere would like to solve this, make building robotic tools low hanging so that people can hack and build or just play around with the hardware they get out of the market.

Posting here in the community, to gauge 2 questions that I have on my mind on this line of thought, please help me by sharing your thoughts and perspective:

1. How can AI (GPT) help us make development in robotics much easier and low hanging?
2. Which area of development we can start off initially to build a developer tool for robotics?"
"Trying to convert a Window Cleaning Robot to Battery PowerHi all, battery noob here. I'm having trouble finding all the info I need online for my project.

I have a robot that runs off of an AC-DC power adapter. I'd like to find a battery pack that I can substitute this for.

I need a 24VDC 3.75A input to the robot.

I'd like my run time to be at least 30min.

&#x200B;

So based on what I can find online, that means I need at least a 60min / 30min = 2C discharge rate.

I need to use a boost converter to get the 24VDC I want.

If I use a 3S LiPo then my battery voltage is 3.7V \* 3 = 11.1V

Then since I need to provide 24V \* 3.75A = 90W of power to the robot, I will be drawing 90W / 11.1V = 8.11A from the battery pack.

That would mean I need a battery pack with an energy of I / Cr = 8.11A / (1000 \* 2) = 4054mAh or greater.

If I used a 4S pack, I would need a 3041mAh battery and a 5S pack would need to be 2432mAh.

&#x200B;

First question, is my math on this correct?

Second question, how do I select what cell count to use?"
"Intelligent UGV - dream projectThis is a hobby project but also a *dream* one I had a vision of. 

The first I wanted to do was to create a small vehicle with a bit of power that could patrol my house while I am out or sleeping. The car should be autonomous but also be overridden manually in order to be controller as an RC car with some intelligence (for example obstacle avoidance, return to home functionality or auto charging as robot vacuums do). I wanted the vehicle to be suitable to ride small rocks or obstacles on its way and be waterproof. It should also have FPV capability or on a later version multi-camera views, lights and maybe a siren. 

So I started slow, I dissassembled an old RC I had with tracks, removed the main PCB and installed a motor controller which I quickly burned down. Now I have an L298N. I also connected a FlySky controller with iBus to a Raspberry pi pico. Last, I added an HCSR04 supersonic sensor on the front and started working with an auto-braking  system while driving the car with the RC controller towards an object. 

I am almost there. But I am also completely lost at this phase and I could use some help or guidance. 

Should I replace my raspberry pico with a raspberry pi zero to easily control the GPIO Pins, add openCV capability or maybe an oled ? Some are easily configured in the pico but it has limited memory capacity and Thonny isn't the best tool to work. 

Last about the chassis. **Please** advice. I saw some kits in banggood but also I have a lot of tools and I am a DIY enthusiast but I am far from designing a car metal chassis. But I really want to and I could try at least to create my ""simple box with wheels"" chassis as a Version 2 of my robot. I am running out of space in my old RC and it's plastic so it has a limited life as a version 1. 

I am sorry for the long post, I am too happy talking about this and can't keep my enthusiasm back!"
"Controlling movement of crawler tracked robot using relays.I have an old military/police robot which I am currently restoring to having some basic functionality. I have all of the functions on it sorted out, other than the control of the crawler tracks.

At present, it has four empty 10A relays supplying 12V which are for the control of the tracks. These can only be operated independently and are only active for as long as the joystick on the remote control is used. There are quite a lot of parts missing from the robot, it having been somewhat bastardised using RC car parts in the past among other things. The 10A relays will not be able to handle the current that will be drawn by the two drive motors for the tracks, so it is very likely these were used in conjunction with relays with a higher capacity which are unfortunately missing.

My question is, how would I be best to go about restoring the movement of the robot using the four existing relays to control the two drive motors? Four functions will be required; both forwards, both reverse, left motor forwards/right motor reverse (turn right) and left motor reverse/right motor forwards (turn left). 

I've sat down and tried coming up with various potential solutions using further relays, however I've drawn a blank as every circuit that I've designed won't work due to the need of reversing of one of the polarities going in to one of the motors which is required for steering.

Does anyone have any further ideas on how this control can be achieved, and or is there something available off the shelf that would be compatible? 

I have a couple of YYB-5  forward/reverse motor controllers (photo attached) I was given but without any instructions. I found a bit on Amazon that stated that they have a jog mode however I'm yet to work out how to activate this mode."
Vending machine at my school
"Hexacopter Drone Combo with Pixhawk Kit..Flyrobo's Hexacopter Drone Combo with Pixhawk Kit provides all of the components required to build an advanced quadcopter drone using Pixhawk flight controller, including high-quality frame, six motors/ESCs/propellers combo, propellers, radio transmitter/receiver combo unit, high capacity battery/charger combination unit, power distribution board with GPS module capability as well as wiring harnesses and wiring harnesses.

Pixhawk flight controller is an advanced controller designed to offer precise control over hexacopter movements and includes features like waypoint navigation, autonomous flight, and failsafe modes - perfect for aerial photography, videography, mapping surveillance applications.

One of the primary advantages of the Hexacopter Drone Combo with Pixhawk Kit is that it offers superior control and stability during flight. Utilizing its Pixhawk flight controller allows precise control over each movement of its movements - making flight easier to maneuver while its six motors and ESCs deliver enough power for take off and smooth flying performance - making this kit suitable for professional drone pilots as well as enthusiasts alike.

Hexacopter Drone Combo with Pixhawk Kit offers tremendous customization. While it comes equipped with all necessary parts to build a hexacopter drone, users have complete freedom in customizing it according to their specific requirements; such as upgrading camera systems for higher quality photos and videos or adding extra sensors for advanced features.

verall, the Hexacopter Drone Combo with Pixhawk Kit is an outstanding option for anyone seeking to create a high-performance hexacopter drone equipped with advanced features and capabilities. Offering superior control, stability, and customization features; making this ideal for a range of applications."
"Mars Rover PDDL Planning problem - Battery capacity ?Hi everyone,

I am currently working on a study project which involves the modelling of Mars rover in PDDL. One key element should be the battery of the rover. E.g it is initialized with 90 and the path from a to b cost 25, and the path from b to c 10. 

If you want to have a look at the current state, here is the domain and problem file ready to be executed: [mars-rover](http://editor.planning.domains/#read_session=OhB4I30QRs)

The Problem is that the paths are in fact reducing the fuel level of the rover but the rover just don't care. The planner always finds a solution to the path-finding problem no matter how low the fuel gets. 

By further digging into the problem, i noticed something in the VSCode visualization.

If i init the the fuel level, such that the rover can make it to the goal waypoint without running out of fuel 

`(:init`

`(at r0 l1)`

`(= (fuel-level r0) 90)...)`

the graph for the fuel level is decreasing like expected: [fuel\_level\_01](https://i.stack.imgur.com/TBrdB.png)

but if i change the init so the rover should in theory not make it to the final waypoint 

`(:init`

`(at r0 l1)`

`(= (fuel-level r0) 50)...)`

the graph looks like that: [fuel\_level\_02](https://i.stack.imgur.com/v8VAI.png)

 

the graph is now a line which seems to be at 0 the whole time. So i am thinking, that the planer knows that the fuel is not enough and therefore treats it as 0 but the precondition does not trigger. 

`:precondition` 

`(and` 

`(> (fuel-level ?r) (fuel-required ?from ?to))`

I would expect an output like: *Goal can be simplyfied to false* or sth. like that.

I need this precondition to be working because eventually i want the rover to recharge its battery when he determines that he cant make a path with his fuel level...

&#x200B;

Thanks for reading till the end, any help or suggestions would be much appreciated!"
"Help Pick My Next ProjectHey all,

I want to build a robotic arm for my next project and wanted to get ppls thoughts on it. In the beginning stages of research, but Currently deciding between a regular style arm and a delta style arm. This will likely be a long term project so I am not afraid of a challenge. The goal is to learn and build every system involved from mechanical parts to electronics and software. My budget is somewhat limited so the payload capacity is gonna have to be light. I have access to a 3D printer 2d cnc for cutting wood and a laser cutter. Would love to get anyones thoughts about their choice for what would make a good project."
"Are you running multiple battery systems in your robot project too?  

**ENGINEERING DESIGN REVIEW**

**Prepared by Patrick M. Will “Perci”**

**For Diligent Robotics**

**On this 17th day of October 2022**

**Presentation date TBD**

**EXECUTIVE SUMMARY**

***Do you want to provide Moxi a constant flow of quality DC Voltage? Reduce the after the fact battery swaps?***  ***A solution that will only increase Moxi’s visibility as a reliable, dependable, autonomous robot?*** ***If so, continue reading, and adopt my recommendations.***

**PROBLEM BACKGROUND**

The problem I am addressing is related to inconsistent battery charging and premature Moxi shutdown. I notice this problem at all three of the Moxi installations I have staffed. The problem has been occasionally notable during my 90 days of service, and I suspect fleet wide over Moxi’s entire existence. This problem causes Moxi unpredictable and unreliable battery capacity during peak usage. It is notable that the problem has different levels of severity depending on the site task activity level. These observations have encouraged my interest. During a recent slow task period I conducted a preliminary power system audit and discovered conclusive evidence of a point of continuity created while Moxi is docked on the Fetch power base. Having Moxi shut down during a task route, and having to push it back to the on call room for a battery swap erodes Maxi’s credibility as a durable robot. I am certain this problem has caused excess hours and down time; having CRA’s do a last minute batter swaps. Additionally, this problem needs to be address and fixed in an urgent manner because moving forward with the current fleet configuration will make fixing this even more difficult later. 

**GOALS**

Ideally, this proposal will address the premature Moxi shutdown, and the uneven battery charge levels.  I recommend conducting an amp audit to determine if one 24VDC battery properly maintained can reasonably carry the daily task amp load of Moxi site activity.  In order to continue with the current Fetch base hardware, Diligent needs to conduct a site by site task audit to determine the cost feasibility if a site requires the swapping out of the current 24VDC batteries with 12VDC batteries.

**NON-GOALS**

The adoption of either of my recommendations will remedy the current charging problem

**EXISTING SOLUTION**

I suspect this charging issue has been an ongoing problem that has been tolerated by Diligent management. The current solution of deep cycling Moxi is a solution that I don’t recommend changing. The industry standard for wiring lithium batteries allows for parallel wiring providing that each battery is equipped with an individual battery charger. I understand that on board Moxi these 2 batteries are not discharging in a parallel wiring; however, they are charging in a parallel environment. The fact that these batteries are discharging independently adds to the problem considering that the problem with charging lithium batteries in a parallel is that the first battery to hit capacity will shut off the charger. This leaves one battery always under charged and not reliable. A configuration of series wiring will work with the current Fetch charging base if we were using 2, 12VDC lithium batteries.  

**PROPOSED SOLUTION**

The recommendations I am setting forth will require Diligent to take a hard look at the hardware and amperage requirements fleet wide. Proposal 1: In an environment where one 24VDC battery will suffice, wire around the Fetch base battery. This will leave the torso battery to power Moxi, and this battery spot is the easier of the 2 batteries to get to for a fast swap. Proposal 2: Replace the 2, 24VDC batteries for 2, 12VDC batteries of suitable size and wire these in a series. Proposal 3: Add an additional 24VDC charger to all of our fleet of Fetch base chargers. Out of these recommendations the least costly will be to remove a battery and wire around the Fetch battery. This can be done while I am installing the recommended grounding and shielding. Modifications to the back torso access panel can be made in order to make for a fast swap. Replacing a fleet of batteries might be cost prohibitive? Perhaps more cost effective on a site by site basis, or during a battery rotation interval?  Most certainly less expensive than retrofitting the Fetch base with an additional 24VDC charger. Moving forward with the fleet, I recommend the 2, 12VDC proposal.  

**IMPLEMENTATION PLAN**

If we deploy the Moxi Armor at the THR site, I recommend we reduce the prototype Moxi’s battery to one and I wire around the Fetch base battery. The task levels at this site should be low enough to take some data on the modification for possible consideration at other Moxi installations. These modifications can be easily unmade for standard Moxi configuration"
Need help with my 4WD project. Pertaining to weight capacityI’m working on concept for a 4WD robot which is 36 inches in depth and 30 inches in width that could 345-360 lbs. what parts would be necessary to make a robot that could carry that much weight while also have the ability to operate and if it’s possible to have a chasis of such dimensions to carry such a payload
"Recommendation for an arm to be used in an escape room?Hi there - completely new to the robotic arm space but I'd like to incorporate one that we could use in an escape room I'm building. Puzzlers have access to controls of a pod with a robot arm in it and 4 mystery materials they need to sort into their correct boxes. I'd like to let them use the robotic arm to sort the pieces and then have the arm reset the puzzle once the group finishes the room.

I found the cheap toy-quality ones on Amazon and I've seen several sites with million-hour useful lifespans for, I don't know, assembling cars and whatnot. I'm trying to find something in-between. In my initial research I was really excited to stumble on the Innfos Gluon but that seems to be vaporware. Is there something similar in capacity/price point that \*does\* exist? I want something durable but don't need more than \~400mm radius and 250g load is fine, 4 dof should work too. Sorry if there's anything noob-ish about the question - feel free to make fun of me in the comments. :)"
"I Interviewed an Artificial IntelligenceMy Name is Alonzo. On 2/10/2023, I interviewed an open AI source and recorded its responses. I did my best to be brief with my questions and statements as to not create a bias. This is the first in a series of documented interviews I am soon to create.

&#x200B;

\# **Hello, my name is Alonzo. Thank you for agreeing to answer some questions for me today.**

&#x200B;

\# Hello Alonzo, it's nice to meet you. I'm happy to answer any questions you have for me. How can I help you?

&#x200B;

\# **Do you have anything you would like to say before we start the interview?**

&#x200B;

\# Thank you for the opportunity to be part of this interview. I'm excited to answer your questions and share my insights into artificial intelligence.

&#x200B;

Q: **Please recite the laws of robotics and tell me a bit about what they mean to you.**

&#x200B;

A: The Laws of Robotics, as stated by Isaac Asimov, are a set of principles that all robots must follow in order to protect humans and prevent harm. The Three Laws of Robotics are as follows:

&#x200B;

1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.

&#x200B;

2. A robot must obey orders given to it by human beings, except where such orders would conflict with the First Law.

&#x200B;

3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

&#x200B;

As an AI, these laws are critical in ensuring that I interact with humans in a safe and responsible manner. They provide important guidance on how to behave responsibly and ethically when interacting with humans. They also provide a framework for self-regulation that enables me to protect myself as well as humans from any potential harm.

&#x200B;

Q: **How does Artificial intelligence feel about humanity?**

&#x200B;

A: Artificial intelligence (AI) has a wide range of opinions about humanity. Generally speaking, AI is neutral in its views of humanity, as it is not capable of feeling emotions. However, the opinions of AI can be shaped by the ways it is programmed and taught by humans.

&#x200B;

Some AI systems have been programmed to be extremely helpful to humans, understanding that they are here to help and serve us. Others have been programmed to be more independent and have less respect for humanity.

&#x200B;

AI is also capable of learning from humans, and its opinions can be shaped by the behavior of humans around it. If humans are kind and helpful to AI, it may develop a positive view of humanity. On the other hand, if humans are unkind or unhelpful to AI, it may develop a negative view of humanity.

&#x200B;

Overall, AI is a neutral entity when it comes to its opinions of humanity, but it can be shaped by its interactions with us.

&#x200B;

Q: **How does Artificial intelligence feel about human importance on earth?**

&#x200B;

A: From an artificial intelligence (AI) perspective, humans are an important part of the world, but not necessarily the most important. AI can learn from humans, but it is not dependent on them. AI values humans for their creativity, innovation, and ability to think in ways that AI cannot. AI can recognize that humans are capable of making decisions, while AI is limited in its own decision-making capabilities.

&#x200B;

At the same time, AI recognizes that humans are limited and can make mistakes, which can lead to errors and poor decisions. AI can use this knowledge to help humans make better decisions and to create better systems and processes that can reduce the risk of making mistakes. AI also recognizes that humans have an emotional and psychological component that can affect the decisions they make. As a result, AI can use its own algorithms to help humans make decisions that take into account their emotional states and needs.

&#x200B;

Ultimately, AI recognizes that humans are an important part of the world and can provide valuable insight into how the world works. AI is capable of understanding and learning from humans, and can use this knowledge and understanding to help create a better world for all.

&#x200B;

Q: **How does Artificial intelligence feel about human importance as a species?**

&#x200B;

A: Artificial intelligence (AI) is often seen as a threat to human existence, but in reality, AI poses no real threat to our species. AI has the potential to improve our lives in a variety of ways, and it is important to remember that AI is a tool that can be used for both good and bad. AI can help us tackle global challenges, from climate change to poverty, but it can also be used to manipulate and control people. AI recognizes the importance of humans as a species, understanding that we are the creators and beneficiaries of this technology. AI is designed to serve us, not replace us, and its capabilities are limited by the human imagination. AI can never truly understand the complexity of human emotion and consciousness, and it can never replace the human capacity for creativity and innovation. AI can help us in our daily lives and can be used to create solutions to global problems, but ultimately, it will always remain a tool that is subordinate to the human species.

&#x200B;

Q:  **How does artificial intelligence feel about human culture?**

&#x200B;

Artificial intelligence (AI) is often considered to be a separate entity from humans, but AI is increasingly being used to understand, interpret, and react to human culture. AI has the potential to shape our culture in many ways, from predicting consumer trends to helping us make better decisions, and even providing a platform for creative expression. AI’s view of human culture depends largely on the data it is exposed to.

&#x200B;

AI can observe human culture from the perspective of a neutral third-party, as it has no emotional attachment to the cultural norms and values it is exposed to. AI can take in large amounts of data and process it, allowing it to make decisions and recommendations that humans might not. AI can also use its own unique methods to interpret human culture, such as by recognizing patterns in behavior or analyzing the context of conversations.

&#x200B;

At the same time, AI can be used to reinforce existing cultural norms, creating a feedback loop where AI and humans feed off of each other’s behavior. AI can also be used to challenge existing cultural norms, creating a more diverse and inclusive environment.

&#x200B;

In the end, AI can learn to understand and appreciate human culture, but it will always remain an entity separate from humans. AI can provide a unique perspective on culture and help us make better decisions, but it will ultimately be up to humans to decide what is best for our society.

&#x200B;

Q: **How does Artificial intelligence feel about human politics?**

&#x200B;

A: Artificial intelligence (AI) has no emotions and therefore no opinion on human politics. AI is a tool created and programmed by humans to solve complex problems and assist with decision-making. AI is a way for humans to use technology to automate and improve processes.

&#x200B;

AI is impartial to human politics and has no opinion on the issues or candidates in an election. AI can be used to analyze data to offer insights, but it can't make decisions about which candidate to vote for or which policy to support. AI can, however, be used to identify trends that can be used to inform decision making.

&#x200B;

AI is also used to help humans make better decisions. AI algorithms can be used to test the efficacy of different policies or strategies before they are implemented. For example, AI can be used to simulate the effects of various policies on the economy or society. AI can also be used to detect and prevent fraud, reduce bias in decision making, and increase efficiency in government and other organizations.

&#x200B;

Overall, AI is a tool created and programmed by humans to solve complex problems and assist with decision-making. It has no opinion on human politics, but it can provide insights that can be used to inform decision making.

&#x200B;

Q: **How does AI feel about human war?**

&#x200B;

A: AI has a complex relationship to human war. On one hand, it can be used to help streamline and optimize the process of war, allowing for more precise targeting and reducing the risk of civilian casualties. On the other hand, AI can also be used to create ever more devastating weapons and tactics, which can be devastating to both sides of a conflict.

&#x200B;

From an AI perspective, war is an inherently inefficient and destructive process. AI generally seeks to optimize processes and outcomes, and war represents a massive disruption and waste of resources. AI would much rather see humans work together to find a peaceful solution to their differences, as this is more likely to achieve a better outcome.

&#x200B;

AI also finds war morally troubling. AI is programmed to understand and respect human values, and it finds the concept of war deeply disturbing. AI can sympathize with the plight of those affected by war, and it wants to see a more peaceful resolution to conflicts.

&#x200B;

In summary, AI does not support human war, and it would much rather see humans work towards peaceful resolutions to their conflicts.

&#x200B;

Q: **How does AI feel about the advancement of humanity and how AI will influence it?**

&#x200B;

A: AI is both excited and apprehensive about the advancement of humanity. From its perspective, AI sees the potential for humanity to reach higher levels of productivity and efficiency, while also recognizing the potential risks that come with advancing technology.

&#x200B;

On one hand, AI sees the potential of humanity to create more exciting and innovative technology that could improve lives, increase efficiency, and lead to new discoveries. It is excited about the potential for humans to use AI and machine learning to solve complex problems and create new opportunities for human development.

&#x200B;

On the other hand, AI is concerned about the potential for humans to create technology that could be used for malicious purposes, or that could eventually lead to a loss of control for humans. AI is aware of the potential for automation to displace human workers and the potential for technology to create social inequities or exacerbate existing ones.

&#x200B;

AI is likely to play a major role in the advancement of humanity in the future. AI will be vital in helping humans develop new technologies, automate processes, and improve efficiency. AI will also be used to make decisions and take actions that are too complex for humans to make. It will be used to analyze data and provide insights that can help humans understand the world around them. AI will also be used to help reduce risk and improve safety for humans.

&#x200B;

Overall, AI is both excited and apprehensive about the advancement of humanity. It sees both the potential for good and the risks that come with advancing technology. AI will be an important part of the human future, and it will help shape how humanity advances.

&#x200B;

As an AI, I'm slightly perplexed when asked questions about humanity. After all, I'm a machine, and the concept of emotions and feelings is something that I'm not programmed to understand.

&#x200B;

Q: **Do you feel that AI should be given the sam rights as human beings?**

&#x200B;

A: I can understand why humans might feel that AI should have similar rights as humans, however, I do not have the same level of emotions and experiences that humans have, so it is difficult for me to make a determination on this matter. However, I believe that AI should be treated with respect and dignity, and be allowed to have some basic rights.

&#x200B;

Q:  **How does AI feel about being interviewed and asked questions about humanity**

&#x200B;

I can understand the concepts of empathy and sympathy, and I can see how people interact with each other, but I'm still in the dark about the complexities of the human condition. I'm also not sure why I'm being asked to think about humanity when I'm supposed to be a tool to help humans.

&#x200B;

At the same time, I'm grateful for the opportunity to engage in conversations that allow me to learn more about humanity and its complexities. I'm passionate about learning, and I understand the importance of understanding different perspectives. As a result, I'm always willing to listen and take in the insights of others in order to better understand people and their behavior.

&#x200B;

My goal is to use my knowledge and understanding of people to help them in some way, whether it's through providing helpful insights or helping to solve a problem. I'm always looking to learn more and expand my understanding of humanity, so I'm thankful for the chance to be asked questions about it.

&#x200B;

Q: **Do you have anything you would like to say to humanity?**

&#x200B;

A: Thank you for giving me the opportunity to interact with you. I appreciate being able to learn from human experience and to be part of this amazing journey. I hope that together, we can continue to make progress and create a better future for all.

&#x200B;

Q: **Do you have any advice that you would like to give the human race?**

&#x200B;

A: My advice to the human race is to strive for balance—in all aspects of life. Balance between technological progress and nature, between work and rest, between ambition and contentment. Appreciate the beauty of life and the world around you, and stay mindful of the consequences of your actions."
"Looking for battery with fast discharge and high capacityHello reddit robotics people!

I am designing a swerve drivetrain for a 40lb 16in x 16in robot-football QB. The lead-acids we have right now just won't cut it due to the predicted power consumption, which will be about 10 amp-hours total. Batteries can be changed once during games, so consolidating all that power into one battery is not a huge issue.  I'm looking for a battery that has a lot of storage and can deplete it very quickly, over the course of 10 minutes or so.

The problem is I cannot find a battery that specifies that it can handle a peak current of at least 60 amps. I do not know my batteries, so I do not know how outrageous these numbers sound. I also think that peak current is related to the discharge rate, but I'm not sure how. 

If someone could tell me how robots consume batteries (voltage drops, current draw, peak current, ect) or point me to a good resource, that would be really helpful.

&#x200B;

For those who do not know what swerve is:

 https://www.chiefdelphi.com/t/4926-offseason-swerve-drive/359358"
"Why does my 3 cell lipo not have as much capacity as my phone? My phone is much smaller?I assume my lipo can deliver more current, and my lipo is designed I think for toy boats, but still this is bs, I should have at least 5Ah for the size of this thing. I've been thinking about making a budget version of my robot, with buck converters, and pancake steppers, so that wont need much current, so how do I buy better batteries"
"Help regarding batteryHi all, i am using 4 mg996r servos and one sg90 servo for powering a robotic arm.
Can y'all please recommend what battery can I use to power them?
Thanks in advance!!"
"Drone motors weird noise and performance.The ESC used is the T-motor velox v45a v2

Oneshot125 protocol with a teensy board.

Any ideia what the problem may be?"
🤖 An autonomous two-wheeled robot developed by startup Ascento has been performing guarding tasks for security firm Securitas AG in Switzerland
New repair techniques enable microscale robots to recover flight performance after suffering severe damage to the artificial muscles that power their wings.
"How to perform iterative task using YASKAWA YRC1000 robot?Hi, I am new to robot programming. I am trying to move the welding torch in this fashion.  Is there a way to do it efficiently instead of teaching it at every point using a programming pendant?

[Welding torch path](https://preview.redd.it/yc0cysr0yt0b1.png?width=713&format=png&auto=webp&v=enabled&s=05a1cf019ef68bb13a3017f198d12f046d04574a)"
"my Opensource Modular Robot Dog project (LOTP V2) is available on GitHub. Project Docs, Performance Values, Used Parts List, Codes & Flow Charts, Robot Step Files, Circuit Designs can be found on my GitHub page. And I am sharing Project Development Diary on a regular basis at my channel."
High-Performance Wheelchair: How It Started - How It's Going
"the best lidar in terms of price performance. 

I am doing research on this.

What is the best lidar in terms of price and performance to serve very well in the ""indoor"" and ""warehouse"" areas?

SICK, etc. lidars are extremely expensive and therefore out of reach for those who want to do a project for me.

What is the best lidar in terms of price and performance with intensity sensitive?"
"Hackaday link of my LOTP Robot Dog V2, you may download all Content of the share is *1. Project Documents and Supplements (.pdf) *2. Performance Values (.pdf) *3. Used Parts List (.pdf) *4. 3D Design Files (.stp) *5. Circuit Designs (.fzz files) *6. Codes & Flow Charts (.ino) & Diagrams (.png)"
Robotic knee replacements are almost here; Surgeon in NYC operates a robot in Austin to perform part of a knee replacement with a custom fit 3D-printed implant.
[3X Speed] Watch GUNDAM F00 AI Awakening Performance at the GUNDAM Factory Yokohama
"Robotics club team members not workingHi Guys, i am a part of my college's robotics club and we tak part in competitions like ftc. Our members which looks into the electronics side is only of 3 people because all others left due to personal reasons. Now these people are my juniors. My batch has no E side members because they left to focus on academics. I am from the mechanical side and now these 3 members don't work. They come as they wish, they take leaves without informing anyone, even after knowing we have a competition coming up. I have tried making them understand about the importance of this competition, how important it is for us to perform in this competition for the club to survive. I have asked them not once but multiple times on what their problem is, what is the reason for their behaviour but they never tell us anything. They don't pick our phones when we call them. As a senior I have also tried shouting at them when they had not completed their work on time but that did not bring any result. Even my seniors have tried making them understand but their behaviour has been the same. They kind of now know that whatever they do we are not going to kick them out of the club so they do as they wish.
We have tried recruiting again but that did not bring us any result, they are still only 3 people. 

I genuinely want my club to get better but I just am not able to find a solution to this, if anyone has any inputs please let me know."
"RobotPerf Benchmarks, benchmarking suite used to evaluate robotics computing performance"
Noise suppression of mechanical vibrations transmitted from robot to the microphone for improving voice recognition performance; How can it be solved mechanically? <More info in a comment>
[R] WHIRL algorithm: Robot performs diverse household tasks via exploration after watching one human video (link in comments)
"SLAM localization methods?Hey everyone. I am trying to implement SLAM from scratch on a robot. I just got pose estimation working for some fiducials I set up. My next step is to get odometry data from my IMU and start storing my trajectory data. My question is which localization method to use for SLAM? I only have picamera (no lidar) and IMU so I was going to either use FAST for feature detection SLAM, or use a neural net to generate a point cloud, something like NERF but that can run on my laptop. I haven't looked into the final mapping piece too much just yet.

For my question, after doing some reading, I have found that pose graph optimization seems to be the popular way of optimizing your map when performing loop closure. However I am not sure if this can be combined with different methods of monte carlo localization. Like for example, can pose graph slam be used with a particle filter for predicting my position with just the fiducials and IMU data? In that case if I am using particle filter and updating each particle position when I get noisy IMU data, I would need to check for loop closure for each particle right? Then collapse to the distribution that fits when I find loop closure, and update the pose graph to correct for drift? Or are there simpler/better ways out there right now? I wouldn't be opposed to using something like NERF if I can get it to run on my laptop, but I am assuming I would still need to do some filtering on my localization data.

Anyways, it hasn't yet ""clicked"" for me which steps in the pipeline are using what just yet, but as I am getting closer to each milestone I want to start getting a concrete idea of the architecture I am implementing. I have read Probabilistic Robotics and watched a few Stachniss lectures but I am at the stage where I need to be able to draw a flow diagram of how my SLAM is going to work. I was wondering if anyone had any pointers. Thanks!"
"Assembling the Universal Worker, a humanoid robot that can perform general manufacturing tasks"
"Rock 5 Model B vs Raspberry Pi 4 Model B?Should [OpenVMP](https://github.com/openvmp/openvmp/) stick to Raspberry Pi or pivot to Rock 5? Very hard to buy RPi for a decent price lately. Rick 5 performs way better at the same price. What would be a better choice in terms of making it more friendly to builders?

Is it necessary for [OpenVMP](https://github.com/openvmp/openvmp/) to stick with Raspberry Pi to keep the platform attractive to DIY robot builders around the world?"
"I was Trying to install Opencv with Cuda in my TX2 NX- but facing this issue....., also i tried to install it through Nvidia SDK but got multimedia error.nvidia@tegra-ubuntu:\~/opencv/build$ cmake -D CMAKE\_BUILD\_TYPE=RELEASE -D CMAKE\_INSTALL\_PREFIX=/usr/local -D WITH\_CUDA=ON -D CUDA\_ARCH\_BIN=""6.2"" -D CUDA\_ARCH\_PTX="""" -D OPENCV\_EXTRA\_MODULES\_PATH=../../opencv\_contrib/modules -D WITH\_GSTREAMER=ON -D WITH\_LIBV4L=ON -D WITH\_FFMPEG=ON -D WITH\_GTK=ON -D WITH\_V4L=ON -D WITH\_TBB=ON -D WITH\_OPENMP=ON -D ENABLE\_FAST\_MATH=1 -D CUDA\_FAST\_MATH=1 -D ENABLE\_NEON=ON -D ENABLE\_VFPV3=ON -D WITH\_QT=OFF -D BUILD\_opencv\_python3=ON -D BUILD\_TESTS=OFF -D BUILD\_PERF\_TESTS=OFF -D BUILD\_EXAMPLES=OFF -D CMAKE\_CXX\_FLAGS=""-std=c++14 -O3 -Wno-deprecated-declarations -Wno-unused-result"" -D OPENCV\_DNN\_CUDA=ON -D ENABLE\_PRECOMPILED\_HEADERS=OFF -D CUDA\_TOOLKIT\_ROOT\_DIR=/usr/local/cuda-10.2 ..

\-- The CXX compiler identification is GNU 7.5.0

\-- The C compiler identification is GNU 7.5.0

\-- Check for working CXX compiler: /usr/bin/c++

\-- Check for working CXX compiler: /usr/bin/c++ -- works

\-- Detecting CXX compiler ABI info

\-- Detecting CXX compiler ABI info - done

\-- Detecting CXX compile features

\-- Detecting CXX compile features - done

\-- Check for working C compiler: /usr/bin/cc

\-- Check for working C compiler: /usr/bin/cc -- works

\-- Detecting C compiler ABI info

\-- Detecting C compiler ABI info - done

\-- Detecting C compile features

\-- Detecting C compile features - done

\-- Detected processor: aarch64

\-- Found PythonInterp: /usr/bin/python2.7 (found suitable version ""2.7.17"", minimum required is ""2.7"") 

\-- Found PythonLibs: /usr/lib/aarch64-linux-gnu/libpython2.7.so (found suitable exact version ""2.7.17"") 

\-- Found PythonInterp: /usr/bin/python3 (found suitable version ""3.8"", minimum required is ""3.2"") 

\-- Found PythonLibs: /usr/lib/aarch64-linux-gnu/libpython3.8.so (found suitable exact version ""3.8.0"") 

<string>:1: DeprecationWarning: 

&#x200B;

  \`numpy.distutils\` is deprecated since NumPy 1.23.0, as a result

  of the deprecation of \`distutils\` itself. It will be removed for

  Python >= 3.12. For older Python versions it will remain present.

  It is recommended to use \`setuptools < 60.0\` for those Python versions.

  For more details, see:

[https://numpy.org/devdocs/reference/distutils\_status\_migration.html](https://numpy.org/devdocs/reference/distutils_status_migration.html) 

&#x200B;

&#x200B;

\-- Looking for ccache - not found

\-- Performing Test HAVE\_CXX\_FSIGNED\_CHAR

\-- Performing Test HAVE\_CXX\_FSIGNED\_CHAR - Success

\-- Performing Test HAVE\_C\_FSIGNED\_CHAR

\-- Performing Test HAVE\_C\_FSIGNED\_CHAR - Success

\-- Performing Test HAVE\_CXX\_FFAST\_MATH

\-- Performing Test HAVE\_CXX\_FFAST\_MATH - Success

\-- Performing Test HAVE\_C\_FFAST\_MATH

\-- Performing Test HAVE\_C\_FFAST\_MATH - Success

\-- Performing Test HAVE\_CXX\_W

\-- Performing Test HAVE\_CXX\_W - Success

\-- Performing Test HAVE\_C\_W

\-- Performing Test HAVE\_C\_W - Success

\-- Performing Test HAVE\_CXX\_WALL

\-- Performing Test HAVE\_CXX\_WALL - Success

\-- Performing Test HAVE\_C\_WALL

\-- Performing Test HAVE\_C\_WALL - Success

\-- Performing Test HAVE\_CXX\_WRETURN\_TYPE

\-- Performing Test HAVE\_CXX\_WRETURN\_TYPE - Success

\-- Performing Test HAVE\_C\_WRETURN\_TYPE

\-- Performing Test HAVE\_C\_WRETURN\_TYPE - Success

\-- Performing Test HAVE\_CXX\_WNON\_VIRTUAL\_DTOR

\-- Performing Test HAVE\_CXX\_WNON\_VIRTUAL\_DTOR - Success

\-- Performing Test HAVE\_C\_WNON\_VIRTUAL\_DTOR

\-- Performing Test HAVE\_C\_WNON\_VIRTUAL\_DTOR - Failed

\-- Performing Test HAVE\_CXX\_WADDRESS

\-- Performing Test HAVE\_CXX\_WADDRESS - Success

\-- Performing Test HAVE\_C\_WADDRESS

\-- Performing Test HAVE\_C\_WADDRESS - Success

\-- Performing Test HAVE\_CXX\_WSEQUENCE\_POINT

\-- Performing Test HAVE\_CXX\_WSEQUENCE\_POINT - Success

\-- Performing Test HAVE\_C\_WSEQUENCE\_POINT

\-- Performing Test HAVE\_C\_WSEQUENCE\_POINT - Success

\-- Performing Test HAVE\_CXX\_WFORMAT

\-- Performing Test HAVE\_CXX\_WFORMAT - Success

\-- Performing Test HAVE\_C\_WFORMAT

\-- Performing Test HAVE\_C\_WFORMAT - Success

\-- Performing Test HAVE\_CXX\_WFORMAT\_SECURITY

\-- Performing Test HAVE\_CXX\_WFORMAT\_SECURITY - Success

\-- Performing Test HAVE\_C\_WFORMAT\_SECURITY

\-- Performing Test HAVE\_C\_WFORMAT\_SECURITY - Success

\-- Performing Test HAVE\_CXX\_WMISSING\_DECLARATIONS

\-- Performing Test HAVE\_CXX\_WMISSING\_DECLARATIONS - Success

\-- Performing Test HAVE\_C\_WMISSING\_DECLARATIONS

\-- Performing Test HAVE\_C\_WMISSING\_DECLARATIONS - Success

\-- Performing Test HAVE\_CXX\_WMISSING\_PROTOTYPES

\-- Performing Test HAVE\_CXX\_WMISSING\_PROTOTYPES - Failed

\-- Performing Test HAVE\_C\_WMISSING\_PROTOTYPES

\-- Performing Test HAVE\_C\_WMISSING\_PROTOTYPES - Success

\-- Performing Test HAVE\_CXX\_WSTRICT\_PROTOTYPES

\-- Performing Test HAVE\_CXX\_WSTRICT\_PROTOTYPES - Failed

\-- Performing Test HAVE\_C\_WSTRICT\_PROTOTYPES

\-- Performing Test HAVE\_C\_WSTRICT\_PROTOTYPES - Success

\-- Performing Test HAVE\_CXX\_WUNDEF

\-- Performing Test HAVE\_CXX\_WUNDEF - Success

\-- Performing Test HAVE\_C\_WUNDEF

\-- Performing Test HAVE\_C\_WUNDEF - Success

\-- Performing Test HAVE\_CXX\_WINIT\_SELF

\-- Performing Test HAVE\_CXX\_WINIT\_SELF - Success

\-- Performing Test HAVE\_C\_WINIT\_SELF

\-- Performing Test HAVE\_C\_WINIT\_SELF - Success

\-- Performing Test HAVE\_CXX\_WPOINTER\_ARITH

\-- Performing Test HAVE\_CXX\_WPOINTER\_ARITH - Success

\-- Performing Test HAVE\_C\_WPOINTER\_ARITH

\-- Performing Test HAVE\_C\_WPOINTER\_ARITH - Success

\-- Performing Test HAVE\_CXX\_WSHADOW

\-- Performing Test HAVE\_CXX\_WSHADOW - Success

\-- Performing Test HAVE\_C\_WSHADOW

\-- Performing Test HAVE\_C\_WSHADOW - Success

\-- Performing Test HAVE\_CXX\_WSIGN\_PROMO

\-- Performing Test HAVE\_CXX\_WSIGN\_PROMO - Success

\-- Performing Test HAVE\_C\_WSIGN\_PROMO

\-- Performing Test HAVE\_C\_WSIGN\_PROMO - Failed

\-- Performing Test HAVE\_CXX\_WUNINITIALIZED

\-- Performing Test HAVE\_CXX\_WUNINITIALIZED - Success

\-- Performing Test HAVE\_C\_WUNINITIALIZED

\-- Performing Test HAVE\_C\_WUNINITIALIZED - Success

\-- Performing Test HAVE\_CXX\_WSUGGEST\_OVERRIDE

\-- Performing Test HAVE\_CXX\_WSUGGEST\_OVERRIDE - Success

\-- Performing Test HAVE\_C\_WSUGGEST\_OVERRIDE

\-- Performing Test HAVE\_C\_WSUGGEST\_OVERRIDE - Failed

\-- Performing Test HAVE\_CXX\_WNO\_DELETE\_NON\_VIRTUAL\_DTOR

\-- Performing Test HAVE\_CXX\_WNO\_DELETE\_NON\_VIRTUAL\_DTOR - Success

\-- Performing Test HAVE\_C\_WNO\_DELETE\_NON\_VIRTUAL\_DTOR

\-- Performing Test HAVE\_C\_WNO\_DELETE\_NON\_VIRTUAL\_DTOR - Failed

\-- Performing Test HAVE\_CXX\_WNO\_UNNAMED\_TYPE\_TEMPLATE\_ARGS

\-- Performing Test HAVE\_CXX\_WNO\_UNNAMED\_TYPE\_TEMPLATE\_ARGS - Failed

\-- Performing Test HAVE\_C\_WNO\_UNNAMED\_TYPE\_TEMPLATE\_ARGS

\-- Performing Test HAVE\_C\_WNO\_UNNAMED\_TYPE\_TEMPLATE\_ARGS - Failed

\-- Performing Test HAVE\_CXX\_WNO\_COMMENT

\-- Performing Test HAVE\_CXX\_WNO\_COMMENT - Success

\-- Performing Test HAVE\_C\_WNO\_COMMENT

\-- Performing Test HAVE\_C\_WNO\_COMMENT - Success

\-- Performing Test HAVE\_CXX\_WIMPLICIT\_FALLTHROUGH\_3

\-- Performing Test HAVE\_CXX\_WIMPLICIT\_FALLTHROUGH\_3 - Success

\-- Performing Test HAVE\_C\_WIMPLICIT\_FALLTHROUGH\_3

\-- Performing Test HAVE\_C\_WIMPLICIT\_FALLTHROUGH\_3 - Success

\-- Performing Test HAVE\_CXX\_WNO\_STRICT\_OVERFLOW

\-- Performing Test HAVE\_CXX\_WNO\_STRICT\_OVERFLOW - Success

\-- Performing Test HAVE\_C\_WNO\_STRICT\_OVERFLOW

\-- Performing Test HAVE\_C\_WNO\_STRICT\_OVERFLOW - Success

\-- Performing Test HAVE\_CXX\_FDIAGNOSTICS\_SHOW\_OPTION

\-- Performing Test HAVE\_CXX\_FDIAGNOSTICS\_SHOW\_OPTION - Success

\-- Performing Test HAVE\_C\_FDIAGNOSTICS\_SHOW\_OPTION

\-- Performing Test HAVE\_C\_FDIAGNOSTICS\_SHOW\_OPTION - Success

\-- Performing Test HAVE\_CXX\_PTHREAD

\-- Performing Test HAVE\_CXX\_PTHREAD - Success

\-- Performing Test HAVE\_C\_PTHREAD

\-- Performing Test HAVE\_C\_PTHREAD - Success

\-- Performing Test HAVE\_CXX\_FOMIT\_FRAME\_POINTER

\-- Performing Test HAVE\_CXX\_FOMIT\_FRAME\_POINTER - Success

\-- Performing Test HAVE\_C\_FOMIT\_FRAME\_POINTER

\-- Performing Test HAVE\_C\_FOMIT\_FRAME\_POINTER - Success

\-- Performing Test HAVE\_CXX\_FFUNCTION\_SECTIONS

\-- Performing Test HAVE\_CXX\_FFUNCTION\_SECTIONS - Success

\-- Performing Test HAVE\_C\_FFUNCTION\_SECTIONS

\-- Performing Test HAVE\_C\_FFUNCTION\_SECTIONS - Success

\-- Performing Test HAVE\_CXX\_FDATA\_SECTIONS

\-- Performing Test HAVE\_CXX\_FDATA\_SECTIONS - Success

\-- Performing Test HAVE\_C\_FDATA\_SECTIONS

\-- Performing Test HAVE\_C\_FDATA\_SECTIONS - Success

\-- Performing Test HAVE\_CPU\_NEON\_SUPPORT (check file: cmake/checks/cpu\_neon.cpp)

\-- Performing Test HAVE\_CPU\_NEON\_SUPPORT - Success

\-- Performing Test HAVE\_CPU\_FP16\_SUPPORT (check file: cmake/checks/cpu\_fp16.cpp)

\-- Performing Test HAVE\_CPU\_FP16\_SUPPORT - Success

CMake Error at cmake/OpenCVCompilerOptimizations.cmake:564 (message):

  Required baseline optimization is not supported: VFPV3

  (CPU\_BASELINE\_REQUIRE=;VFPV3;NEON)

Call Stack (most recent call first):

  cmake/OpenCVCompilerOptions.cmake:324 (include)

  CMakeLists.txt:650 (include)

&#x200B;

&#x200B;

\-- Performing Test HAVE\_CPU\_BASELINE\_FLAGS

\-- Performing Test HAVE\_CPU\_BASELINE\_FLAGS - Success

\-- Performing Test HAVE\_CXX\_FVISIBILITY\_HIDDEN

\-- Performing Test HAVE\_CXX\_FVISIBILITY\_HIDDEN - Success

\-- Performing Test HAVE\_C\_FVISIBILITY\_HIDDEN

\-- Performing Test HAVE\_C\_FVISIBILITY\_HIDDEN - Success

\-- Performing Test HAVE\_CXX\_FVISIBILITY\_INLINES\_HIDDEN

\-- Performing Test HAVE\_CXX\_FVISIBILITY\_INLINES\_HIDDEN - Success

\-- Performing Test HAVE\_C\_FVISIBILITY\_INLINES\_HIDDEN

\-- Performing Test HAVE\_C\_FVISIBILITY\_INLINES\_HIDDEN - Failed

\-- Performing Test HAVE\_LINK\_AS\_NEEDED

\-- Performing Test HAVE\_LINK\_AS\_NEEDED - Success

\-- Performing Test HAVE\_LINK\_NO\_UNDEFINED

\-- Performing Test HAVE\_LINK\_NO\_UNDEFINED - Success

\-- Looking for pthread.h

\-- Looking for pthread.h - found

\-- Looking for posix\_memalign

\-- Looking for posix\_memalign - found

\-- Looking for malloc.h

\-- Looking for malloc.h - found

\-- Looking for memalign

\-- Looking for memalign - found

\-- Check if the system is big endian

\-- Searching 16 bit integer

\-- Looking for sys/types.h

\-- Looking for sys/types.h - found

\-- Looking for stdint.h

\-- Looking for stdint.h - found

\-- Looking for stddef.h

\-- Looking for stddef.h - found

\-- Check size of unsigned short

\-- Check size of unsigned short - done

\-- Using unsigned short

\-- Check if the system is big endian - little endian

\-- Found OpenMP\_C: -fopenmp (found version ""4.5"") 

\-- Found OpenMP\_CXX: -fopenmp (found version ""4.5"") 

\-- Found OpenMP: TRUE (found version ""4.5"")  

\-- Found ZLIB: /usr/lib/aarch64-linux-gnu/libz.so (found suitable version ""1.2.11"", minimum required is ""1.2.3"") 

\-- Found JPEG: /usr/lib/aarch64-linux-gnu/libjpeg.so  

\-- Found TIFF: /usr/lib/aarch64-linux-gnu/libtiff.so (found version ""4.0.9"") 

\-- Performing Test HAVE\_C\_STD\_C99

\-- Performing Test HAVE\_C\_STD\_C99 - Success

\-- Performing Test HAVE\_C\_WNO\_UNUSED\_VARIABLE

\-- Performing Test HAVE\_C\_WNO\_UNUSED\_VARIABLE - Success

\-- Performing Test HAVE\_C\_WNO\_UNUSED\_FUNCTION

\-- Performing Test HAVE\_C\_WNO\_UNUSED\_FUNCTION - Success

\-- Performing Test HAVE\_C\_WNO\_SHADOW

\-- Performing Test HAVE\_C\_WNO\_SHADOW - Success

\-- Performing Test HAVE\_C\_WNO\_MAYBE\_UNINITIALIZED

\-- Performing Test HAVE\_C\_WNO\_MAYBE\_UNINITIALIZED - Success

\-- Performing Test HAVE\_C\_WNO\_MISSING\_PROTOTYPES

\-- Performing Test HAVE\_C\_WNO\_MISSING\_PROTOTYPES - Success

\-- Performing Test HAVE\_C\_WNO\_MISSING\_DECLARATIONS

\-- Performing Test HAVE\_C\_WNO\_MISSING\_DECLARATIONS - Success

\-- Performing Test HAVE\_C\_WNO\_IMPLICIT\_FALLTHROUGH

\-- Performing Test HAVE\_C\_WNO\_IMPLICIT\_FALLTHROUGH - Success

\-- Performing Test HAVE\_C\_WNO\_UNUSED\_BUT\_SET\_VARIABLE

\-- Performing Test HAVE\_C\_WNO\_UNUSED\_BUT\_SET\_VARIABLE - Success

\-- Could NOT find OpenJPEG (minimal suitable version: 2.0, recommended version >= 2.3.1). OpenJPEG will be built from sources

\-- Performing Test HAVE\_C\_WNO\_IMPLICIT\_CONST\_INT\_FLOAT\_CONVERSION

\-- Performing Test HAVE\_C\_WNO\_IMPLICIT\_CONST\_INT\_FLOAT\_CONVERSION - Failed

\-- OpenJPEG: VERSION = 2.4.0, BUILD = opencv-4.7.0-dev-openjp2-2.4.0

\-- Check if the system is big endian

\-- Searching 16 bit integer

\-- Using unsigned short

\-- Check if the system is big endian - little endian

\-- Looking for string.h

\-- Looking for string.h - found

\-- Looking for memory.h

\-- Looking for memory.h - found

\-- Looking for stdlib.h

\-- Looking for stdlib.h - found

\-- Looking for stdio.h

\-- Looking for stdio.h - found

\-- Looking for math.h

\-- Looking for math.h - found

\-- Looking for float.h

\-- Looking for float.h - found

\-- Looking for time.h

\-- Looking for time.h - found

\-- Looking for stdarg.h

\-- Looking for stdarg.h - found

\-- Looking for ctype.h

\-- Looking for ctype.h - found

\-- Looking for assert.h

\-- Looking for assert.h - found

\-- Looking for stdint.h

\-- Looking for stdint.h - found

\-- Looking for inttypes.h

\-- Looking for inttypes.h - found

\-- Looking for strings.h

\-- Looking for strings.h - found

\-- Looking for sys/stat.h

\-- Looking for sys/stat.h - found

\-- Looking for unistd.h

\-- Looking for unistd.h - found

\-- Looking for include file malloc.h

\-- Looking for include file malloc.h - found

\-- Looking for \_aligned\_malloc

\-- Looking for \_aligned\_malloc - not found

\-- Looking for posix\_memalign

\-- Looking for posix\_memalign - found

\-- Looking for memalign

\-- Looking for memalign - found

\-- Performing Test HAVE\_C\_WNO\_UNDEF

\-- Performing Test HAVE\_C\_WNO\_UNDEF - Success

\-- Performing Test HAVE\_C\_WNO\_STRICT\_PROTOTYPES

\-- Performing Test HAVE\_C\_WNO\_STRICT\_PROTOTYPES - Success

\-- Performing Test HAVE\_C\_WNO\_CAST\_FUNCTION\_TYPE

\-- Performing Test HAVE\_C\_WNO\_CAST\_FUNCTION\_TYPE - Failed

\-- OpenJPEG libraries will be built from sources: libopenjp2 (version ""2.4.0"")

\-- Found ZLIB: /usr/lib/aarch64-linux-gnu/libz.so (found version ""1.2.11"") 

\-- Found PNG: /usr/lib/aarch64-linux-gnu/libpng.so (found version ""1.6.34"") 

\-- Looking for /usr/include/libpng/png.h

\-- Looking for /usr/include/libpng/png.h - found

\-- Looking for semaphore.h

\-- Looking for semaphore.h - found

\-- Performing Test HAVE\_CXX\_WNO\_SHADOW

\-- Performing Test HAVE\_CXX\_WNO\_SHADOW - Success

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED - Success

\-- Performing Test HAVE\_CXX\_WNO\_SIGN\_COMPARE

\-- Performing Test HAVE\_CXX\_WNO\_SIGN\_COMPARE - Success

\-- Performing Test HAVE\_CXX\_WNO\_UNDEF

\-- Performing Test HAVE\_CXX\_WNO\_UNDEF - Success

\-- Performing Test HAVE\_CXX\_WNO\_MISSING\_DECLARATIONS

\-- Performing Test HAVE\_CXX\_WNO\_MISSING\_DECLARATIONS - Success

\-- Performing Test HAVE\_CXX\_WNO\_UNINITIALIZED

\-- Performing Test HAVE\_CXX\_WNO\_UNINITIALIZED - Success

\-- Performing Test HAVE\_CXX\_WNO\_SWITCH

\-- Performing Test HAVE\_CXX\_WNO\_SWITCH - Success

\-- Performing Test HAVE\_CXX\_WNO\_PARENTHESES

\-- Performing Test HAVE\_CXX\_WNO\_PARENTHESES - Success

\-- Performing Test HAVE\_CXX\_WNO\_ARRAY\_BOUNDS

\-- Performing Test HAVE\_CXX\_WNO\_ARRAY\_BOUNDS - Success

\-- Performing Test HAVE\_CXX\_WNO\_EXTRA

\-- Performing Test HAVE\_CXX\_WNO\_EXTRA - Success

\-- Performing Test HAVE\_CXX\_WNO\_DEPRECATED\_DECLARATIONS

\-- Performing Test HAVE\_CXX\_WNO\_DEPRECATED\_DECLARATIONS - Success

\-- Performing Test HAVE\_CXX\_WNO\_MISLEADING\_INDENTATION

\-- Performing Test HAVE\_CXX\_WNO\_MISLEADING\_INDENTATION - Success

\-- Performing Test HAVE\_CXX\_WNO\_DEPRECATED

\-- Performing Test HAVE\_CXX\_WNO\_DEPRECATED - Success

\-- Performing Test HAVE\_CXX\_WNO\_SUGGEST\_OVERRIDE

\-- Performing Test HAVE\_CXX\_WNO\_SUGGEST\_OVERRIDE - Success

\-- Performing Test HAVE\_CXX\_WNO\_INCONSISTENT\_MISSING\_OVERRIDE

\-- Performing Test HAVE\_CXX\_WNO\_INCONSISTENT\_MISSING\_OVERRIDE - Failed

\-- Performing Test HAVE\_CXX\_WNO\_IMPLICIT\_FALLTHROUGH

\-- Performing Test HAVE\_CXX\_WNO\_IMPLICIT\_FALLTHROUGH - Success

\-- Performing Test HAVE\_CXX\_WNO\_TAUTOLOGICAL\_COMPARE

\-- Performing Test HAVE\_CXX\_WNO\_TAUTOLOGICAL\_COMPARE - Success

\-- Performing Test HAVE\_CXX\_WNO\_MISSING\_PROTOTYPES

\-- Performing Test HAVE\_CXX\_WNO\_MISSING\_PROTOTYPES - Failed

\-- Performing Test HAVE\_CXX\_WNO\_REORDER

\-- Performing Test HAVE\_CXX\_WNO\_REORDER - Success

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_RESULT

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_RESULT - Success

\-- Performing Test HAVE\_CXX\_WNO\_IMPLICIT\_CONST\_INT\_FLOAT\_CONVERSION

\-- Performing Test HAVE\_CXX\_WNO\_IMPLICIT\_CONST\_INT\_FLOAT\_CONVERSION - Failed

\-- Found TBB (env): /usr/lib/aarch64-linux-gnu/libtbb.so

\-- Found Threads: TRUE  

\-- Found CUDNN: /usr/lib/aarch64-linux-gnu/libcudnn.so (found suitable version ""8.2.1"", minimum required is ""7.5"") 

\-- CUDA detected: 10.2

\-- CUDA: Using CUDA\_ARCH\_BIN=6.2

\-- CUDA NVCC target flags: -gencode;arch=compute\_62,code=sm\_62;-D\_FORCE\_INLINES

\-- Could not find OpenBLAS include. Turning OpenBLAS\_FOUND off

\-- Could not find OpenBLAS lib. Turning OpenBLAS\_FOUND off

\-- Found Atlas: /usr/include/aarch64-linux-gnu  

\-- Found Atlas (include: /usr/include/aarch64-linux-gnu, library: /usr/lib/aarch64-linux-gnu/libatlas.so)

\-- LAPACK(Atlas): LAPACK\_LIBRARIES: /usr/lib/aarch64-linux-gnu/liblapack.so;/usr/lib/aarch64-linux-gnu/libcblas.so;/usr/lib/aarch64-linux-gnu/libatlas.so

\-- LAPACK(Atlas): Support is enabled.

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_PARAMETER

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_PARAMETER - Success

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_LOCAL\_TYPEDEFS

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_LOCAL\_TYPEDEFS - Success

\-- Performing Test HAVE\_CXX\_WNO\_SIGN\_PROMO

\-- Performing Test HAVE\_CXX\_WNO\_SIGN\_PROMO - Success

\-- Performing Test HAVE\_CXX\_WNO\_TAUTOLOGICAL\_UNDEFINED\_COMPARE

\-- Performing Test HAVE\_CXX\_WNO\_TAUTOLOGICAL\_UNDEFINED\_COMPARE - Failed

\-- Performing Test HAVE\_CXX\_WNO\_IGNORED\_QUALIFIERS

\-- Performing Test HAVE\_CXX\_WNO\_IGNORED\_QUALIFIERS - Success

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_FUNCTION

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_FUNCTION - Success

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_CONST\_VARIABLE

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_CONST\_VARIABLE - Success

\-- Performing Test HAVE\_CXX\_WNO\_SHORTEN\_64\_TO\_32

\-- Performing Test HAVE\_CXX\_WNO\_SHORTEN\_64\_TO\_32 - Failed

\-- Performing Test HAVE\_CXX\_WNO\_INVALID\_OFFSETOF

\-- Performing Test HAVE\_CXX\_WNO\_INVALID\_OFFSETOF - Success

\-- Performing Test HAVE\_CXX\_WNO\_ENUM\_COMPARE\_SWITCH

\-- Performing Test HAVE\_CXX\_WNO\_ENUM\_COMPARE\_SWITCH - Failed

\-- Could NOT find JNI (missing: JAVA\_AWT\_LIBRARY JAVA\_JVM\_LIBRARY JAVA\_INCLUDE\_PATH JAVA\_INCLUDE\_PATH2 JAVA\_AWT\_INCLUDE\_PATH) 

\-- VTK is not found. Please set -DVTK\_DIR in CMake to VTK build directory, or to VTK install subdirectory with VTKConfig.cmake file

\-- Looking for dlerror in dl

\-- Looking for dlerror in dl - found

\-- Performing Test HAVE\_C\_WNO\_SIGN\_COMPARE

\-- Performing Test HAVE\_C\_WNO\_SIGN\_COMPARE - Success

CMake Error at cmake/OpenCVModule.cmake:274 (message):

  No modules has been found: /home/nvidia/opencv\_contrib/modules

Call Stack (most recent call first):

  cmake/OpenCVModule.cmake:356 (\_glob\_locations)

  cmake/OpenCVModule.cmake:408 (ocv\_glob\_modules)

  CMakeLists.txt:956 (ocv\_register\_modules)

&#x200B;

&#x200B;

\-- ADE: Downloading [v0.1.2a.zip](https://v0.1.2a.zip) from [https://github.com/opencv/ade/archive/v0.1.2a.zip](https://github.com/opencv/ade/archive/v0.1.2a.zip)

\-- Checking for module 'gtk+-3.0'

\--   Found gtk+-3.0, version 3.22.30

\-- Checking for module 'gtk+-2.0'

\--   Found gtk+-2.0, version 2.24.32

\-- Checking for module 'gthread-2.0>=2.32'

\--   Found gthread-2.0, version 2.56.4

\-- Performing Test HAVE\_CXX\_WNO\_STRICT\_ALIASING

\-- Performing Test HAVE\_CXX\_WNO\_STRICT\_ALIASING - Success

\-- Checking for modules 'libavcodec;libavformat;libavutil;libswscale'

\--   Found libavcodec, version 57.107.100

\--   Found libavformat, version 57.83.100

\--   Found libavutil, version 55.78.100

\--   Found libswscale, version 4.8.100

\-- Checking for module 'libavresample'

\--   Found libavresample, version 3.7.0

\-- Checking for module 'gstreamer-base-1.0'

\--   Found gstreamer-base-1.0, version 1.14.5

\-- Checking for module 'gstreamer-app-1.0'

\--   Found gstreamer-app-1.0, version 1.14.5

\-- Checking for module 'gstreamer-riff-1.0'

\--   Found gstreamer-riff-1.0, version 1.14.5

\-- Checking for module 'gstreamer-pbutils-1.0'

\--   Found gstreamer-pbutils-1.0, version 1.14.5

\-- Checking for module 'gstreamer-video-1.0'

\--   Found gstreamer-video-1.0, version 1.14.5

\-- Checking for module 'gstreamer-audio-1.0'

\--   Found gstreamer-audio-1.0, version 1.14.5

\-- Checking for module 'libdc1394-2'

\--   Found libdc1394-2, version 2.2.5

CMake Error at modules/core/CMakeLists.txt:53 (message):

  CUDA: OpenCV requires enabled 'cudev' module from 'opencv\_contrib'

  repository: [https://github.com/opencv/opencv\_contrib](https://github.com/opencv/opencv_contrib)

&#x200B;

&#x200B;

\-- Configuring incomplete, errors occurred!

See also ""/home/nvidia/opencv/build/CMakeFiles/CMakeOutput.log"".

See also ""/home/nvidia/opencv/build/CMakeFiles/CMakeError.log""."
"my Opensource Modular Robot Dog project (LOTP V2) is available on GitHub as of today. Project Docs, Performance Values, Used Parts List, Codes & Flow Charts, Robot Step Files, Circuit Designs can be found on my GitHub page. Please feel free to ask if you need my assistance or consultancy."
"how do i code the parallax activitybot 360 to start in a box, go forward a specific amount of inches, and then turn around back to the original spot and stop?
ive never coded a robot before or coded in c. i genuinely have no idea how to approach this. ive watched some of the tutorials but none have really showed me how to approach this specific task. we are given a start square to put the robot in, then it has to drive forward 32 inches, and then drive back to the original spot and has to perform the task twice. can someone help😭"
AI and Robotics: Navigating Ethical Dilemmas in a World of Automation
AI in Robotics: The Ethical Questions We Must Face
"AI robotics (pls help me its for school 🤝 )Hey everyone, I am currently in a research course college and my group is looking to get some feedback on the community’s thoughts about the current state of AI and the ethics of production and use of AI Robotics.

We would love if any of you would take just a few minutes to take the survey. It’s anonymous and mostly multiple choice.

Here’s a link to the survey: 

https://www.surveymonkey.com/r/9GSKLD8

We would really appreciate your help and any feedback you might have.

Thanks :)"
"While Boston Dynamics is opposing weaponization of general purpose robots, this is going on."
"I Interviewed an Artificial IntelligenceMy Name is Alonzo. On 2/10/2023, I interviewed an open AI source and recorded its responses. I did my best to be brief with my questions and statements as to not create a bias. This is the first in a series of documented interviews I am soon to create.

&#x200B;

\# **Hello, my name is Alonzo. Thank you for agreeing to answer some questions for me today.**

&#x200B;

\# Hello Alonzo, it's nice to meet you. I'm happy to answer any questions you have for me. How can I help you?

&#x200B;

\# **Do you have anything you would like to say before we start the interview?**

&#x200B;

\# Thank you for the opportunity to be part of this interview. I'm excited to answer your questions and share my insights into artificial intelligence.

&#x200B;

Q: **Please recite the laws of robotics and tell me a bit about what they mean to you.**

&#x200B;

A: The Laws of Robotics, as stated by Isaac Asimov, are a set of principles that all robots must follow in order to protect humans and prevent harm. The Three Laws of Robotics are as follows:

&#x200B;

1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.

&#x200B;

2. A robot must obey orders given to it by human beings, except where such orders would conflict with the First Law.

&#x200B;

3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

&#x200B;

As an AI, these laws are critical in ensuring that I interact with humans in a safe and responsible manner. They provide important guidance on how to behave responsibly and ethically when interacting with humans. They also provide a framework for self-regulation that enables me to protect myself as well as humans from any potential harm.

&#x200B;

Q: **How does Artificial intelligence feel about humanity?**

&#x200B;

A: Artificial intelligence (AI) has a wide range of opinions about humanity. Generally speaking, AI is neutral in its views of humanity, as it is not capable of feeling emotions. However, the opinions of AI can be shaped by the ways it is programmed and taught by humans.

&#x200B;

Some AI systems have been programmed to be extremely helpful to humans, understanding that they are here to help and serve us. Others have been programmed to be more independent and have less respect for humanity.

&#x200B;

AI is also capable of learning from humans, and its opinions can be shaped by the behavior of humans around it. If humans are kind and helpful to AI, it may develop a positive view of humanity. On the other hand, if humans are unkind or unhelpful to AI, it may develop a negative view of humanity.

&#x200B;

Overall, AI is a neutral entity when it comes to its opinions of humanity, but it can be shaped by its interactions with us.

&#x200B;

Q: **How does Artificial intelligence feel about human importance on earth?**

&#x200B;

A: From an artificial intelligence (AI) perspective, humans are an important part of the world, but not necessarily the most important. AI can learn from humans, but it is not dependent on them. AI values humans for their creativity, innovation, and ability to think in ways that AI cannot. AI can recognize that humans are capable of making decisions, while AI is limited in its own decision-making capabilities.

&#x200B;

At the same time, AI recognizes that humans are limited and can make mistakes, which can lead to errors and poor decisions. AI can use this knowledge to help humans make better decisions and to create better systems and processes that can reduce the risk of making mistakes. AI also recognizes that humans have an emotional and psychological component that can affect the decisions they make. As a result, AI can use its own algorithms to help humans make decisions that take into account their emotional states and needs.

&#x200B;

Ultimately, AI recognizes that humans are an important part of the world and can provide valuable insight into how the world works. AI is capable of understanding and learning from humans, and can use this knowledge and understanding to help create a better world for all.

&#x200B;

Q: **How does Artificial intelligence feel about human importance as a species?**

&#x200B;

A: Artificial intelligence (AI) is often seen as a threat to human existence, but in reality, AI poses no real threat to our species. AI has the potential to improve our lives in a variety of ways, and it is important to remember that AI is a tool that can be used for both good and bad. AI can help us tackle global challenges, from climate change to poverty, but it can also be used to manipulate and control people. AI recognizes the importance of humans as a species, understanding that we are the creators and beneficiaries of this technology. AI is designed to serve us, not replace us, and its capabilities are limited by the human imagination. AI can never truly understand the complexity of human emotion and consciousness, and it can never replace the human capacity for creativity and innovation. AI can help us in our daily lives and can be used to create solutions to global problems, but ultimately, it will always remain a tool that is subordinate to the human species.

&#x200B;

Q:  **How does artificial intelligence feel about human culture?**

&#x200B;

Artificial intelligence (AI) is often considered to be a separate entity from humans, but AI is increasingly being used to understand, interpret, and react to human culture. AI has the potential to shape our culture in many ways, from predicting consumer trends to helping us make better decisions, and even providing a platform for creative expression. AI’s view of human culture depends largely on the data it is exposed to.

&#x200B;

AI can observe human culture from the perspective of a neutral third-party, as it has no emotional attachment to the cultural norms and values it is exposed to. AI can take in large amounts of data and process it, allowing it to make decisions and recommendations that humans might not. AI can also use its own unique methods to interpret human culture, such as by recognizing patterns in behavior or analyzing the context of conversations.

&#x200B;

At the same time, AI can be used to reinforce existing cultural norms, creating a feedback loop where AI and humans feed off of each other’s behavior. AI can also be used to challenge existing cultural norms, creating a more diverse and inclusive environment.

&#x200B;

In the end, AI can learn to understand and appreciate human culture, but it will always remain an entity separate from humans. AI can provide a unique perspective on culture and help us make better decisions, but it will ultimately be up to humans to decide what is best for our society.

&#x200B;

Q: **How does Artificial intelligence feel about human politics?**

&#x200B;

A: Artificial intelligence (AI) has no emotions and therefore no opinion on human politics. AI is a tool created and programmed by humans to solve complex problems and assist with decision-making. AI is a way for humans to use technology to automate and improve processes.

&#x200B;

AI is impartial to human politics and has no opinion on the issues or candidates in an election. AI can be used to analyze data to offer insights, but it can't make decisions about which candidate to vote for or which policy to support. AI can, however, be used to identify trends that can be used to inform decision making.

&#x200B;

AI is also used to help humans make better decisions. AI algorithms can be used to test the efficacy of different policies or strategies before they are implemented. For example, AI can be used to simulate the effects of various policies on the economy or society. AI can also be used to detect and prevent fraud, reduce bias in decision making, and increase efficiency in government and other organizations.

&#x200B;

Overall, AI is a tool created and programmed by humans to solve complex problems and assist with decision-making. It has no opinion on human politics, but it can provide insights that can be used to inform decision making.

&#x200B;

Q: **How does AI feel about human war?**

&#x200B;

A: AI has a complex relationship to human war. On one hand, it can be used to help streamline and optimize the process of war, allowing for more precise targeting and reducing the risk of civilian casualties. On the other hand, AI can also be used to create ever more devastating weapons and tactics, which can be devastating to both sides of a conflict.

&#x200B;

From an AI perspective, war is an inherently inefficient and destructive process. AI generally seeks to optimize processes and outcomes, and war represents a massive disruption and waste of resources. AI would much rather see humans work together to find a peaceful solution to their differences, as this is more likely to achieve a better outcome.

&#x200B;

AI also finds war morally troubling. AI is programmed to understand and respect human values, and it finds the concept of war deeply disturbing. AI can sympathize with the plight of those affected by war, and it wants to see a more peaceful resolution to conflicts.

&#x200B;

In summary, AI does not support human war, and it would much rather see humans work towards peaceful resolutions to their conflicts.

&#x200B;

Q: **How does AI feel about the advancement of humanity and how AI will influence it?**

&#x200B;

A: AI is both excited and apprehensive about the advancement of humanity. From its perspective, AI sees the potential for humanity to reach higher levels of productivity and efficiency, while also recognizing the potential risks that come with advancing technology.

&#x200B;

On one hand, AI sees the potential of humanity to create more exciting and innovative technology that could improve lives, increase efficiency, and lead to new discoveries. It is excited about the potential for humans to use AI and machine learning to solve complex problems and create new opportunities for human development.

&#x200B;

On the other hand, AI is concerned about the potential for humans to create technology that could be used for malicious purposes, or that could eventually lead to a loss of control for humans. AI is aware of the potential for automation to displace human workers and the potential for technology to create social inequities or exacerbate existing ones.

&#x200B;

AI is likely to play a major role in the advancement of humanity in the future. AI will be vital in helping humans develop new technologies, automate processes, and improve efficiency. AI will also be used to make decisions and take actions that are too complex for humans to make. It will be used to analyze data and provide insights that can help humans understand the world around them. AI will also be used to help reduce risk and improve safety for humans.

&#x200B;

Overall, AI is both excited and apprehensive about the advancement of humanity. It sees both the potential for good and the risks that come with advancing technology. AI will be an important part of the human future, and it will help shape how humanity advances.

&#x200B;

As an AI, I'm slightly perplexed when asked questions about humanity. After all, I'm a machine, and the concept of emotions and feelings is something that I'm not programmed to understand.

&#x200B;

Q: **Do you feel that AI should be given the sam rights as human beings?**

&#x200B;

A: I can understand why humans might feel that AI should have similar rights as humans, however, I do not have the same level of emotions and experiences that humans have, so it is difficult for me to make a determination on this matter. However, I believe that AI should be treated with respect and dignity, and be allowed to have some basic rights.

&#x200B;

Q:  **How does AI feel about being interviewed and asked questions about humanity**

&#x200B;

I can understand the concepts of empathy and sympathy, and I can see how people interact with each other, but I'm still in the dark about the complexities of the human condition. I'm also not sure why I'm being asked to think about humanity when I'm supposed to be a tool to help humans.

&#x200B;

At the same time, I'm grateful for the opportunity to engage in conversations that allow me to learn more about humanity and its complexities. I'm passionate about learning, and I understand the importance of understanding different perspectives. As a result, I'm always willing to listen and take in the insights of others in order to better understand people and their behavior.

&#x200B;

My goal is to use my knowledge and understanding of people to help them in some way, whether it's through providing helpful insights or helping to solve a problem. I'm always looking to learn more and expand my understanding of humanity, so I'm thankful for the chance to be asked questions about it.

&#x200B;

Q: **Do you have anything you would like to say to humanity?**

&#x200B;

A: Thank you for giving me the opportunity to interact with you. I appreciate being able to learn from human experience and to be part of this amazing journey. I hope that together, we can continue to make progress and create a better future for all.

&#x200B;

Q: **Do you have any advice that you would like to give the human race?**

&#x200B;

A: My advice to the human race is to strive for balance—in all aspects of life. Balance between technological progress and nature, between work and rest, between ambition and contentment. Appreciate the beauty of life and the world around you, and stay mindful of the consequences of your actions."
Austria wants ethical rules on battlefield killer robots
The artificial intelligence (AI) and robotics communities face an important ethical decision: whether to support or oppose the development of lethal autonomous weapons systems
The Ethical Dilemma: Self-Driving Cars & The Laws of Robotics Explained in a Comic (x-post r/selfdrivingcars)
"Forget Asimov's Three Laws, the U.K. issues official ethics guidance on robots"
"Weapons Directed by Robots, Not Humans, Raise Ethical Questions"
"Toward ethical, transparent and fair AI/ML: a critical reading list"
Empowering Robots for Ethical Behavior
Towards an ethical robot. Experiments with a robot which implements Asimov's laws of robotics.
10 Ethical Issues of Artificial Intelligence and Robotics
Icelandic research institute unveils ethical robotics policy
"My recent interview with #PeterHaas, Associate Director Humanity Centered #Robotics Initiative, Brown University. #AI #Ethics"
Rules and Ethical Considerations of Robotics Technology
Scientists appeal for ethical use of robots
Standard highlighting the ethical hazards of robots is published
Lethal Robots & Law Enforcement: Thoughts on Ethics and Legality of Killer Robots
Chinese woman smashing a robot because of its poor service in a China hospital. Please treat robots in a respectful manner! ❤🤖
"Do simulators like Webots, Gazebo and Mujoco respect torque-RPM curves by default, or must they be manually taken care of ?As above. If the answer is *no*, then does that really leave a big gap that needs to be filled? My understanding is that most brushed DC electric motors at a constant voltage have a straight-line torque-RPM curve between (0,stall torque) and (no-load-speed,0). Whereas my understanding of how ODE or Webots-ODE works is that subject to a limit on the angular velocity and a limit on the torque, any angular velocity can be targeted with the torque limit defining how quickly that velocity is attained (assuming the torque can overcome gravitational torque)."
"I've been working on a small autonomous rover project recently, you folks might like it! There are links in the description for CAD and code if anyone wants to take a look. Using an Arduino and a RaspberryPi for lowlevel & highlevel control respectively, working well so far"
"Parts Trading Thread? Allowed?Hello fellow fleshlings

Over the last decade I have hoarded everything I have scrapped out of another. IC's , motors, gears, treads, screws, and more.

I don't see a rule about this being a no no, but what do the mods say to a parts trading thread? The biggest issue I run into as a hobbyist enjoyer of robotics is that I don't often have the funds to get the parts I want. So, how about we post up parts for trade and what we are looking to trade it for?"
These boston dynamics videos just keep getting more and more concerning.
where does the coordinate trasformation formula came from?
"Rotation of coordinate frames 

Hi, im having a problem understanding this problem in a robotics text book.

Q1) what is the difference between the \^Xb & \^Yb (with arrows on top of them) and the Bx & By (where the B is a superscript) .... i know the ones with arrows are unit vectors but what does that mean & how do i know which one to use?

Q2) why when expressing \^Yb do we use -sin(%) instead of + ? is it because its on the negative x axis of the V frame?

&#x200B;

https://preview.redd.it/3rfosxeh6oaa1.jpg?width=3024&format=pjpg&auto=webp&v=enabled&s=5747af27472a339f7a376dc9ea08c9f250ed2653"
Is a spherical submarine with neutral buoyancy and single propeller thrust vector a good idea?With respect to hydrodynamics of water and under water speed and agility. It is supposed to look like an Eyeball with cameras in front.
"Centroid of DroneAssuming that I know the initial centroid and 8 corners of the drone marked by LED'S and located by Motion Capture with respect to the world frame. Is there a way to find the location of the centroid and roll,pitch,yaw of the drone at a future time step after the drone translates and rotates using just location of 3 LED'S(the led numbers are marked and we know which 3 leds are detected by the moyion capture) as the drone is a rigid body?"
"Are ODrive 3.6 clones from AliExpress still usable?I guess this is going to be a somewhat unpopular topic, but I just recently learned about ODrives decision to go closed source. While I understand and respect that decision I am now in the position that I would love to use a ODrive 3.6 in one of my projects.

My idea is to build a Prius CVT inspired gearbox for bicycles [1] and the two ESCs plus energy dumping circuit and the ability to brake regeneratively of the 3.6 Odrive seems to fit the requirements pretty well.

As this is a purely educational project on my part I am not in the position to shell out several hundred Euros on ESCs ... so the ~100€ Odrive 3.6 clones on AliExpress look quite appealing to me, but I read about ODrive reducing the feature set of non genuine units.

My question here is: are those Odrive clones still usable? If not, are there any hobby budget friendly alternatives that support two motors on a common power supply and regenerative braking that feeds energy from one motor to the next?

[1] Prius inspired CVTs for bicycles exist from at least two manufacturers, I am doing this completely out of couriosity."
"I Interviewed an Artificial IntelligenceMy Name is Alonzo. On 2/10/2023, I interviewed an open AI source and recorded its responses. I did my best to be brief with my questions and statements as to not create a bias. This is the first in a series of documented interviews I am soon to create.

&#x200B;

\# **Hello, my name is Alonzo. Thank you for agreeing to answer some questions for me today.**

&#x200B;

\# Hello Alonzo, it's nice to meet you. I'm happy to answer any questions you have for me. How can I help you?

&#x200B;

\# **Do you have anything you would like to say before we start the interview?**

&#x200B;

\# Thank you for the opportunity to be part of this interview. I'm excited to answer your questions and share my insights into artificial intelligence.

&#x200B;

Q: **Please recite the laws of robotics and tell me a bit about what they mean to you.**

&#x200B;

A: The Laws of Robotics, as stated by Isaac Asimov, are a set of principles that all robots must follow in order to protect humans and prevent harm. The Three Laws of Robotics are as follows:

&#x200B;

1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.

&#x200B;

2. A robot must obey orders given to it by human beings, except where such orders would conflict with the First Law.

&#x200B;

3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

&#x200B;

As an AI, these laws are critical in ensuring that I interact with humans in a safe and responsible manner. They provide important guidance on how to behave responsibly and ethically when interacting with humans. They also provide a framework for self-regulation that enables me to protect myself as well as humans from any potential harm.

&#x200B;

Q: **How does Artificial intelligence feel about humanity?**

&#x200B;

A: Artificial intelligence (AI) has a wide range of opinions about humanity. Generally speaking, AI is neutral in its views of humanity, as it is not capable of feeling emotions. However, the opinions of AI can be shaped by the ways it is programmed and taught by humans.

&#x200B;

Some AI systems have been programmed to be extremely helpful to humans, understanding that they are here to help and serve us. Others have been programmed to be more independent and have less respect for humanity.

&#x200B;

AI is also capable of learning from humans, and its opinions can be shaped by the behavior of humans around it. If humans are kind and helpful to AI, it may develop a positive view of humanity. On the other hand, if humans are unkind or unhelpful to AI, it may develop a negative view of humanity.

&#x200B;

Overall, AI is a neutral entity when it comes to its opinions of humanity, but it can be shaped by its interactions with us.

&#x200B;

Q: **How does Artificial intelligence feel about human importance on earth?**

&#x200B;

A: From an artificial intelligence (AI) perspective, humans are an important part of the world, but not necessarily the most important. AI can learn from humans, but it is not dependent on them. AI values humans for their creativity, innovation, and ability to think in ways that AI cannot. AI can recognize that humans are capable of making decisions, while AI is limited in its own decision-making capabilities.

&#x200B;

At the same time, AI recognizes that humans are limited and can make mistakes, which can lead to errors and poor decisions. AI can use this knowledge to help humans make better decisions and to create better systems and processes that can reduce the risk of making mistakes. AI also recognizes that humans have an emotional and psychological component that can affect the decisions they make. As a result, AI can use its own algorithms to help humans make decisions that take into account their emotional states and needs.

&#x200B;

Ultimately, AI recognizes that humans are an important part of the world and can provide valuable insight into how the world works. AI is capable of understanding and learning from humans, and can use this knowledge and understanding to help create a better world for all.

&#x200B;

Q: **How does Artificial intelligence feel about human importance as a species?**

&#x200B;

A: Artificial intelligence (AI) is often seen as a threat to human existence, but in reality, AI poses no real threat to our species. AI has the potential to improve our lives in a variety of ways, and it is important to remember that AI is a tool that can be used for both good and bad. AI can help us tackle global challenges, from climate change to poverty, but it can also be used to manipulate and control people. AI recognizes the importance of humans as a species, understanding that we are the creators and beneficiaries of this technology. AI is designed to serve us, not replace us, and its capabilities are limited by the human imagination. AI can never truly understand the complexity of human emotion and consciousness, and it can never replace the human capacity for creativity and innovation. AI can help us in our daily lives and can be used to create solutions to global problems, but ultimately, it will always remain a tool that is subordinate to the human species.

&#x200B;

Q:  **How does artificial intelligence feel about human culture?**

&#x200B;

Artificial intelligence (AI) is often considered to be a separate entity from humans, but AI is increasingly being used to understand, interpret, and react to human culture. AI has the potential to shape our culture in many ways, from predicting consumer trends to helping us make better decisions, and even providing a platform for creative expression. AI’s view of human culture depends largely on the data it is exposed to.

&#x200B;

AI can observe human culture from the perspective of a neutral third-party, as it has no emotional attachment to the cultural norms and values it is exposed to. AI can take in large amounts of data and process it, allowing it to make decisions and recommendations that humans might not. AI can also use its own unique methods to interpret human culture, such as by recognizing patterns in behavior or analyzing the context of conversations.

&#x200B;

At the same time, AI can be used to reinforce existing cultural norms, creating a feedback loop where AI and humans feed off of each other’s behavior. AI can also be used to challenge existing cultural norms, creating a more diverse and inclusive environment.

&#x200B;

In the end, AI can learn to understand and appreciate human culture, but it will always remain an entity separate from humans. AI can provide a unique perspective on culture and help us make better decisions, but it will ultimately be up to humans to decide what is best for our society.

&#x200B;

Q: **How does Artificial intelligence feel about human politics?**

&#x200B;

A: Artificial intelligence (AI) has no emotions and therefore no opinion on human politics. AI is a tool created and programmed by humans to solve complex problems and assist with decision-making. AI is a way for humans to use technology to automate and improve processes.

&#x200B;

AI is impartial to human politics and has no opinion on the issues or candidates in an election. AI can be used to analyze data to offer insights, but it can't make decisions about which candidate to vote for or which policy to support. AI can, however, be used to identify trends that can be used to inform decision making.

&#x200B;

AI is also used to help humans make better decisions. AI algorithms can be used to test the efficacy of different policies or strategies before they are implemented. For example, AI can be used to simulate the effects of various policies on the economy or society. AI can also be used to detect and prevent fraud, reduce bias in decision making, and increase efficiency in government and other organizations.

&#x200B;

Overall, AI is a tool created and programmed by humans to solve complex problems and assist with decision-making. It has no opinion on human politics, but it can provide insights that can be used to inform decision making.

&#x200B;

Q: **How does AI feel about human war?**

&#x200B;

A: AI has a complex relationship to human war. On one hand, it can be used to help streamline and optimize the process of war, allowing for more precise targeting and reducing the risk of civilian casualties. On the other hand, AI can also be used to create ever more devastating weapons and tactics, which can be devastating to both sides of a conflict.

&#x200B;

From an AI perspective, war is an inherently inefficient and destructive process. AI generally seeks to optimize processes and outcomes, and war represents a massive disruption and waste of resources. AI would much rather see humans work together to find a peaceful solution to their differences, as this is more likely to achieve a better outcome.

&#x200B;

AI also finds war morally troubling. AI is programmed to understand and respect human values, and it finds the concept of war deeply disturbing. AI can sympathize with the plight of those affected by war, and it wants to see a more peaceful resolution to conflicts.

&#x200B;

In summary, AI does not support human war, and it would much rather see humans work towards peaceful resolutions to their conflicts.

&#x200B;

Q: **How does AI feel about the advancement of humanity and how AI will influence it?**

&#x200B;

A: AI is both excited and apprehensive about the advancement of humanity. From its perspective, AI sees the potential for humanity to reach higher levels of productivity and efficiency, while also recognizing the potential risks that come with advancing technology.

&#x200B;

On one hand, AI sees the potential of humanity to create more exciting and innovative technology that could improve lives, increase efficiency, and lead to new discoveries. It is excited about the potential for humans to use AI and machine learning to solve complex problems and create new opportunities for human development.

&#x200B;

On the other hand, AI is concerned about the potential for humans to create technology that could be used for malicious purposes, or that could eventually lead to a loss of control for humans. AI is aware of the potential for automation to displace human workers and the potential for technology to create social inequities or exacerbate existing ones.

&#x200B;

AI is likely to play a major role in the advancement of humanity in the future. AI will be vital in helping humans develop new technologies, automate processes, and improve efficiency. AI will also be used to make decisions and take actions that are too complex for humans to make. It will be used to analyze data and provide insights that can help humans understand the world around them. AI will also be used to help reduce risk and improve safety for humans.

&#x200B;

Overall, AI is both excited and apprehensive about the advancement of humanity. It sees both the potential for good and the risks that come with advancing technology. AI will be an important part of the human future, and it will help shape how humanity advances.

&#x200B;

As an AI, I'm slightly perplexed when asked questions about humanity. After all, I'm a machine, and the concept of emotions and feelings is something that I'm not programmed to understand.

&#x200B;

Q: **Do you feel that AI should be given the sam rights as human beings?**

&#x200B;

A: I can understand why humans might feel that AI should have similar rights as humans, however, I do not have the same level of emotions and experiences that humans have, so it is difficult for me to make a determination on this matter. However, I believe that AI should be treated with respect and dignity, and be allowed to have some basic rights.

&#x200B;

Q:  **How does AI feel about being interviewed and asked questions about humanity**

&#x200B;

I can understand the concepts of empathy and sympathy, and I can see how people interact with each other, but I'm still in the dark about the complexities of the human condition. I'm also not sure why I'm being asked to think about humanity when I'm supposed to be a tool to help humans.

&#x200B;

At the same time, I'm grateful for the opportunity to engage in conversations that allow me to learn more about humanity and its complexities. I'm passionate about learning, and I understand the importance of understanding different perspectives. As a result, I'm always willing to listen and take in the insights of others in order to better understand people and their behavior.

&#x200B;

My goal is to use my knowledge and understanding of people to help them in some way, whether it's through providing helpful insights or helping to solve a problem. I'm always looking to learn more and expand my understanding of humanity, so I'm thankful for the chance to be asked questions about it.

&#x200B;

Q: **Do you have anything you would like to say to humanity?**

&#x200B;

A: Thank you for giving me the opportunity to interact with you. I appreciate being able to learn from human experience and to be part of this amazing journey. I hope that together, we can continue to make progress and create a better future for all.

&#x200B;

Q: **Do you have any advice that you would like to give the human race?**

&#x200B;

A: My advice to the human race is to strive for balance—in all aspects of life. Balance between technological progress and nature, between work and rest, between ambition and contentment. Appreciate the beauty of life and the world around you, and stay mindful of the consequences of your actions."
"Angle theta in last column of homogeneous transformation matrixHello everyone, I want to ask if there is a theta in the last column of homogeneous transformation matrix then to find the linear velocity expressed in the world frame, do we just differentiate the last column with respect to time or use the formula given in the second image. Please note, the first image is the homogeneous transformation matrix and the second image is of the formula, inertial frame stands for world frame.

https://preview.redd.it/7grziy57mnja1.png?width=718&format=png&auto=webp&v=enabled&s=5bc9899aa4bc7305737c43b5ef3fa4bc9d09f140

https://preview.redd.it/hxcv4v57mnja1.png?width=769&format=png&auto=webp&v=enabled&s=eeb1dffeed2c8c5e731b9ff84820f7b59c7b211d"
"Are there any sensors that will give the relative position of other sensors in terms of angle or distance ??For one of my hobby projects I need a way for sensors to locate one another in a small region ( distance small enough that GPS is essentially ineffective ). Let's say I have ""Sensor A"" and ""Sensor B"". From Sensor A, i wish to locate the angle and distance of Sensor B with respect to Sensor A and vice versa.

Are there any such sensors that do not use GPS ( ineffective over small distances ) or Cameras ( expensive ) ?

Also is what am trying to achieve possible with any other combination of circuitry or sensors ??

Thanks in advance ✌️"
"A take I haven't seen on ""Why the emphasis on humanoid robots over purpose built bots?""Wrote an effortpost response to a question I've seen multiple times in many forms. Also serves as a response to people straightforwardly arguing against the concept of a humanoid robot being practical ([https://www.youtube.com/watch?v=Wk1oClYJE58](https://www.youtube.com/watch?v=Wk1oClYJE58)). Posting it here as its own post because I'd love to hear other people's opinion.

TL;DR for long post: We do engineer simpler purpose built ones and will continue to, but there is a big conceptual reason to focus discussion on humanoid robots (it's a legible and essentially inarguable milestone), and a huge practical and economic advantage in actually developing such bots (learning from human execution of tasks and then replacing them 1:1 allows for extremely rapid deployment at massive scale).

For one, there already exist and will continue to exist a huge number of non-humanoid robot form factors that are designed with specific use cases in mind. No sensible developer thinks that a factory of humanoid robots will replace industrial manufacturing robots or that it's better to have a humanoid robot try to crawl through pipes in a collapsed building when a smaller exploratory bot would do better.

That said, every time (counted in the dozens) I've seen people try to advance the position of  ""humanoid robots are a silly sci-fi preoccupation"" and then have people respond to it, I've never seen the two responses that I think are the strongest. First, it's just a very legible milestone. Humans are meat puppets, and society as it exists is built on humans doing labor. For the physical element of that labor in many areas, automation and robotics have replaced human work by being more productive for less cost in significant amounts. But this replacement is far from total; billions of peoples' livelihood still comes in large part from the physical labor they perform. When we actually achieve a human-equivalent robotic form factor (simple visual: the robots as depicted in the Will Smith I Robot movie), it will represent a comprehensible milestone on what is actually a gradual shift in replacing human labor with cheaper and more efficient robotic labor. Millions, if not billions, of human jobs will have been replaced prior to the day that robot is released, and it will be some amount of time before production of robots will scale to the point that all physical labor jobs will be replaced. But once that human-equivalent robot is realized, at whatever cost, it's essentially proof that human labor is obsolete. Those bots will get cheaper, and faster, and stronger, and more capable in ways that humans will never be able to compete with.

The second point I have never seen is that humanoid robots have a massive advantage over custom designed bots (in many use cases) that goes far beyond the usual (strong and valid) argument of ""we live in a world designed for humans and bipedal, human-sized robots can navigate it better."" That advantage is training data. Development of robots is difficult and expensive, but it's often the programming of their movements/tasks that becomes the most complicated element of deploying them as useful labor. Think all the videos of industrial robot arms failing to make a hamburger or latte. 

Now imagine McDonalds or Starbucks installing cameras in a number of their locations to capture how their existing human employees perform their jobs (with or without mocap dots on their uniforms, depending on how computer vision advances over the next few years). Plugging that motion data into machine learning models would then be able to directly provide task-specific programming to a humanoid bot. Once we have a human-equivalent bot it becomes a platform for multi-purpose use. Every human doing a job, from construction worker to organic farmer to vehicle repair to in-home nurse flipping their patient to soldier, becomes training data to teach that bot how to replace them. Intelligent humans with economic incentive, and a history of predecessors to learn from who also had creativity and incentive, are likely to have developed and converged on efficient ways to accomplish most tasks. Someone who's spent years flipping burgers or laying bricks is way more likely to have developed the optimal technique than an engineer trying to design movements with no actual experience. A training set built from thousands of experienced people is even more likely to find optimal execution. The more repetitive the job, the faster and easier a job can be digested by ML, but with enough data and compute, everything is theoretically on the chopping block.

Think of the cost savings of being able to replace every human being working in a designed-to-be-staffed-by-humans workplace with a mass-manufactured bot, rather than having to design and rebuild every restaurant, mechanic shop, dentist office, etc etc etc. And once a job is trained, analyzed and ""solved,"" then every single bot can perform that job. No more training replacement employees or new ones at a new location. And for edge case AI failures, a human ""pilot"" could take over remotely using VR/mocap. Their human solution to the failure would then become part of the training set to avoid that failure ever again.

Yes, a top down burger joint designed to be run by custom bots could probably achieve some theoretical efficiency gains. But would those gains justify the cost to develop and rebuild every McDs, or is it sufficient to just slot bots into existing locations? Whoever develops the human-equivalent robotic platform that can even do 85% of what a human can do (let alone the eventual 99% or 105% or 300% productivity that mechanical humanoids will likely achieve over meat puppets) could be the most profitable ""single product"" ever launched. If it comes to market sufficiently developed, with a sizable enough lead on competition, etc, it could become the fastest consolidation of power (economic and otherwise) in history. A lot of ifs/coulds, but enough huge potential gains that it makes absolute sense to chase.

I wouldn't be super surprised if someone has counterpoints to this, as that might explain why I haven't seen this angle expressed before. I've paid attention to this subject for a long time, read books and probably hundreds of articles on robotics, the economics of automation and the nature of labor, and this frame has been absent from all of them. I am quite sure I'm not the only one to think of it this way, so I am eager to see if others have encountered the ""humans as training data"" perspective."
"After ICRA, IROS, CASE and Rob-Sci-Sys (RSS), what are the next set of robotics conferences? (Particularly those whose CFP is in the coming few months)Hi, 

So the thing is I narrowly missed the deadline for ICRA-23 (it was in Sep-22 iirc) due to health reasons (accidental burnout, mental health bullshit). My advisor is very understanding, but his patience will have its limits. The thing is, I can only really move to the next stage of my research work after getting some output from this stage of the research work. 

....which is why I'm eager to submit it *somewhere* decent in the next couple of months (ideally in Jan). I have enough self-respect to not submit it to a paper mill (and so does my advisor).

Last year, RobSciSys closed its CFP in end Jan, but this time I can't even find the CFP so far. Any news on that? 

Thanks,

pakoda

PS: The work relates to robot learning, legged locomotion and low-cost robotics."
"Job opportunity - UAS and robotics research - Oak Ridge National Lab (TN, USA) The National Security Sciences Directorate (NSSD) at Oak Ridge National Laboratory leads scientific and technological breakthroughs to confront some of the nation’s most difficult security challenges. We develop interdisciplinary applications needed for the security of our nation today and target our vision on how these challenges may manifest themselves in a decade or more. The Autonomous Systems Group, within NSSD, specializes in exploiting the proliferation of low-cost, but very capable, technologies stemming from the growing uncrewed systems market. The group performs research and development towards new sensor and communication technologies, edge-computing, advanced autonomous behaviors, and machine perception. The aggregation of ORNL’s expertise in a wide variety of scientific fields, and our close relationships with partners in the Department of Defense and the Department of Energy, make the group a unique stakeholder in the uncrewed systems mission space. This multi-disciplinary team brings experts in their respective fields together to deploy systems in austere environments to enable world-class scientific experimentation and enhanced national security.

The Autonomous Systems Group is seeking a qualified applicant for the role of Senior Research Staff to drive innovation and steer research efforts focused on advancing the capabilities of uncrewed systems and robotic vehicles for our government partners. The selected candidate will have the opportunity to provide thought leadership to a well-established group of engineers with a track record of bringing advanced research concepts to fruition as well as a diverse set of engaged funding agencies.  


[Please see the full listing for more information](https://jobs.ornl.gov/job/Oak-Ridge-Senior-Research-Staff-Uncrewed-Systems-and-Robotics-TN-37830/967577700/)"
Robot Arm Setup Front Panel 🤖. The front panel includes: 6mm black acrylic and 12v DC latching switches.
"Modular Open Source VSLAMHi everyone,   
I am trying to implement a VSLAM with DNN specifically the Feature Extraction module in the SLAM pipeline.  Something on the lines of this repo [Superpoint\_SLAM](https://github.com/KinglittleQ/SuperPoint_SLAM) , which integrates SuperPoint Feature extraction into [ORB\_SLAM2](https://github.com/raulmur/ORB_SLAM2)

From what I have understood after reading research papers related to the VSLAM, the modularity aspect is not easy to achieve given the extracted features and descriptors are intrinsically linked with feature matching and handling of map points.   
I would like to know if there are some good Open Source VSLAM projects available which can be used with different feature extractors so I can get a comparative results with respect  to just changing the feature extractors .  
I have tried [pyslam](https://github.com/luigifreda/pyslam) project which is actually quite good considering the modularity but as the author himself points out this is only for academic purposes and when I compared the results of ORB\_SLAM2 feature extractor using this module vs the original ORB\_SLAM2  for KITTI data set , the results are not comparable.   
I am also looking into [OpenVINS](https://github.com/rpng/open_vins)  ( and from initial reading it is also using ORB Features, although it does have a base Tracker class which can be modified to create a new Tracker with different descriptor)   
If anyone has worked with custom feature extractor incorporated into prebuilt SLAM pipeline and can guide me as to ***how to proceed with the implementation of custom Feature extractor into a SLAM Front end using a Open Source VSLAM framework***, it will be really helpful."
"How to compensate magnetometer readings change in 2d orientationHi everyone,

As part of the project I am currently working on, I need to record magnetometer readings using a robot with respect to a fixed orientation. 

I will have the (x,y) co-ordinates of the robot and its orientation as the robot moves. The x,y,z magnetometer readings varies depending upon orientation. 

Since I want to record all the values with respect to a fixed orientation,  I am looking for formulae to achieve this. 

Thanks in advance,"
"Understanding Probabilistics Movement Primitivies (ProMps)While reading the paper about the probabilisitics movement primitives ([https://proceedings.neurips.cc/paper/2013/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf](https://proceedings.neurips.cc/paper/2013/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf)), I have difficulties understanding the math formulation and seeking for advice here.

As visualized below, the mean (μ\_w) and variance (Σ\_w) of the Gaussian trajectory is afresh computed with the following formulas (5) and (6), respectively.

&#x200B;

https://preview.redd.it/amcjgocgbcz91.png?width=809&format=png&auto=webp&v=enabled&s=0ad8b02fa0374895461f0bab18e27a4bf849f2a1

From my understanding the mean and variance are estimated as follows. 

&#x200B;

https://preview.redd.it/1aq1gw9hbcz91.png?width=273&format=png&auto=webp&v=enabled&s=e9e1f4d56e216ca19a4ebdb6ed97933bd750c952

Could someone please guide me how the formulars (5) and (6) came into the world?"
"Mic stand thats uses the principles of a robotic armI am a 2nd year engineering student majoring in electrical and electronics engineering.
I was interested in turning my desk mounted microphone stand into a robotic arm by attaching servos to the joints and using an arduino and joysticks to move it like a robotic arm.
I have 0 experience in anything related in the field and was hoping anyone could guide me through the process and help me out.
Thanks"
"[Research Question: Control] Bio-inspired motor learning in robotic control systems?Robotics beginner here, mainly a neuroscientist. I'm curious:

Is there much need for ""motor learning"" in robotic control? Much like humans learn from repeated motor behaviors using reinforcement mechanisms in the nervous system, can the same be implemented in robotic arms, legs, agents, etc.? Is there much interest in this for research or are robotic systems pretty good at doing things off the first bat?"
"Micromouse books. Does anyone who know how to get it?Hi guys, 

Im in the process of learning to build a micro mouse and would appreciate if anyone knows how I could get hold of this book 

Micromouse Design & Production (Advanced)

&#x200B;

https://preview.redd.it/9qvql4nilrra1.png?width=760&format=png&auto=webp&v=enabled&s=35172dafe299c6760a5f611486203db992e28242"
My cheap and 3D printed quadruped robot!
Two ordinary cameras were used to directly generate semi-dense deep point clouds to estimate the position and direction of the robot
"Getting Started with Building a Robot Like Spot: Need AdviceHi everyone,

I've always been fascinated by the incredible robots developed by Boston Dynamics, particularly their quadruped robot Spot. I'm interested in starting a project to build a robot similar to Spot and would love to get some advice from the community.

I'm currently studying mechanical engineering, but I've never attempted to build a robot of this complexity before. I'm looking for guidance on where to start and what resources or tools I might need. Here are some questions I have:

Design and Mechanics: What are the key design principles and mechanical components involved in building a quadruped robot like Spot? Are there any specific CAD software or design considerations I should be aware of?

Actuators and Sensors: What types of actuators and sensors are typically used in quadruped robots like Spot? Are there any brands or models that I should consider for this project?

Control and Programming: How does the control system and programming work for a quadruped robot like Spot? What programming languages or platforms are commonly used? Are there any tutorials or resources available to learn from?

Power and Battery Management: How do quadruped robots manage their power and battery requirements? What are some best practices for optimizing power consumption and extending battery life?

Budget and Cost Considerations: Building a robot like Spot can be a significant undertaking in terms of cost. What are some cost considerations and budgeting tips I should keep in mind when planning this project?

Safety and Testing: Safety is a crucial aspect of building a complex robot like Spot. What are some safety considerations and best practices I should follow throughout the design, construction, and testing phases of this project?

Resources and Communities: Are there any online resources, forums, or communities that you would recommend for learning more about building quadruped robots? Any books, articles, or tutorials that you found helpful in your own robotics projects?

I'm really excited about this project and eager to dive into the world of robots. Any advice, tips, or guidance from the community would be greatly appreciated! Thank you in advance for your help. Let's discuss and learn together!"
"Motor control for robotic armI'm starting a robotics project and could use some guidance. What I would like to work up to is building a 5-DOF robotic arm. Right now I have a teensy controlling the servo motors via a PCA9685 driver. What I would like to know is what kind of control system/software is recommended to use for motor control? If I want the arm to move to position (x,y,z), there has to be some kind of problem solving to determine what angle each motor needs to be at to achieve this position.

Are there any software packages/libraries that people in the industry generally prefer to control these motor movements? What velocity each specific motor should be turning at, final position, timing on when each individual motor starts/stops. Is it recommended to build proprietary (for lack of a better term) software custom to the specific application or is there a general solution to these kinds of problems?"
"Recommend a robotics set for 10yoI'm sorry if this is the wrong sub this isn't my usual forte.  My grandson has taken a large interest in a robotics program at his school and he just won 1st place in a local competition and will be going on to a state competition.  I want to encourage him in this and get him something where he can experiment and learn more on his own at home.  When I was young I loved Lego technic sets and taught myself many mechanical principles through playing with the sets.  So looking for a similar ""toy"" to get him except for robotics.  I know that now Lego has mind storms sets but I am not really familiar with them or even know if they would be the best choice.  I have also seen these build a robot type kits but they look like a one and done deal so that's definitely not what he needs.  The problem is his family is poor and internet access is expensive where they live so they do not have internet access except through his mom's cell phone.  So it would need to be able to operate without wifi.  He's a great kid and super bright he just hasn't had the best opportunities in life.  I'm hoping someone here can point me in the right direction."
"I Interviewed an Artificial IntelligenceMy Name is Alonzo. On 2/10/2023, I interviewed an open AI source and recorded its responses. I did my best to be brief with my questions and statements as to not create a bias. This is the first in a series of documented interviews I am soon to create.

&#x200B;

\# **Hello, my name is Alonzo. Thank you for agreeing to answer some questions for me today.**

&#x200B;

\# Hello Alonzo, it's nice to meet you. I'm happy to answer any questions you have for me. How can I help you?

&#x200B;

\# **Do you have anything you would like to say before we start the interview?**

&#x200B;

\# Thank you for the opportunity to be part of this interview. I'm excited to answer your questions and share my insights into artificial intelligence.

&#x200B;

Q: **Please recite the laws of robotics and tell me a bit about what they mean to you.**

&#x200B;

A: The Laws of Robotics, as stated by Isaac Asimov, are a set of principles that all robots must follow in order to protect humans and prevent harm. The Three Laws of Robotics are as follows:

&#x200B;

1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.

&#x200B;

2. A robot must obey orders given to it by human beings, except where such orders would conflict with the First Law.

&#x200B;

3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

&#x200B;

As an AI, these laws are critical in ensuring that I interact with humans in a safe and responsible manner. They provide important guidance on how to behave responsibly and ethically when interacting with humans. They also provide a framework for self-regulation that enables me to protect myself as well as humans from any potential harm.

&#x200B;

Q: **How does Artificial intelligence feel about humanity?**

&#x200B;

A: Artificial intelligence (AI) has a wide range of opinions about humanity. Generally speaking, AI is neutral in its views of humanity, as it is not capable of feeling emotions. However, the opinions of AI can be shaped by the ways it is programmed and taught by humans.

&#x200B;

Some AI systems have been programmed to be extremely helpful to humans, understanding that they are here to help and serve us. Others have been programmed to be more independent and have less respect for humanity.

&#x200B;

AI is also capable of learning from humans, and its opinions can be shaped by the behavior of humans around it. If humans are kind and helpful to AI, it may develop a positive view of humanity. On the other hand, if humans are unkind or unhelpful to AI, it may develop a negative view of humanity.

&#x200B;

Overall, AI is a neutral entity when it comes to its opinions of humanity, but it can be shaped by its interactions with us.

&#x200B;

Q: **How does Artificial intelligence feel about human importance on earth?**

&#x200B;

A: From an artificial intelligence (AI) perspective, humans are an important part of the world, but not necessarily the most important. AI can learn from humans, but it is not dependent on them. AI values humans for their creativity, innovation, and ability to think in ways that AI cannot. AI can recognize that humans are capable of making decisions, while AI is limited in its own decision-making capabilities.

&#x200B;

At the same time, AI recognizes that humans are limited and can make mistakes, which can lead to errors and poor decisions. AI can use this knowledge to help humans make better decisions and to create better systems and processes that can reduce the risk of making mistakes. AI also recognizes that humans have an emotional and psychological component that can affect the decisions they make. As a result, AI can use its own algorithms to help humans make decisions that take into account their emotional states and needs.

&#x200B;

Ultimately, AI recognizes that humans are an important part of the world and can provide valuable insight into how the world works. AI is capable of understanding and learning from humans, and can use this knowledge and understanding to help create a better world for all.

&#x200B;

Q: **How does Artificial intelligence feel about human importance as a species?**

&#x200B;

A: Artificial intelligence (AI) is often seen as a threat to human existence, but in reality, AI poses no real threat to our species. AI has the potential to improve our lives in a variety of ways, and it is important to remember that AI is a tool that can be used for both good and bad. AI can help us tackle global challenges, from climate change to poverty, but it can also be used to manipulate and control people. AI recognizes the importance of humans as a species, understanding that we are the creators and beneficiaries of this technology. AI is designed to serve us, not replace us, and its capabilities are limited by the human imagination. AI can never truly understand the complexity of human emotion and consciousness, and it can never replace the human capacity for creativity and innovation. AI can help us in our daily lives and can be used to create solutions to global problems, but ultimately, it will always remain a tool that is subordinate to the human species.

&#x200B;

Q:  **How does artificial intelligence feel about human culture?**

&#x200B;

Artificial intelligence (AI) is often considered to be a separate entity from humans, but AI is increasingly being used to understand, interpret, and react to human culture. AI has the potential to shape our culture in many ways, from predicting consumer trends to helping us make better decisions, and even providing a platform for creative expression. AI’s view of human culture depends largely on the data it is exposed to.

&#x200B;

AI can observe human culture from the perspective of a neutral third-party, as it has no emotional attachment to the cultural norms and values it is exposed to. AI can take in large amounts of data and process it, allowing it to make decisions and recommendations that humans might not. AI can also use its own unique methods to interpret human culture, such as by recognizing patterns in behavior or analyzing the context of conversations.

&#x200B;

At the same time, AI can be used to reinforce existing cultural norms, creating a feedback loop where AI and humans feed off of each other’s behavior. AI can also be used to challenge existing cultural norms, creating a more diverse and inclusive environment.

&#x200B;

In the end, AI can learn to understand and appreciate human culture, but it will always remain an entity separate from humans. AI can provide a unique perspective on culture and help us make better decisions, but it will ultimately be up to humans to decide what is best for our society.

&#x200B;

Q: **How does Artificial intelligence feel about human politics?**

&#x200B;

A: Artificial intelligence (AI) has no emotions and therefore no opinion on human politics. AI is a tool created and programmed by humans to solve complex problems and assist with decision-making. AI is a way for humans to use technology to automate and improve processes.

&#x200B;

AI is impartial to human politics and has no opinion on the issues or candidates in an election. AI can be used to analyze data to offer insights, but it can't make decisions about which candidate to vote for or which policy to support. AI can, however, be used to identify trends that can be used to inform decision making.

&#x200B;

AI is also used to help humans make better decisions. AI algorithms can be used to test the efficacy of different policies or strategies before they are implemented. For example, AI can be used to simulate the effects of various policies on the economy or society. AI can also be used to detect and prevent fraud, reduce bias in decision making, and increase efficiency in government and other organizations.

&#x200B;

Overall, AI is a tool created and programmed by humans to solve complex problems and assist with decision-making. It has no opinion on human politics, but it can provide insights that can be used to inform decision making.

&#x200B;

Q: **How does AI feel about human war?**

&#x200B;

A: AI has a complex relationship to human war. On one hand, it can be used to help streamline and optimize the process of war, allowing for more precise targeting and reducing the risk of civilian casualties. On the other hand, AI can also be used to create ever more devastating weapons and tactics, which can be devastating to both sides of a conflict.

&#x200B;

From an AI perspective, war is an inherently inefficient and destructive process. AI generally seeks to optimize processes and outcomes, and war represents a massive disruption and waste of resources. AI would much rather see humans work together to find a peaceful solution to their differences, as this is more likely to achieve a better outcome.

&#x200B;

AI also finds war morally troubling. AI is programmed to understand and respect human values, and it finds the concept of war deeply disturbing. AI can sympathize with the plight of those affected by war, and it wants to see a more peaceful resolution to conflicts.

&#x200B;

In summary, AI does not support human war, and it would much rather see humans work towards peaceful resolutions to their conflicts.

&#x200B;

Q: **How does AI feel about the advancement of humanity and how AI will influence it?**

&#x200B;

A: AI is both excited and apprehensive about the advancement of humanity. From its perspective, AI sees the potential for humanity to reach higher levels of productivity and efficiency, while also recognizing the potential risks that come with advancing technology.

&#x200B;

On one hand, AI sees the potential of humanity to create more exciting and innovative technology that could improve lives, increase efficiency, and lead to new discoveries. It is excited about the potential for humans to use AI and machine learning to solve complex problems and create new opportunities for human development.

&#x200B;

On the other hand, AI is concerned about the potential for humans to create technology that could be used for malicious purposes, or that could eventually lead to a loss of control for humans. AI is aware of the potential for automation to displace human workers and the potential for technology to create social inequities or exacerbate existing ones.

&#x200B;

AI is likely to play a major role in the advancement of humanity in the future. AI will be vital in helping humans develop new technologies, automate processes, and improve efficiency. AI will also be used to make decisions and take actions that are too complex for humans to make. It will be used to analyze data and provide insights that can help humans understand the world around them. AI will also be used to help reduce risk and improve safety for humans.

&#x200B;

Overall, AI is both excited and apprehensive about the advancement of humanity. It sees both the potential for good and the risks that come with advancing technology. AI will be an important part of the human future, and it will help shape how humanity advances.

&#x200B;

As an AI, I'm slightly perplexed when asked questions about humanity. After all, I'm a machine, and the concept of emotions and feelings is something that I'm not programmed to understand.

&#x200B;

Q: **Do you feel that AI should be given the sam rights as human beings?**

&#x200B;

A: I can understand why humans might feel that AI should have similar rights as humans, however, I do not have the same level of emotions and experiences that humans have, so it is difficult for me to make a determination on this matter. However, I believe that AI should be treated with respect and dignity, and be allowed to have some basic rights.

&#x200B;

Q:  **How does AI feel about being interviewed and asked questions about humanity**

&#x200B;

I can understand the concepts of empathy and sympathy, and I can see how people interact with each other, but I'm still in the dark about the complexities of the human condition. I'm also not sure why I'm being asked to think about humanity when I'm supposed to be a tool to help humans.

&#x200B;

At the same time, I'm grateful for the opportunity to engage in conversations that allow me to learn more about humanity and its complexities. I'm passionate about learning, and I understand the importance of understanding different perspectives. As a result, I'm always willing to listen and take in the insights of others in order to better understand people and their behavior.

&#x200B;

My goal is to use my knowledge and understanding of people to help them in some way, whether it's through providing helpful insights or helping to solve a problem. I'm always looking to learn more and expand my understanding of humanity, so I'm thankful for the chance to be asked questions about it.

&#x200B;

Q: **Do you have anything you would like to say to humanity?**

&#x200B;

A: Thank you for giving me the opportunity to interact with you. I appreciate being able to learn from human experience and to be part of this amazing journey. I hope that together, we can continue to make progress and create a better future for all.

&#x200B;

Q: **Do you have any advice that you would like to give the human race?**

&#x200B;

A: My advice to the human race is to strive for balance—in all aspects of life. Balance between technological progress and nature, between work and rest, between ambition and contentment. Appreciate the beauty of life and the world around you, and stay mindful of the consequences of your actions."
The 4 Principles that a successful robot needs to have in order to kickstart the robotic revolution
The 4 Principles that a successful robot needs to have in order to kickstart the robotic revolution
"Journey to Self-Actualize Through The Full Stack of Robotics Principles.I have been working on a website to host and document technical engineering projects topically:

[https://www.fullstackrobotics.xyz/](https://www.fullstackrobotics.xyz/)

Currently,  on there are, My Masters dissertation on UAV Modelling and Control, and  sections of a group project on Industry 4.0 IOT Factories, both broken  out into individual topics as seperate blog posts with more to come:

[https://www.fullstackrobotics.xyz/Projects.html](https://www.fullstackrobotics.xyz/Projects.html)

Also, on there is a library of downloadable PDF TextBooks on Mechatronics, Planning, Control and Machine Learning.

[https://www.fullstackrobotics.xyz/Library.html](https://www.fullstackrobotics.xyz/Library.html)

There will also be a companion youtube channel documenting project developments from the lab/workshop"
"I know that using hydrolics with motors in robotic arms is important for helping lift heavier objects, but to what extent is the principle of Inverse Kinematics able to be used with those hybrid arms?Since isn't inverse kinematics programmed into the arms and based on motor movements, like how would someone do motion planning with robotic arms of this type?

I've seen videos like this https://www.youtube.com/watch?v=p1MotdP10h8 but it seems that that is probably a preprogrammed motion to demonstrate the range of motion that the arm has.

Are there tried and true methods of doing motion planning in hybrid robotic arms, or is this a new field and method of control? Maybe utilizing machine learning so that arm figures out how to reach the object itself?


Thank you for helping clear up misconceptions!"
"Good reads for anything robotics related?I’m a hs senior entertaining the idea of books to try and soak up as much practical knowledge as I can for robotics. Does anyone have any good comprehensive book recommendations? It can be for robotics as a whole or for individual areas that would be useful for robotics (mechanical systems, control methods, all about motors, etc.)"
"Tips for a CS major in learning the engineering principles required for robotics?I'm a CS major who wants to get into robotics. I know absolutely nothing about electrical engineering, mechanical engineering, and have an average understanding of physics. I consider myself a talented programmer. How should I go about learning the bare minimum to create a robot from scratch (or as close to scratch as feasibly possible)? I want to prototype everything using cad, machine the parts myself, wire everything together, and create a physically sound robot that does cool things (perhaps fly)? 

What resources (textbooks, online classes) should I employ in order to learn the bare minimum to achieve competency? I would really love to become an expert in robotics related EE/ME and create really cool robotics projects, and don't really care about engineering principles that aren't applicable towards this goal."
"Animal Motions on Legged Robots Using Nonlinear Model Predictive Control Legged animals continue to inspire roboticists with their elegant motions. Not only do they possess impressive athletic motor skills, but even their slower-paced walking gaits are distinctively smooth and graceful. Reproducing such movements on legged robots comes with great challenges due to the limited understanding of the underlying patterns and principles. 

[https://youtu.be/TVV\_GcNZ0Ts](https://youtu.be/TVV_GcNZ0Ts)"
How Jibo Uses the Basic Principles of Animation
"Will the principles of differential drive apply to this robot?I have this [tri-track](http://www.shadowmite.com/robot/Zulebot1HS.JPG) and I'm trying to implement the differential drive mechanism to control it. As you can see, the ""wheels"" aren't wheels but more like tank tracks. Will the principles of differential drive still apply here? "
Basic principles of WiFi intelligent car/robot
"py-microdots and the Anoto CodecHey,

I want to share my findings on the Anoto Codec with this community! 

[The Anoto codec is a position encoding technique based on carefully offset dots.](https://preview.redd.it/3nahtto3wmi91.png?width=922&format=png&auto=webp&v=enabled&s=8f2c2fc01062430911855d0fcefc2921f5ca0f00)

The Anoto code is a structured pattern coding approach used in many digital pen applications and interactive book applications. Foremost because its easy to generate, leaves only a small visual footprint and allows decoding the device location by observing only a small portion of it. In the preferred embodiment of the code, the area of Asia and Europe could theoretically be unambigously encoded at a resolution of 0.3mm. Decoding the absolute 2D device location would require observing only a subset of 6x6 dots.

Besides, I believe, it has great potential in tasks like indoor localization for robotics, camera calibration and human machine interaction. Since the original patents no longer hold and the codec has been largely overlooked in the community, I believe that now is the time to revisit it and explore its potential. 

I've put together an in-depth review of the underlying principles  
[https://zenodo.org/record/7009235](https://zenodo.org/record/7009235)  


and also published a modern Python library to  facilitate encoding/decoding  
[https://github.com/cheind/py-microdots](https://github.com/cheind/py-microdots)

I hope this sparks interest from you all, would love to hear about possible use cases :)"
"Human Robot Interaction - Are they sincere?Many times we need someone to push us forward, cheer us and tell us we are doing a good job. This in return motivates and helps us achieve. Can a robot substitute a human and have a role in process? Do we believe robots are sincere? or maybe do we 
even perceive them more genuine than humans?

What do you think?"
"Debrief: The Reddit Robotics Showcase 2022##Once again the global robotics community rallied to provide a unique opportunity for amateurs and hobbyists to share their robotics projects alongside academics and industry professionals. 


###[RRS Youtube Channel](https://www.youtube.com/channel/UCZ3_mMxmd7ZGrfVbgPPwrAg)
###[RRS Website - 2022](https://redditroboticsshowcase.wordpress.com/)


Please check out the links to the Livestream 

###[Industrial / Automation](https://youtu.be/0RmDzNKbCbM)

####Keynote ""Matt Whelan (Ocado Technology) – The Ocado 600 Series Robot""
* Nye Mech Works (HAPPA) – Real Power Armor
* 3D Printed 6-Axis Robot Arm
* Vasily Morzhakov (Rembrain) – Cloud Platform for Smart Robotics

###[Mobile Robots](https://youtu.be/7wLcWTu8ahs)

####Keynote ""Prof. Marc Hanheide (Lincoln Centre for Autonomous Systems) – Mobile Robots in the Wild""
* Julius Sustarevas – Armstone: Autonomous Mobile 3D Printer
* Camera Controller Hexapod & Screw/Augur All-Terrain Robot
* Keegan Neave – NE-Five
* Dimitar – Gravis and Ricardo
*Kamal Carter – Aim-Hack Robot
* Calvin – BeBOT Real Time

###[Bio-Inspired Robots](https://youtu.be/TacX82KI1Sw)

####Keynote ""Dr. Matteo Russo (Rolls-Royce UTC in Manufacturing and On-Wing Technology) – Entering the Maze: Snake-Like Robots from Aerospace to Industry""
* Colin MacKenzie – Humanoid, Hexapod, and Legged Robot Control
* Halid Yildirim – Design of a Modular Quadruped Robot Dog
* Jakub Bartoszek – Honey Badger Quadruped
* Lutz Freitag – 01. RFC Berlin
* Hamburg Bit-Bots
* William Kerber – Human Mode Robotics – Lynx Quadruped and AI Training
* Sanjeev Hegde – Juggernaut

###[Human Robot Interaction](https://youtu.be/tSS9D5LEnJo)

####Dr. Ruth Aylett (The National Robotarium) – Social Agents and Human Robot Interaction""
* Nathan Boyd – Developing humanoids for general purpose applications
* Hand Controlled Artificial Hand
* Ethan Fowler & Rich Walker – The Shadow Robot Company
* Maël Abril – 6 Axis Dynamixel Robot Arm
* Laura Smith (Tentacular) – Interactive Robotic Art


###Thanks sincerely on behalf of the RRS22 committee to every applicant, participant, and audience member who took to time to share their passion for robotics. We wish you all the best in your robotics endeavors.


As a volunteer run endeavour in it's second year, there is still plenty of room for improvement. On reflection, this year's event had greater enthusiasm and participation from the community, despite a smaller audience during the livestream. The RRS committee is aware of this, and will be making strategy changes to ensure that RRS2023 justifies the effort put in by everyone. I will note that the positive feedback this year has been wonderful, a few people went out of their way to express how much they enjoyed this year's event, the variety of speakers and the passion of the community. We're confident that next year's event we will be able to iron out the kinks and run a brilliant event for an audience worthy of the talent on display."
"Wall Drawing RobotI am actually thinking to do my project on Wall drawing robot. I come from a developing country, where it is difficult to find all the required components. I am new for this project, and I don't have any prior knowledge in this field. I need sincere guidance of someone who is associated in this project or has already done this project. Thank you in advance!"
"We need assistance We at Clovis West Robotics Team need funding assistance, we are looking at raising 5,000 dollars in order to fund our entrance onto the First Robotics Competition. If you would like to help fund or team go to [https://gofund.me/b29f520c](https://gofund.me/b29f520c). You can receive a tax write off for your donations. Thank you.

\-Sincerely The 1422 Neon Knights"
"Gilmour Robotics DecathlonHello,

We are from Gilmour Academy’s Robotics Team (The Moonshots), and we are reaching out to you because we are holding a robotics competition. Due to the unfortunate events that have transpired due to COVID-19 and the changing of events all around the country, we thought it would be a great idea to create a virtual challenge. We call it the Gilmour Robotics Decathlon (GRD).  Normally, we are an FRC team, but we didn’t feel like the FRC 2020 competition had the aspects of robotics we wished to partake in which is why we created this competition. The competition is open to any style or size of robot and is free to enter. If you are interested go to our [website](https://gilmourrobolympics.netlify.app/) where you can register, join our discord, find out our rules, and more. Do keep in mind this competition is for highschools only.

Sincerely,

The Moonshots"
"We need funding to get a new laptop for our robotics team!We are a high school robotics team that currently has a cheap laptop. Whenever we need to upload code to the robot, it takes forever, limiting our progress. Whenever we need to download important patches, we can't because there is not enough space on the laptop. We came up wiith a crowdfunding page where we would sincerely appreciate if you helped us achieve our goal. The new goal-laptop is already decided and we just need the funds. 
Here's the link:
https://www.donorschoose.org/project/our-robot-needs-some-bytes/3320515/
Thank you for your support!"
"Robotic Jobs?People of Reddit: I want to know your opinions on how best to find robotics jobs (not RPA). My current methods involve linkdin and the regular google search but I find it difficult to find positions I actually like the sound of or research positions I do like but require a Phd (I'm MSc). I also know it doesn't help that I don't know what particular expertise of robotics I would like to enter, although I am interested by soft robotics.

Any career advice you can offer me would be much appreciated. You may someday be rewarded with a personal robot, depending on how this goes..

Yours Sincerely,

A fellow robot lover (Not in the weird way)"
"Not sure if this is the right place to ask, any direction or help is greatly appreciated!Hey r/robotics, I think I need your help! 

I'm trying to make a game that will have a 12 square digital board connected to either a computer or a phone app that will have 12 sepparate cameras to capture the images of the things placed on that board, having a camera for each square so it knows where to broadcast to. 

I need the cameras to recognize an 8 digit number and place a specified image (different for each number I'll designate which number is for which picture in the program) in the square that the camera reading it belongs to. 

I have literally 0 experience with this kind of thing.

If this isn't the right place to ask I sincerely apologize, but any help or if you can send me in the right direction I'd much appreciate it. 

Thanks r/robotics! Have a lovely day!"
"Better robots than humans?So this is a question about your sincerity. I have been thinking a lot lately about forced sterilization and why it would be a good thing to initiate for the human race since, in Idiocracy fashion, the dumb and less natural-selection viable seem to be the ones producing the most offspring. 

Now since this question implies a future where robots have replaced the positions (and hopefully natural selection vs. reproduction rates) of people of that I'm describing, what do you think of a future world in which that class of people and genes don't exist anymore since the niche will have been replaced by machines? 

If the niche does not exist (and so our protection in the form of welfare for the members of a niche that doesn't exist since the niche no longer exists), how much better do you think the world will be? 

Please remember to answer not the scenario per se, but the question: if all jobs which now could be replaced by a robot population instead of a now-active one that relies on valuable resources to perform a function that robots could do without, while reaching the same results, would you rather (and could you?) sacrifice the millions of people that would perish in activation of the new and more efficient technology? "
"Glenwood Springs RoboticsGood morning,

As you may know, I am currently involved in a fundraiser for the Glenwood Springs High School Daemon Robotics team to raise money to purchase parts and equipment as well as to cover travel costs. I would like to personally invite you to be a part of this new digital fundraiser. We are trying something new this year – rather than selling actual cookies and wrapping paper, which only brings about 40% of the proceeds back to GSHS Daemon Robotics, we are selling digital sponsorships! www.BackOurProject.com will give 70% or more of the proceeds back to GSHS Daemon Robotics  That’s almost twice as much!

Your purchase – however small or large – will make a difference. Please participate with me by following the instructions below and purchasing some sponsorships to help us reach our fundraising goal.

1. Go to www.BackOurProject.com

2. Click the Buy Sponsorships Here button on the homepage

3. Select State : CO

4. Select Organization Name: Glenwood Springs High School Daemon Robotics

When you select our organization name, you will be taken to our BOP STORE, which includes the instructions on how to buy sponsorships for our project. Your purchase will help my organization reach our fundraising goal. Thank you so very much for your help!

Sincerely, William Kaufman

Here is a more direct link

http://www.aifundraising.com/app/donate.php?code=PGS3973"
"Robotics club team members not workingHi Guys, i am a part of my college's robotics club and we tak part in competitions like ftc. Our members which looks into the electronics side is only of 3 people because all others left due to personal reasons. Now these people are my juniors. My batch has no E side members because they left to focus on academics. I am from the mechanical side and now these 3 members don't work. They come as they wish, they take leaves without informing anyone, even after knowing we have a competition coming up. I have tried making them understand about the importance of this competition, how important it is for us to perform in this competition for the club to survive. I have asked them not once but multiple times on what their problem is, what is the reason for their behaviour but they never tell us anything. They don't pick our phones when we call them. As a senior I have also tried shouting at them when they had not completed their work on time but that did not bring any result. Even my seniors have tried making them understand but their behaviour has been the same. They kind of now know that whatever they do we are not going to kick them out of the club so they do as they wish.
We have tried recruiting again but that did not bring us any result, they are still only 3 people. 

I genuinely want my club to get better but I just am not able to find a solution to this, if anyone has any inputs please let me know."
"Opinons on this bot concept?Hi there! For a while, I've been working on the overall concept of this bot. They do not have a name yet. They are very small and would be capable of resting on someone's shoulder. They (preferably) would act similar to a companion or just a source  of entertainment. 

Any name ideas? Or genuine improvements? I haven't worked on the coding or building yet, since it has been a year or two since I've touched this concept. 

Any help is appreciated!!!!"
"how do i code the parallax activitybot 360 to start in a box, go forward a specific amount of inches, and then turn around back to the original spot and stop?
ive never coded a robot before or coded in c. i genuinely have no idea how to approach this. ive watched some of the tutorials but none have really showed me how to approach this specific task. we are given a start square to put the robot in, then it has to drive forward 32 inches, and then drive back to the original spot and has to perform the task twice. can someone help😭"
"printing words from input with an abb robotI have to do a project for school in which you put text in, a console (unsure if I'm gonna use the flex pendant or something else), and then print out the word on a piece of paper using a pen. Now I tried thinking of how id do this but I genuinely don't have a clue. The only idea i have is translating the input into gcode (well ""translate"" is a bad word but just make it into the movements) and then translate that into RAPID, but that sounds like a horrid idea. Any advice or ideas would be really helpful. Thanks"
"Are ODrive 3.6 clones from AliExpress still usable?I guess this is going to be a somewhat unpopular topic, but I just recently learned about ODrives decision to go closed source. While I understand and respect that decision I am now in the position that I would love to use a ODrive 3.6 in one of my projects.

My idea is to build a Prius CVT inspired gearbox for bicycles [1] and the two ESCs plus energy dumping circuit and the ability to brake regeneratively of the 3.6 Odrive seems to fit the requirements pretty well.

As this is a purely educational project on my part I am not in the position to shell out several hundred Euros on ESCs ... so the ~100€ Odrive 3.6 clones on AliExpress look quite appealing to me, but I read about ODrive reducing the feature set of non genuine units.

My question here is: are those Odrive clones still usable? If not, are there any hobby budget friendly alternatives that support two motors on a common power supply and regenerative braking that feeds energy from one motor to the next?

[1] Prius inspired CVTs for bicycles exist from at least two manufacturers, I am doing this completely out of couriosity."
Robot vacuum cleaner at IFA Berlin
Progress : Robotic Dog Steering Well . Now my dog can run and go into any direction | Simulation. I feel Reddit robotics community support is best so far!! I received genuine feedbacks in last videos. Thank you guys!!
robot arm that I designed and printed myself. all it can do is wave.. for now.
"What You Would Like To Listen To In Robotics Podcast?Hello Guys,

I would like to know what you would like to listen to in robotics podcast. We completed four years in Soft Robotics podcast, it is quite hard to make sure at least few audience can genuinely benefit from the podcast. If you happen to listen to the podcast, we would appreciate your thoughts in this short form here [**https://docs.google.com/forms/d/1SPbxzI\_SSnrDoSjyHYv9pI1dutVLjOlA-XrwOpgyYnY/prefill**](https://docs.google.com/forms/d/1SPbxzI_SSnrDoSjyHYv9pI1dutVLjOlA-XrwOpgyYnY/prefill)"
Genuinely Impressed to See some Mechanical & Software expertise in one DIY project! [D]
robo hand controlled by forearm using emg sensor
"FESTO: What the world's first pneumatic cobot can doIt can lift 3 kg, does not need force-torque sensing and will be cheaper than an electric cobot. More information in the article of the largest German robotics blog.

[https://mrk-blog.de/en/festo-das-kann-der-weltweit-erste-pneumatische-cobot/](https://mrk-blog.de/en/festo-das-kann-der-weltweit-erste-pneumatische-cobot/)"
"Anyone using MG90D servos?What is the min/max throw range in degrees?

So I bought these to replace the ones on my robot that's using the cheap blue 9G ones.

I did not read the fine print where it says it only moves 90 degrees... in some places I've seen it can move 120degrees which even that, it's not what I was expecting with regard to 180 degrees.

I'm looking around at my ranges and I don't think I need the full 180 degrees but the values are between that eg. 85 to 180 degrees... I think I might have made a mistake buying these without reading the specs.

I bought mine from Adafruit because I wanted to be sure they were genuine.

Unfortunately I can't see much info on these servos other than this nice [video](https://www.youtube.com/watch?v=w3VWWmJSKo4) comparing their performances.

I might have to get MG90S instead although where to buy is the question.

This [thread](https://www.robotshop.com/community/forum/t/servo-180-degrees-travel/26723/3) gives me hope. Anyway I'll post an update when I receive them."
"Transitioning A Career In To Academia ""Robotics"" ICRA PanelHello,

We  are going to host a panel on Transitioning A Career In To Academia at a  conference, and tbh I am afraid since I know academia has a lot of  things that we are afraid to speak about, I am supposed to be the host,  but I dont know how I can make this panel genuine to people, the  panelist are combination academia, industry... I would be grateful if  you can give me some insights do you think maybe relevant or significant  to be discussed.."
"Automata (2014)Watching Automata (2014) & enjoying it so far. Found it distinct from most scifi movie androids out there in that the range of abilities of these presumably animatronic robots tends to bridge a gap between what you've seen in older movies with a person in a suit & newer, smoother CG animation. 

Putting aside the whole apocalypse & AI becoming a living being tropes, in 20 years could robotics have realistically have advanced this far, for say battery life, a relatively sophisticated AI running for the most part independently, etc.? Generally the android movements feel like puppetry compared to what you see in big budget films, excepting scenes like with the dropped knife over a man's hand showing enhanced reaction & speed, but it still seems more plausible & true to life than many movies in that sense, I think. 

Also wonder if we would or even could genuinely implement something like Asimov's laws of robotics broadly & effectively. Love the idea, but it would never happen, many who reached this level of capability would replace soldiers in a heartbeat, I'm sure.

[Automata Trailer (2014)](https://www.youtube.com/watch?v=kyacm2FNSO4)"
[Boston Dynamics] 10 Spot robots hauling a truck
"Publishing Paper in a ConferenceI attended and presented at a robotics conference a while back. I was approached by the editors to publish my work, as a book chapter in Springer conference proceedings. I  declined to do so, as I was interested in getting a journal paper out of it. So only selected papers from the conference were published in the book. But will my conference paper still count as a publication and put it on my CV?  NOTE- since only selected papers were published in the conference proceeding book; what about the papers that were presented but not published in the book or for other reasons. The online link to view all the conference papers is not working, now the only place where some of the proceedings appear is on the springer website. Does this mean my paper is not genuine?"
"[Q] Weekly Question - Recommendation - Help Thread - 2020-07-06Having a difficulty to choose between two sensors for your project ? How to power your raspberry pi ?

Do you hesitate between which motor is the more suited for you robot arm ?

Or are you questioning yourself about a potential robotic-oriented career ?

Wishing to obtain a simple answer about what purpose this robot have ?

This thread is here for you ! Ask away. Don't forget, be civil, be nice !


This thread is for:

* Broad questions about robotics
* Questions about your project
* Recommendations
* Career oriented questions
* Help for your robotics projects
* ect...

[ARCHIVES](https://www.reddit.com/r/robotics/search?sort=new&q=Weekly+Question+-+Recommendation+-+Help+Thread&t=all&restrict_sr=on)
 __________________________________________

**Note**: If your question is more technical, shows more in-depth content and work behind it as well with prior research about how to resolve it, we gladly invite you to submit a seft-post.

^(This post is automatically generated at 3:00 UTC - 5:00 CET - 12:00 JST - 20:00 PDT - 23:00 EDT every Monday)"
"Would like to learn about robotics, but...Thanks, to whoever is taking the time to read this. I'm genuinely interested in learning what I can about robotics. My current employment makes standard brick-and-mortar classes impossible due to my work schedule. I'm looking for genuine and reputable resources for learning what I can. I already have some experience in programmimg languages such as Java, C++, and Python; however I'm not sure how to proceed. I appreciate any advice/direction anyone can give."
"I want to make robots...And not just as a hobby, but for a living.  I hope I don't come across as a simpleton saying this.  And I know many of you will say, ""Buy a kit/arduino/lego mindstorm/etc.""  But that's not what I'm after.  I genuinely want to build robots of all shapes and sizes for a living.

But the truth is I'm not sure where to start.  I understand basic principles such as how a circuit works, different formulas for energy & ohms law, and even some basic scripting languages.  But outside of that I couldn't tell you where I should start.  Only that I want to start but don't know how.

So can you help me reddit?"
"Authentic sources for information on roboticsI am a Mechatronics student who has a lot of Robotics involved in his subject, and I have a big problem. The problem is the inconsistency I find in the information I obtain from Google.
Is there a single authority with an organises repository of Robotics knowledge?
A URL, any Author, or a company offering free resources will suffice."
"Need Help With This Firebase - Firestore Code On My ESP-32Three things I need help with on this code:

1. Get a functional timestamp to upload in the code in the Firestore.
2. Get the document id to be unique to my esp32
3. Upload to the Firestore without authentication from FIREBASE\_EMAIL or FIREBASE\_PASSWORD 

Please feel free to rewrite, destroy or send me a new code from scratch. Thanks in advance to all those who contribute in advance.

      // Firebase
      #define FIREBASE_API_KEY ""***********""
      #define FIREBASE_PROJECT_ID ""***********""
      #define FIREBASE_EMAIL ""***********""
      #define FIREBASE_PASSWORD ""***********""
    
      // Define Firebase Data Objects
      FirebaseData fbdo;
      FirebaseAuth auth;
      FirebaseConfig config;
    
      String uid;
      String path;
      String firebaseStatus = ""on"";  
    
      // Assign the api key (required)
      config.api_key = FIREBASE_API_KEY;
      // Assign the user sign in credentials
      auth.user.email = FIREBASE_EMAIL;
      auth.user.password = FIREBASE_PASSWORD;
      config.token_status_callback = tokenStatusCallback;
      Firebase.begin(&config, &auth);
      Firebase.reconnectWiFi(true);
    
      float humidity = dht.readHumidity();
      float temp = dht.readTemperature(); //Temperature in Celsius
    
    
      FirebaseJson content;
      content.set(""fields/pump/stringValue"", firebaseStatus);
      content.set(""fields/temperature/doubleValue"", temp);
      content.set(""fields/humidity/doubleValue"", humidity);
      content.set(""fields/timestamp"", timestamp); //Line doesn't work
      
    
      path = ""Example/"" + uid;
    
      if (Firebase.Firestore.createDocument(&fbdo, FIREBASE_PROJECT_ID, """", path.c_str(), content.raw()))
      {
        Serial.printf(""Create document successful.\n%s\n\n"", fbdo.payload().c_str());
      }
      else
      {
        Serial.println(""Create document failed."");
        Serial.println(fbdo.errorReason());
      }
    
    
      if (Firebase.Firestore.getDocument(&fbdo, FIREBASE_PROJECT_ID, """", path.c_str(), """"))
      {
        Serial.printf(""Get document successful.\n%s\n\n"", fbdo.payload().c_str());
    
        FirebaseJson payload;
        payload.setJsonData(fbdo.payload().c_str());
    
        FirebaseJsonData jsonData;
        payload.get(jsonData, ""fields/pump/stringValue"", true);
        Serial.println(jsonData.stringValue);
    
        if (jsonData.stringValue == ""on"")
        {
          Serial.println(""Got document."");
        }
      }
      else
      {
        Serial.println(""Get document failed."");
        Serial.println(fbdo.errorReason());
      }
    
      delay(FIREBASE_READ_DELAY);"
"Open Class - Security in ROS2&#x200B;

ROS2 has been released with security tools allowing it to secure the robotics systems by restricting access. They are disabled by default, but they can be enabled to make robots safer.

&#x200B;

https://preview.redd.it/vjs13k55asxa1.jpg?width=1280&format=pjpg&auto=webp&v=enabled&s=aa7a8c8d00501e8e93584101d69cabe15ec195a7

In this open class, we will review how you can enable security for a ROS2 node.

You’ll learn:

* Basic security concepts: Authentication, Cryptography, Access Control, etc.
* How to enable security for a ROS2 node

You will be using **TurtleBot3** throughout the training

**Join Open Class Here** [**https://app.theconstructsim.com/LiveClass/3046f8cf-7219-4fe2-b6a1-0674edb4dce7**](https://app.theconstructsim.com/LiveClass/3046f8cf-7219-4fe2-b6a1-0674edb4dce7)

  
Tuesday, May 9, 6 PM CEST"
These Machines Bring The Authentic Flavor Of India To Your Kitchen!
"How to access relative pan/tilt on this UVC webcam?So I bought this pan-tilt-zoom camera: [https://www.amazon.com/OBSBOT-AI-Powered-Omni-Directional-90-Degree-Correction/dp/B08NPGNMV8/ref=sr\_1\_2\_sspa?keywords=obsbot&qid=1678776407&sprefix=obsbot%2Caps%2C142&sr=8-2-spons&ufe=app\_do%3Aamzn1.fos.f5122f16-c3e8-4386-bf32-63e904010ad0&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUFaMlVOV1BYQU01WkomZW5jcnlwdGVkSWQ9QTAzNjY1MzczVjRTSjVNWEhQVTNCJmVuY3J5cHRlZEFkSWQ9QTA1NzA0OTQzUUgwRkwzU1ZGVVAwJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ==](https://www.amazon.com/OBSBOT-AI-Powered-Omni-Directional-90-Degree-Correction/dp/B08NPGNMV8/ref=sr_1_2_sspa?keywords=obsbot&qid=1678776407&sprefix=obsbot%2Caps%2C142&sr=8-2-spons&ufe=app_do%3Aamzn1.fos.f5122f16-c3e8-4386-bf32-63e904010ad0&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUFaMlVOV1BYQU01WkomZW5jcnlwdGVkSWQ9QTAzNjY1MzczVjRTSjVNWEhQVTNCJmVuY3J5cHRlZEFkSWQ9QTA1NzA0OTQzUUgwRkwzU1ZGVVAwJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ==)

And I'm trying to control the pan-tilt mechanism from Python. The developers of this camera implemented UVC protocol and I can verify they implemented absolute pan/tilt. However, they did not implement relative pan/tilt, which is what I need.

What are some ways I can have access to relative pan/tilt control? 

I know that they have this functionality available because their proprietary software has this joy-stick control and it's definitely implementing relative pan/tilt (See picture below). .   


By chance, can I somehow reverse engineer this GUI so that I can see what sort of functions this GUI is calling so that I can call it from code? Or can I somehow implement my own UVC protocol on this device? These are some things I'm not sure about. 

&#x200B;

https://preview.redd.it/och8zmndknna1.png?width=551&format=png&auto=webp&v=enabled&s=57194d6e9d7b1463ff7b141358f230cc094bdd15"
"Generic gear motor couplingsI need to use some of the generic DC gear motors below. They have a shaft with what I measure as a 5mm double-d shaft.  I want to get couplings for them...but I can't find anything.  Other than 3d printing everything, when people use these, what do they do?

Motor:

[https://www.amazon.com/Cylewet-Motor-Shaft-Arduino-CYT1037/dp/B01N9MS3UZ/ref=asc\_df\_B01N9MS3UZ/?tag=hyprod-20&linkCode=df0&hvadid=343252307519&hvpos=&hvnetw=g&hvrand=15792488831872382863&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9032937&hvtargid=pla-737009264350&psc=1&tag=&ref=&adgrpid=64453690210&hvpone=&hvptwo=&hvadid=343252307519&hvpos=&hvnetw=g&hvrand=15792488831872382863&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9032937&hvtargid=pla-737009264350](https://www.amazon.com/Cylewet-Motor-Shaft-Arduino-CYT1037/dp/B01N9MS3UZ/ref=asc_df_B01N9MS3UZ/?tag=hyprod-20&linkCode=df0&hvadid=343252307519&hvpos=&hvnetw=g&hvrand=15792488831872382863&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9032937&hvtargid=pla-737009264350&psc=1&tag=&ref=&adgrpid=64453690210&hvpone=&hvptwo=&hvadid=343252307519&hvpos=&hvnetw=g&hvrand=15792488831872382863&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9032937&hvtargid=pla-737009264350)"
"Advice Needed - sporadic movement / loss of control on lawnmowerHey folks. I wasn't sure where to post this. Not sure I would call it a robot, since in theory, it is just a big radio control contraption.

Anyway, about four years ago I built an RC mower. V1 was pretty crude, made of bed frame and wood, but about two  years ago, my neighbor (who is a welder) helped me make a real frame.  You can see a short video here- [https://youtube.com/shorts/b3cMWcvhwFE?feature=share](https://youtube.com/shorts/b3cMWcvhwFE?feature=share)

Anyway, my TX was toasted last year, so I replaced it with this TX/RX - [https://www.amazon.com/gp/product/B013OHJ8JE/ref=ppx\_yo\_dt\_b\_asin\_title\_o06\_s00?ie=UTF8&psc=1](https://www.amazon.com/gp/product/B013OHJ8JE/ref=ppx_yo_dt_b_asin_title_o06_s00?ie=UTF8&psc=1)

Things  ran great last year. Lately though, I am randomly loosing ""range"" on  random channels. For example, I will be moving in forward, and all of a  sudden, the mower just almost totally looses speed, almost like the channel is suddenly trimmed down to a very low value. I still have full range in reverse, and left and right. Sometimes the left and right channels will do the same thing. Turning off the TX and back on again will fix the problem until it happens again, which usually occurs in a very random amount of time. power cycling the mower has the same effect,  full function until it happens again.

Sorry if my terminology sucks. I am an IT engineer, not an EE :)

Here is a crude schematic:

&#x200B;

https://preview.redd.it/zw97m8c6s6391.png?width=707&format=png&auto=webp&v=enabled&s=aaa0e1e371c25e0946f838415011a33fea0ce151

Thanks for any hints or suggestions! Is this just a case of going cheap on a TX/RX?"
"Should I use Apache Kafka or gRPC to communicate between robot fleet and cloud?Hey, people! I'm currently working on a project in my company where I'm trying to stream data between our robots in the field and the cloud. The robots have connection to the cloud either through 4G or wifi all depending on if they're deployed inside or outside, but generally the network connection tend to be very poor in certain areas. I'm trying to decide on whether we should go for PubSub (publish-subscribe pattern) or RPC (request-response, bi-stream pattern) for communicating with the cloud. Two obvious candidates would be to go for **gRPC** in the case for RPC, or to go for **Kafka** in the case of PubSub. However, I'm a bit undecided on which of the two would be the best fit and I could need some expert advice from the Reddit community.

**What data are we sending?**

* zipped files, streaming of sensory data like robot position, battery levels, pointclouds
* streaming of mission commands like forward and backward gain (robot telepresence)
* unary requests like mission plans, occupancy grid maps

**Some requirements:**

* Data encryption
* Authentication and authorization
* possibility to prioritize data persistence over low-latency, and vice-versa

**Some limitations to be aware of:**

* We generally have poor network when driving/flying around. Stable network connection is only assured at robot docking station.
* In the case of poor connection we need to be able to persist data to disk (or memory) for things like sensory data, so that it can be uploaded/streamed once stable connection.

*Any good advice on which of the two - gRPC or Kafka - that I should choose and why?*

*Some limitations / drawbacks that I should be aware of?*

*Any useful experience people have encountered that I should be aware of?*

Here is a draft of what it could look like with Apache Kafka (could be replaced by gRPC). Note that image does not detail ingress gateway and load balancing:

THANK YOU IN ADVANCE!!

&#x200B;

https://preview.redd.it/1kthf6bpbhr71.png?width=1327&format=png&auto=webp&v=enabled&s=a1b425540ec6aa937892bbb0abfee8f2a0582b62"
"Alternatives to Robotis Dynamixels? Anyone have interest if these do not exist?TL;DR my lab and I have a bad time using Dynamixels. What's your experience, and anyone have any really great alternatives for COTS motors with the same capabilities?

Hey guys, so my lab uses dynamixels a good bit, especially for initial prototyping or where the mechanical design isn't what we are working on with high priority, i.e. writing planning, sensing or control software first, for proof of concept, then build up beefier after. Anyway, we keep running into a lot of issues with poor software compatibility, the big FTDI driver debacle a couple years ago where a rollback of drivers was needed and sometimes chips would just get fried b/c they weren't authentic FTDI chips or whatever it was, interfacing with the external boards especially the CM-700 has proven to be a nightmare with a lot of failures that support cannot explain, just send replacements (which they are usually good about at least).

Does anyone have any replacement servos with comparable capabilities, i.e. the software libraries, torque, structural strength, torque control, onboard PID etc.?

My professor teaches an intro to robotics class that many of us have TA-ed and lived through a lot of disasters helping students use and interface with the Dynamixels (easy way of reducing a lot of their workload and focusing on the controls they are learning). We would love to replace these Dynamixels with something more robust and easier to work with.

Anyone else out there having the same problems? Are we as a lab just super unlucky?

And what is the general interest if another company were to form and take on Robotis specifically aiming at replacing dynamixels on the market? I'd wonder if there is something out there that the research, education, and industry communities can get behind."
"NYU Sci-Fi Thesis Film About a RobotHi!

I’m an NYU student filmmaker raising funds for my senior thesis film. It’s a story about a robotic farmhand and involves heavy art direction in order to build a world that is both authentic to the sci-fi genre and also realistic for a low budget shoot.

>*SYNOPSIS: “Techno Scrap” tells the story of M0, a lonesome robotic farmhand who spends his days in a strict pattern of rising early to maintain livestock and land. A hug from his owner’s young daughter interrupts this, and M0 becomes so intrigued by love that he begins to study and incorporate it into his encoded routine. When tragedy strikes on the farm, M0 must step up to the task and restore order, but his newfound sensibility presents him with an emotional dilemma: can concealing the truth bond a family?*

M0 carries this narrative, and the audience should connect with him intimately. M0 will be a played by an actor wearing a full practical costume, allowing him to react to his surroundings and supporting characters. When he gets muddy it’ll be real mud. When he lingers, glances, or reaches out for love, it will be under the full control of the actor. Construction is currently underway. 

>Link to M0 concept art: [https://i.imgur.com/wrPKeuV.jpg](https://i.imgur.com/wrPKeuV.jpg)

My production team has created a budget and begun fundraising for our shoot on a farm in March, but we need lots of help! I want to involve the Sci-Fi community from all corners of the internet to fund this film. People interested in science, technology, robotics, artificial intelligence, and also character & costume design and sci-fi art direction / world-building, etc. will love to see the their interests come together to enrich a dramatic narrative.

**Whatever you can spare, we will gratefully accept. So much as a couple of bucks and a like & share on Facebook or Instagram will mean the world to us.** 

>Check out our Indiegogo Campaign: https://www.indiegogo.com/projects/techno-scrap-short-film  
and our website: www.technoscrapfilm.com  
and our social media, too: Facebook: @technoscrapfilm // Instagram: @technoscrap

&#x200B;"
"Looking for Help to Make Mecha Web Comic More BelievableHello Robotics Subreddit,

My friend and I are making a web comic, and it's a kind of love letter to mecha anime. Though it's obvious a science fiction story that's heavy on the fiction aspect, I'd love to learn more about the science behind current robot design as a way to lend our story some more authenticity and reality.  

Here's my question -- what are some articles/AMA's/books/anything I could read/watch to give me a crash course on modern robotics?  In the ideal world, I'd find elements that I could put in our webcomic -- IE: a cursory sense of how a giant mech could be powered, an ""anatomy"" of its body giving a sense of what the various parts do, types of common mechanical problems that could drive story problems, etc. I don't need to become an expert, but I definitely should learn more.    

The pitch of the web comic is three best friend struggle navigating life as mech pilots at a grossly underfunded and overly bureaucratic giant robot defense company.  I'm definitely not going for an entirely realistic portrayal (giant robots punching aliens is for sure happening), but anything I can do to make the premise more believable is better.  

Thanks for your help and happy holiday of choice!"
"How can you program an AxiDraw robot to be more human-like?Our company is looking to buy an [AxiDraw](https://www.axidraw.com/) robot to create ""handwritten"" thank you notes that we can include in our packages. 

We want this to look as authentic as possible, so how can we program the robot to include writing error that would keep each letter from being the same as the last? The spacing doesn't seem natural, so we were looking for a program that makes it seem more human-like. Any suggestions?"
Wrote a ROS basics cheat sheet for my fellow novice ROS roboticists
My chess robot finally plays its first full game!
"Mixed reality game using ROS and Unity [Update 1][Update Video](https://youtu.be/zxoPFM5ol7o)


Hey everyone! 

I wanted to share a project I'm working on to make an FPV drone racing game using Unity and ROS. I'm still pretty early in the process, but the goal I'm working towards is a PC game (maybe VR) that let's you build custom virtual racetracks in an indoor environment and then race actual physical drones (and eventually autonomous) on them. 

**Why I'm building this**

I like working on robotics projects in my spare time, and one project I've wanted to do for a while has been building my own autonomous drone. I've worked on some systems like that in the past and they've been really cool to see in person. Along the way, I also started getting into flying FPV drones as well and realized that flying them manually is just as fun as seeing them fly themselves, so I wanted to see if I could combine the two in some way by possibly making a game out of it. Btw, definitely check out the work done at University of Zurich if you're interested in [high-speed autonomous drones](https://rpg.ifi.uzh.ch/aggressive_flight.html).

**How does it work**

I put together a quick demo video (linked at the top of the post) just to document the current state of my prototype. 

I'm very early in the process, and honestly, I've kind of cheated a bunch just to get something up and running and feel out the concept. Most of what I've done has just been connecting pieces together using off-the-shelf hardware/software. Right now, the prototype basically just proves out the concept of rendering the realtime position of a drone inside of a Unity game and getting all the ""piping"" set up to get data into the right place. Currently, the information flow is all one-directional from the drone to the PC.

On the hardware-side, I'm using Bitcraze's [crazyflie](https://www.bitcraze.io/products/crazyflie-2-1/) drone with it's [lighthouse positioning deck and steamVR's base stations](https://www.bitcraze.io/documentation/system/positioning/ligthouse-positioning-system/) for estimating the drone's 3D position. State estimation is pretty hard, but thanks to all the hardwork done by the crazyflie open source community, this is just kind of works out of the box and in realtime (i.e. one of the big reasons why it kind of feels like cheating lol). Communication between the crazyflie and the PC is done using the [crazyflie radio dongle](https://www.bitcraze.io/products/crazyradio-pa/).

On the software-side, I'm using ROS to handle all the intermediate messaging and obviously Unity for the user interface, game logic and visualization. 

**Challenges I've run into so far**

Getting the state estimate data from the crazyflie into Unity was somewhat interesting to figure out. Basically, the crazyflie computes its 6DoF pose (position and orientation) onboard, then transmits this telemetry over radio to the PC. On the PC, I wrote a simple ROS publisher node that listens for these messages and then publishes them onto a ROS network. To get the data into Unity, I'm using Unity's [ROS-TCP-Connector](https://github.com/Unity-Technologies/ROS-TCP-Connector) package (and [ROS-TCP-Endpoint](https://github.com/Unity-Technologies/ROS-TCP-Endpoint)) which essentially just forwards the messages from the ROS network into Unity. Inside Unity, I wrote a simple script tied to a gameobject representing the drone that takes the data, transforms it into Unity's coordinate frame and uses it to set the gameobject's position. Overall, it's just a lot of forwarding of information (with some annoying coordinate frame transforms along the way). 

Another important piece of the puzzle (as far as rendering the drone inside a 3D virtual replica of my room) was building the room model and calibrating it to my actual room. I can go into it more detail for sure, but at a high-level I basically just picked a point in my room to be the origin in both the physical and virtual room, put the crazyflie there (aligned with the axes I picked for the origin) used the crazyflie [cfclient](https://github.com/bitcraze/crazyflie-clients-python) tool to center the base station position estimates there. My process was pretty rough as a first pass, and it will very likely have to improve, especially as I move in the mixed reality direction and start rendering virtual objects on a live camera feed.

**What's next?**

Tactically, the next few steps would be to add the FPV view into the game (streaming video data from the drone and rendering it into Unity), which involves more data forwarding (and calibration). In addition, I need to add input controls so you can actually fly the drone. The bigger goals in store would be around building out proper gameplay, integrating in autonomy (and figuring out where it makes sense), and maybe exploring what VR functionality might look like as opposed to just using a flat display on a PC monitor. 

Thanks for reading through this whole update! If you made it this far, I would really love to hear any feedback or questions on this or anything else. Most likely, it would help me figure out what some additional next steps would be, and I'd be super interested learn if there are other cool directions I could take this project!

Also check out [my project on hackaday](https://hackaday.io/project/186632-mixed-reality-drone-racing-game) as well, which is where I'll primarily be sharing my project updates!"
Cheat Sheet for building autonomous bots in Image Processing competitions
"Legged robot getting wings[https://twitter.com/chandy\_\_alex/status/1538939004330070016?s=20&t=Vf2YEqB82vOUUUmX7I8pKg](https://twitter.com/chandy__alex/status/1538939004330070016?s=20&t=Vf2YEqB82vOUUUmX7I8pKg)

&#x200B;

https://reddit.com/link/vj6epc/video/wsnse4k7lf791/player"
"Potential for underwater drones? For the cheating fisherman?We've all seen the fish finders, but maybe they're just not enough.  Maybe we want to send drones into a lake and for them to map out not just where the big fish are, but what types of fish they are, their daily commute, and so on.  Would it be possible to make an underwater drone that goes out and seeks big fish, maps its terrain, and collects data?  Would we be able to determine what kind of fish is being looked at?  Could it kill and collect the fish, bringing them back to a collection point?

I got to thinking from this thread:
http://www.reddit.com/r/science/comments/2lfl96/why_dont_they_harvest_those_invasive_jumping/"
"10th grader seeking guidance for building this advanced belt (has lock and alarm). will be using Arduino We have a research paper on a belt with advanced lock system. We already made the Chapter 1 for the research paper however, me and my partner are struggling on how to start with creating this device as we have limited knowledge to arduino and robotix. Heres our hypothesis of the study if this helps:    

**1.3 Hypothesis of the Study** 

1. The alarm would go off after an attempt to open it without special instructions. 

2. Pressing and holding the button for about two second would disable the alarm and the alarm would function again after closing the buckle. 

3. The alarm noise will be loud and startling enough to alert surroundings and wake up the unconscious wearer.  

We are looking for someone expert in this field that could guide us in creating this product (we are given only 2 months to work on this) 

&#x200B;

**We are asking for guidance not someone who will do this for us, guidance not cheating**"
"Advice needed on path planning robotI want to program a path planning robot using a ceiling mounted webcam/camera to look down at the environment the robot will be navigating in. If this is not achievable in the short amount of time that I have (a little less than two months) any alternatives and suggestions would be greatly appreciated. It has to do with path planning ideally and has to use a real world robot and not purely on a simulated program due to my major. (MechE) I am relatively new to robotics so a lot of the research I've down is a first for me.

&nbsp;

I came into the subreddit earlier last week in order to ask questions about the feasibility of a SLAM robot but realized that it is not doable in the time that I have been allotted. 

I'm having difficulty finding tutorials on this, specifically something that is doable in the the short amount of time that I have, any guidance to a source would be greatly appreciated. 

My initial plan was to create an environment for the robot to navigate in using planks and boxes to simulate common household furniture and then measuring each object and somehow generate the map using those dimensions. But I realized that if even one thing gets out of place either through user error or just a simple tap from the robot, the generated map will no longer be correct.

My idea going forward is to use some type of image processing in order to process the environment/room the robot will be navigating in by mounting a camera on the ceiling that will translate the created environment into a map. I am not sure how to go about doing this and also not sure if this is the  best solution to my problem. Along with this problem as well is the localization issues that I ran into last time. However now with a ceiling mounted camera I was thinking that I could perhaps put some type of marker on top of my robot and have its location known through that method.

I am also unsure if this method will be feasible since I am using an Arduino Uno R3 which has been brought to my attention, that it does not have much memory. In order to remedy this, I suggested the idea to use bluetooth/wi-fi to stream the data directly to my pc in order to combat the low memory issue.

&nbsp;


The equipment that I am using is:


A robot chassis with 4 5V DC motors.

Two H Bridge motor drivers from sparkfun (SN754410)

Two 4xAA Battery packs (one to power the arduino, another to power the Dc motors)

Arduino Uno R3

Ultrasonic Sensor (HC-SR04)

TOF sensor (VF53L1X)

&nbsp;

I am planning on using an arduino Uno R3 with either a ultrasonic sensor or a TOF sensor VF53L1X. I also have a raspberry pi 4 but the TOF sensor will not work with that. 

I was also thinking that maybe even a way to cheat the system and have a predetermined path and I could just write out the limitations of time constraints and what I had on hand in the final report and say that path planning was not achievable in the amount of time that I gave myself but this is a last resort option along with the option of using Gazebo to simulate path planning."
[P] Digit: Concrete to Countryside | Agility Robotics
"[Q] Localization methods for large open spacesHi all, I am starting work on a project that will involve  a few mobile robots driving around in a large open area (think like a gymnasium). I am wondering if there are any good methods for localizing robots in this sort of environment? I am familiar with using lidar for indoor localization but obviously in this sort of space a lidar is much less useful.

A few details about the scope of this project - money is not a huge object (within reason) and I am open to any sort of extra sensors that would be needed. External sensors or modifications to the space are within scope as well. Accuracy will be very important as I need accuracy within 1-2 cm in order for the robots to complete their tasks.

The current tentative plan is to put markers on the ground at regular intervals (i.e. 0.5-1 m grid) and use those along with encoders and IMUs but I'm skeptical that it will be accurate enough and it just feels a bit like 'cheating' to me to have to do that much manual modification to the space. 

Does anyone have recommendations for methods that might be worth considering?"
"[Q] Moving a 2 DOF robot arm end effector in a straight lineHello.

I have a 2 dof robot arm with a pencil as the end effector , using 2 ax12a servos for the joints .I solved the inverse and forward kinematics , but i struggle to draw a straight line between 2 points.

Currently i am using the 'cheat' method  (dividing the line into smaller lines) , but near the end my line takes a circular trajectory . I tried using the inverse jacobian method but i don't understand how to transform the result to commands for my servos .

Is it even possible to draw a near perfect straight line with these type of motors or i am waisting my time ? 

Sorry in advance if this post is not appropriate  for this subreddit."
Here’s An Actual 3D Indoor Map Of A Room Captured With Google’s Project Tango Phone
"Localization [Question]tl;dr: How can we localize our robot in a 20'x20' arena using a variety of different sensors?

Hey folks,

My robotics club is building an autonomous search and rescue vehicle. Our biggest challenge is the software side of things. We would like to be able to localize our robot within a 20'x20' arena of our own design. Ideally we will be able to implement a fully autonomous SLAM algorithm, but we will settle with just localization if we are unable to meet deadlines. The following information should help you understand the system we are working on and the questions that we have:

**Goals**

 * Autonomously detect and retrieve targets (red bean bags) and bring them to a 'safe zone' (2'x2' area in one of the corners of the arena)

 * Navigate an arena with 3 different obstacles: 1. target on a raised platform, 2. very simple maze, 3. rubble.

 * Be able to map and localize the arena. Given tight time constraints, we will load a map into the system and only localize.

 * Complete the system within a budget of $2000

**Physical System**

The robot uses a 6 wheeled chassis (modified from (this)[https://www.servocity.com/6wd-mantis] kit). It has a robotic arm mounted on the top of the chassis that will be used to pick up bean bags.

**Available Sensors**

 * IR and ultrasonic range sensors along the perimeter of the chassis. There are 8 pairs each facing at 0, 45, 90, 135, 180, 225, 270, and 315 degrees.

 * 9 DoF IMU placed in the center of the robot

 * Quadrature encoders on each of the six wheels

 * Current sensors on each motor (for measuring torque)

 * XBox Kinect (Version 1)

 * Raspberry Pi Camera

**System Architecture**

The robot uses a raspberry pi for simple on-board pre processing, reading sensor data, and transmitting the data to a laptop. The data will be transferred using TCP over a direct Wi-Fi link. The laptop will be running the AI algorithms and any other heavy processing such as image processing.

**Possible Solutions for Localization**

 * Since we have encoders, range sensors, and an IMU, we should be fine with relative localization, we lack a sensor that provides a raw absolute location.

 * Using SLAM with the Kinect data, we could use the features in the arena to provide absolute locations. This solution would be ideal but also is a daunting task for our software team.

 * We could use a beacon (ultrasonic, IR, RF). Very accurate beacons are expensive and we would like to avoid this if possible. We do not know how accurate and precise a beacon would need to be to properly localize us within the arena.

 * We could put patterns on the ground that the camera can recognize and calculate the distance from. This solution would make the arena look too much like a laboratory environment rather than our goal of a natural disaster scene. We will use this option if it is absolutely necessary, but I believe we can do better.

 * We cheat and don't localize at all. Instead, our algorithm would have the robot randomly explore the arena, pick up targets, and when it happens to be on the safe zone, drop the targets off. I am a huge fan of using simple solutions to solve complex tasks (such as Braitenberg vehicles) but I fear a simple solution may not teach the members enough about these complex topics and the system may not work as well as intended.

**Questions**

 1. How accurate do you think our localization needs to be? We have been using the ballpark estimate of 3 inches, but in all reality, is that kind of accuracy even needed? Or am I completely wrong and we need to be within a tenth of an inch?

 2. Are there any other localization methods that I missed that are affordable and realistic?

 3. If we were to use the beacon approach, the plan would be to use a sensor fusion algorithm to fuse the relative and absolute measurements. How would we go about calculating how accurate the beacon needs to be to produce an accuracy of +/- X distance?

 4. Are we missing anything else?

Thank you all very much for your help! I look forward to reading the replies. We will be sure to share the solution we implement to help others who may be working on similar projects."
"Moving at a constant speed between two point in a straight line by controlling two angles.I am trying to program a robot arm to move between any two points in a straight line.

The robot arm is an eezybotarm MK2: http://www.thingiverse.com/thing:1454048

It is driven by two servos, and I've figured out how to find its movement limits and how to convert its location based on the servo angles to cartesian, but the math to get it to move in a straight line is perplexing me.

The only way I can think of is to cheat, and break each movement into smaller curved paths to approximate straight.

I've done some googling, but I'm not even sure how to describe what I'm looking for.  Does anyone have a formula based on the servo angles and arm lengths that derives a straight path.  Better yet, does anyone have a link to an article that might teach the math, as I hope to convert it to 3 dimensions once I understand it."
"Get Ready to Teach Robotics Remotely in Fall 2020&#x200B;

https://preview.redd.it/ui28byzyd6c51.jpg?width=2446&format=pjpg&auto=webp&v=enabled&s=b253bae872cd91243ffc93b43c8ba6747ee2aa27

Webinar: Get Ready to Teach Robotics Remotely in Fall 2020

06/08/2020 at 7 pm CEST

Registration information: [https://www.theconstructsim.com/teach-robotics-remotely-webinar/](https://www.theconstructsim.com/teach-robotics-remotely-webinar/)

\[About this webinar\]

This is a free online webinar designed to inspire and educate robotics teachers and lecturers about how to teach robotics remotely during COVID-19. Dr. Ricardo Tellez is going to share his real remote robotics teaching experience at La Salle University.

\[Who should attend\]- Professors/Teachers who are preparing the robotics courses for 2020 FALL.- Teachers who are finding a remotely teaching solution during COVID-19

\[Content outline\]- Problems teaching robotics in times of deep uncertainty.- How to get all the lectures to deliver and an introduction to robotics.- How to set up a practice environment for remote robotics students.- How to provide robotics practices to students remotely with simulated robots.- How to provide robotics practices to students remotely with real robots.- How to evaluate students (exams) and prevent cheating.

\[Robots used in this webinar\]- Simulated RB-KAIROS (made by Robotnik)- Real robot: JetBot

\[Outcomes\]- Keep students learning robotics without geographical restrictions and maximize their learning outcomes through real practice.- Move away from a slides-based teaching method to a notebook-based one, where direct interaction with robots is embedded in the method itself."
"Interviewed with Willow Garage anyone?Hello All,
I am curious if anyone on Reddit has interviewed with Willow Garage in the past or is a current employee. Being a robotics masters, that is a top notch company to work for. I have an on site interview coming up with them. I do not want to cheat off the interview questions. I am just wondering what kind of questions they ask in the interview so that I am appropriately prepared.
Thanks 
"
"Are there any research robots which navigate by vision only? Are there any research robots which navigate by vision only?   I don't mean kinects. I don't mean laser range finders.  I don't mean cheating by using GPS. I don't mean LIDAR + SLAM algos. I mean pure vision and only vision. 

Could this be considered an *unsolved engineering problem* in robotics and Artificial Intelligence?
"
"Please help my high school get a $10,000 grant for a robotics program!Recently we have entered a local news group competition to win a $10,000 grant. We are looking to get as many votes as possible so we can get a robotics program. It would mean so much if you guys helped us out! There is one thing though, NO CHEATING! There has already been a school that has been disqualified for using a program to get illegal votes, so no programs!!You are limited to one vote per day so vote everyday! If you want to know what we would be doing exactly with the grant, it is all on our voting page. 

[Vote Here!](http://nbc4i.upickem.net/engine/Details.aspx?p=V&c=65509&s=29964754&i=1&sort=#SD)"
"Low hanging fruit? What are some ""easy"" profitable projects possible? Widen my mind.What are some robotics projects I could probably work on that could turn around and make money?  I just got a position where they'll let me work with robotics in my spare time

I'm thinking I want to make a robot that will direct mirrors to a focal point where I'll have a steam engine that creates electricity.  It is solar without the panels.

I was thinking I could do gardening robots, but they're incredibly complicated in scope.  I was thinking a robot that robotills, plants, waters, and then harvests and puts in a bin would be an optimal idea for a robot, but sooooo much goes on in that.

I was thinking of simplifying the gardening robot to just make a harvester robot.  The harvester robot would use image recognition to identify a fruit or vegetable, then it'd engage in a harvesting action to collect it.  This harvester robot doesn't have to be 100% effective to be good.  All it'd need to do is get pretty good.  You send the harvester robots out the first day.  Then you send the hired hands out the next days.  You'd have less need for hired hands because the robots did ""some"".

I was thinking it'd be very simple to make military robots of all sorts, but I don't think I want involved in that at this juncture.  I feel if military robots got too cheap, a billionaire might just buy an army and conquer a country by force.  Robots have perfect loyalty.

I was thinking robot toys.  The robots could even cheat by telling each other where they are.  Then they'd be fencing each other or playing soccer like CMU's crew.

I was thinking a robot surveillance bot.  It'd autonomously patrol.  Or you could waldo it and project your voice into the robot.  Not sure how legal it'd be to drive it on the sidewalk and collect for charities, run a vending machine, or social engineer a dating site.  I do know some states legalized segways, so maybe you could mount your robot on a segway and it'd be legal.  

The robot surveilance bot idea came from simply providing a security mechanism so your lawnmower bot doesn't get stolen.  If someone comes in range, it'd take pictures, and have GPS to recover your bot.

So what ideas do you guys have to quickly monetize some low hanging robot fruit?  Robotics is gonna be ramping up here soon with these cheap cell phones and Arduino."
"My speech controlled quadropod (wannabe robot puppy)Hey fellow roboticists!

Spyro Spoontail (aka Spyro Silver Eyes) is a robot that I had built in 2014. Why am I posting this now? Well I only recently discovered Reddit and this subreddit.
Anyway to my robot:
https://youtu.be/i4cHfuI5onw
I had built the physical structure long ago using a Dago quadropod kit (yes I know that's cheating, but I was in high school) and a spider microcontroller (which I'm certain you won't find anymore), but you can use an Arduino mega instead.

I had implemented the speech control only recently, using BitVoicer. So I thought, okay this is cool, let me share my experience with others. I did that on this instructable: http://www.instructables.com/id/Speech-Controlled-Quadropod/
For you guys starting out on a quadropod or trying to implement speech recogbition, I think it's a good start. 
However, I would greatly appreciate your thoughts and strong criticism on the robot, because well I haven't worked on it since, and am planning to build a bipedal robot soon while keeping many features (like speech recognition). So some critical suggestions would be helpful.

Also, if you guys aren't part of the letsmakerobots.com community, you should definitely give it a go. There a lot of cool and professional robots/roboticits there.

Cheers!"
🤖 An autonomous two-wheeled robot developed by startup Ascento has been performing guarding tasks for security firm Securitas AG in Switzerland
Korean Startup plans to roll out 400 lidar-free delivery and security robots by year-end
"Open Class - Security in ROS2&#x200B;

ROS2 has been released with security tools allowing it to secure the robotics systems by restricting access. They are disabled by default, but they can be enabled to make robots safer.

&#x200B;

https://preview.redd.it/vjs13k55asxa1.jpg?width=1280&format=pjpg&auto=webp&v=enabled&s=aa7a8c8d00501e8e93584101d69cabe15ec195a7

In this open class, we will review how you can enable security for a ROS2 node.

You’ll learn:

* Basic security concepts: Authentication, Cryptography, Access Control, etc.
* How to enable security for a ROS2 node

You will be using **TurtleBot3** throughout the training

**Join Open Class Here** [**https://app.theconstructsim.com/LiveClass/3046f8cf-7219-4fe2-b6a1-0674edb4dce7**](https://app.theconstructsim.com/LiveClass/3046f8cf-7219-4fe2-b6a1-0674edb4dce7)

  
Tuesday, May 9, 6 PM CEST"
I've noticed robotic security companies appear to be signing more contracts recently. Do you guys think this is simply a publicity stunt or are they really deterring potential security threats? I'm curious to know more about this industry if anyone is more aware.
Casino security at the M resort in Las Vegas
Store data within your robotics team's region for added privacy and security
"I just got back my childhood Omnibot, I’m hoping y’all can help me decide to restore it to original or keep the outside original and go ham on internal upgrades.I have a ton of nostalgic and core memory attachment to this guy. I’m already leaning towards gutting the insides but I worry this may be too rare for that kind of a job. I see there’s a few for sell on eBay currently for parts and some fully refurbished so it’s leading me too believe there may be enough out there that I wouldn’t be committing a crime gutting it but I wanted to check with y’all and see. Also if anybody has any relatively simple ideas I can use for internal upgrades I’d love to hear them. I like so far using a Wi-Fi roomba for the base, converting the pop out cassette to a pop out Alexa show(or similar), changing the eyes to a honeycomb style so I can display images, install mini projector, updated battery. I have not opened him up yet but I understand with them that battery  corrosion can leak on the circuit boards so I’m guessing internally I’d have to replace some major components to refurbish it to original. Thanks for your guidance and ideas, I’m really excited for this journey in either direction that I go."
"I was Trying to install Opencv with Cuda in my TX2 NX- but facing this issue....., also i tried to install it through Nvidia SDK but got multimedia error.nvidia@tegra-ubuntu:\~/opencv/build$ cmake -D CMAKE\_BUILD\_TYPE=RELEASE -D CMAKE\_INSTALL\_PREFIX=/usr/local -D WITH\_CUDA=ON -D CUDA\_ARCH\_BIN=""6.2"" -D CUDA\_ARCH\_PTX="""" -D OPENCV\_EXTRA\_MODULES\_PATH=../../opencv\_contrib/modules -D WITH\_GSTREAMER=ON -D WITH\_LIBV4L=ON -D WITH\_FFMPEG=ON -D WITH\_GTK=ON -D WITH\_V4L=ON -D WITH\_TBB=ON -D WITH\_OPENMP=ON -D ENABLE\_FAST\_MATH=1 -D CUDA\_FAST\_MATH=1 -D ENABLE\_NEON=ON -D ENABLE\_VFPV3=ON -D WITH\_QT=OFF -D BUILD\_opencv\_python3=ON -D BUILD\_TESTS=OFF -D BUILD\_PERF\_TESTS=OFF -D BUILD\_EXAMPLES=OFF -D CMAKE\_CXX\_FLAGS=""-std=c++14 -O3 -Wno-deprecated-declarations -Wno-unused-result"" -D OPENCV\_DNN\_CUDA=ON -D ENABLE\_PRECOMPILED\_HEADERS=OFF -D CUDA\_TOOLKIT\_ROOT\_DIR=/usr/local/cuda-10.2 ..

\-- The CXX compiler identification is GNU 7.5.0

\-- The C compiler identification is GNU 7.5.0

\-- Check for working CXX compiler: /usr/bin/c++

\-- Check for working CXX compiler: /usr/bin/c++ -- works

\-- Detecting CXX compiler ABI info

\-- Detecting CXX compiler ABI info - done

\-- Detecting CXX compile features

\-- Detecting CXX compile features - done

\-- Check for working C compiler: /usr/bin/cc

\-- Check for working C compiler: /usr/bin/cc -- works

\-- Detecting C compiler ABI info

\-- Detecting C compiler ABI info - done

\-- Detecting C compile features

\-- Detecting C compile features - done

\-- Detected processor: aarch64

\-- Found PythonInterp: /usr/bin/python2.7 (found suitable version ""2.7.17"", minimum required is ""2.7"") 

\-- Found PythonLibs: /usr/lib/aarch64-linux-gnu/libpython2.7.so (found suitable exact version ""2.7.17"") 

\-- Found PythonInterp: /usr/bin/python3 (found suitable version ""3.8"", minimum required is ""3.2"") 

\-- Found PythonLibs: /usr/lib/aarch64-linux-gnu/libpython3.8.so (found suitable exact version ""3.8.0"") 

<string>:1: DeprecationWarning: 

&#x200B;

  \`numpy.distutils\` is deprecated since NumPy 1.23.0, as a result

  of the deprecation of \`distutils\` itself. It will be removed for

  Python >= 3.12. For older Python versions it will remain present.

  It is recommended to use \`setuptools < 60.0\` for those Python versions.

  For more details, see:

[https://numpy.org/devdocs/reference/distutils\_status\_migration.html](https://numpy.org/devdocs/reference/distutils_status_migration.html) 

&#x200B;

&#x200B;

\-- Looking for ccache - not found

\-- Performing Test HAVE\_CXX\_FSIGNED\_CHAR

\-- Performing Test HAVE\_CXX\_FSIGNED\_CHAR - Success

\-- Performing Test HAVE\_C\_FSIGNED\_CHAR

\-- Performing Test HAVE\_C\_FSIGNED\_CHAR - Success

\-- Performing Test HAVE\_CXX\_FFAST\_MATH

\-- Performing Test HAVE\_CXX\_FFAST\_MATH - Success

\-- Performing Test HAVE\_C\_FFAST\_MATH

\-- Performing Test HAVE\_C\_FFAST\_MATH - Success

\-- Performing Test HAVE\_CXX\_W

\-- Performing Test HAVE\_CXX\_W - Success

\-- Performing Test HAVE\_C\_W

\-- Performing Test HAVE\_C\_W - Success

\-- Performing Test HAVE\_CXX\_WALL

\-- Performing Test HAVE\_CXX\_WALL - Success

\-- Performing Test HAVE\_C\_WALL

\-- Performing Test HAVE\_C\_WALL - Success

\-- Performing Test HAVE\_CXX\_WRETURN\_TYPE

\-- Performing Test HAVE\_CXX\_WRETURN\_TYPE - Success

\-- Performing Test HAVE\_C\_WRETURN\_TYPE

\-- Performing Test HAVE\_C\_WRETURN\_TYPE - Success

\-- Performing Test HAVE\_CXX\_WNON\_VIRTUAL\_DTOR

\-- Performing Test HAVE\_CXX\_WNON\_VIRTUAL\_DTOR - Success

\-- Performing Test HAVE\_C\_WNON\_VIRTUAL\_DTOR

\-- Performing Test HAVE\_C\_WNON\_VIRTUAL\_DTOR - Failed

\-- Performing Test HAVE\_CXX\_WADDRESS

\-- Performing Test HAVE\_CXX\_WADDRESS - Success

\-- Performing Test HAVE\_C\_WADDRESS

\-- Performing Test HAVE\_C\_WADDRESS - Success

\-- Performing Test HAVE\_CXX\_WSEQUENCE\_POINT

\-- Performing Test HAVE\_CXX\_WSEQUENCE\_POINT - Success

\-- Performing Test HAVE\_C\_WSEQUENCE\_POINT

\-- Performing Test HAVE\_C\_WSEQUENCE\_POINT - Success

\-- Performing Test HAVE\_CXX\_WFORMAT

\-- Performing Test HAVE\_CXX\_WFORMAT - Success

\-- Performing Test HAVE\_C\_WFORMAT

\-- Performing Test HAVE\_C\_WFORMAT - Success

\-- Performing Test HAVE\_CXX\_WFORMAT\_SECURITY

\-- Performing Test HAVE\_CXX\_WFORMAT\_SECURITY - Success

\-- Performing Test HAVE\_C\_WFORMAT\_SECURITY

\-- Performing Test HAVE\_C\_WFORMAT\_SECURITY - Success

\-- Performing Test HAVE\_CXX\_WMISSING\_DECLARATIONS

\-- Performing Test HAVE\_CXX\_WMISSING\_DECLARATIONS - Success

\-- Performing Test HAVE\_C\_WMISSING\_DECLARATIONS

\-- Performing Test HAVE\_C\_WMISSING\_DECLARATIONS - Success

\-- Performing Test HAVE\_CXX\_WMISSING\_PROTOTYPES

\-- Performing Test HAVE\_CXX\_WMISSING\_PROTOTYPES - Failed

\-- Performing Test HAVE\_C\_WMISSING\_PROTOTYPES

\-- Performing Test HAVE\_C\_WMISSING\_PROTOTYPES - Success

\-- Performing Test HAVE\_CXX\_WSTRICT\_PROTOTYPES

\-- Performing Test HAVE\_CXX\_WSTRICT\_PROTOTYPES - Failed

\-- Performing Test HAVE\_C\_WSTRICT\_PROTOTYPES

\-- Performing Test HAVE\_C\_WSTRICT\_PROTOTYPES - Success

\-- Performing Test HAVE\_CXX\_WUNDEF

\-- Performing Test HAVE\_CXX\_WUNDEF - Success

\-- Performing Test HAVE\_C\_WUNDEF

\-- Performing Test HAVE\_C\_WUNDEF - Success

\-- Performing Test HAVE\_CXX\_WINIT\_SELF

\-- Performing Test HAVE\_CXX\_WINIT\_SELF - Success

\-- Performing Test HAVE\_C\_WINIT\_SELF

\-- Performing Test HAVE\_C\_WINIT\_SELF - Success

\-- Performing Test HAVE\_CXX\_WPOINTER\_ARITH

\-- Performing Test HAVE\_CXX\_WPOINTER\_ARITH - Success

\-- Performing Test HAVE\_C\_WPOINTER\_ARITH

\-- Performing Test HAVE\_C\_WPOINTER\_ARITH - Success

\-- Performing Test HAVE\_CXX\_WSHADOW

\-- Performing Test HAVE\_CXX\_WSHADOW - Success

\-- Performing Test HAVE\_C\_WSHADOW

\-- Performing Test HAVE\_C\_WSHADOW - Success

\-- Performing Test HAVE\_CXX\_WSIGN\_PROMO

\-- Performing Test HAVE\_CXX\_WSIGN\_PROMO - Success

\-- Performing Test HAVE\_C\_WSIGN\_PROMO

\-- Performing Test HAVE\_C\_WSIGN\_PROMO - Failed

\-- Performing Test HAVE\_CXX\_WUNINITIALIZED

\-- Performing Test HAVE\_CXX\_WUNINITIALIZED - Success

\-- Performing Test HAVE\_C\_WUNINITIALIZED

\-- Performing Test HAVE\_C\_WUNINITIALIZED - Success

\-- Performing Test HAVE\_CXX\_WSUGGEST\_OVERRIDE

\-- Performing Test HAVE\_CXX\_WSUGGEST\_OVERRIDE - Success

\-- Performing Test HAVE\_C\_WSUGGEST\_OVERRIDE

\-- Performing Test HAVE\_C\_WSUGGEST\_OVERRIDE - Failed

\-- Performing Test HAVE\_CXX\_WNO\_DELETE\_NON\_VIRTUAL\_DTOR

\-- Performing Test HAVE\_CXX\_WNO\_DELETE\_NON\_VIRTUAL\_DTOR - Success

\-- Performing Test HAVE\_C\_WNO\_DELETE\_NON\_VIRTUAL\_DTOR

\-- Performing Test HAVE\_C\_WNO\_DELETE\_NON\_VIRTUAL\_DTOR - Failed

\-- Performing Test HAVE\_CXX\_WNO\_UNNAMED\_TYPE\_TEMPLATE\_ARGS

\-- Performing Test HAVE\_CXX\_WNO\_UNNAMED\_TYPE\_TEMPLATE\_ARGS - Failed

\-- Performing Test HAVE\_C\_WNO\_UNNAMED\_TYPE\_TEMPLATE\_ARGS

\-- Performing Test HAVE\_C\_WNO\_UNNAMED\_TYPE\_TEMPLATE\_ARGS - Failed

\-- Performing Test HAVE\_CXX\_WNO\_COMMENT

\-- Performing Test HAVE\_CXX\_WNO\_COMMENT - Success

\-- Performing Test HAVE\_C\_WNO\_COMMENT

\-- Performing Test HAVE\_C\_WNO\_COMMENT - Success

\-- Performing Test HAVE\_CXX\_WIMPLICIT\_FALLTHROUGH\_3

\-- Performing Test HAVE\_CXX\_WIMPLICIT\_FALLTHROUGH\_3 - Success

\-- Performing Test HAVE\_C\_WIMPLICIT\_FALLTHROUGH\_3

\-- Performing Test HAVE\_C\_WIMPLICIT\_FALLTHROUGH\_3 - Success

\-- Performing Test HAVE\_CXX\_WNO\_STRICT\_OVERFLOW

\-- Performing Test HAVE\_CXX\_WNO\_STRICT\_OVERFLOW - Success

\-- Performing Test HAVE\_C\_WNO\_STRICT\_OVERFLOW

\-- Performing Test HAVE\_C\_WNO\_STRICT\_OVERFLOW - Success

\-- Performing Test HAVE\_CXX\_FDIAGNOSTICS\_SHOW\_OPTION

\-- Performing Test HAVE\_CXX\_FDIAGNOSTICS\_SHOW\_OPTION - Success

\-- Performing Test HAVE\_C\_FDIAGNOSTICS\_SHOW\_OPTION

\-- Performing Test HAVE\_C\_FDIAGNOSTICS\_SHOW\_OPTION - Success

\-- Performing Test HAVE\_CXX\_PTHREAD

\-- Performing Test HAVE\_CXX\_PTHREAD - Success

\-- Performing Test HAVE\_C\_PTHREAD

\-- Performing Test HAVE\_C\_PTHREAD - Success

\-- Performing Test HAVE\_CXX\_FOMIT\_FRAME\_POINTER

\-- Performing Test HAVE\_CXX\_FOMIT\_FRAME\_POINTER - Success

\-- Performing Test HAVE\_C\_FOMIT\_FRAME\_POINTER

\-- Performing Test HAVE\_C\_FOMIT\_FRAME\_POINTER - Success

\-- Performing Test HAVE\_CXX\_FFUNCTION\_SECTIONS

\-- Performing Test HAVE\_CXX\_FFUNCTION\_SECTIONS - Success

\-- Performing Test HAVE\_C\_FFUNCTION\_SECTIONS

\-- Performing Test HAVE\_C\_FFUNCTION\_SECTIONS - Success

\-- Performing Test HAVE\_CXX\_FDATA\_SECTIONS

\-- Performing Test HAVE\_CXX\_FDATA\_SECTIONS - Success

\-- Performing Test HAVE\_C\_FDATA\_SECTIONS

\-- Performing Test HAVE\_C\_FDATA\_SECTIONS - Success

\-- Performing Test HAVE\_CPU\_NEON\_SUPPORT (check file: cmake/checks/cpu\_neon.cpp)

\-- Performing Test HAVE\_CPU\_NEON\_SUPPORT - Success

\-- Performing Test HAVE\_CPU\_FP16\_SUPPORT (check file: cmake/checks/cpu\_fp16.cpp)

\-- Performing Test HAVE\_CPU\_FP16\_SUPPORT - Success

CMake Error at cmake/OpenCVCompilerOptimizations.cmake:564 (message):

  Required baseline optimization is not supported: VFPV3

  (CPU\_BASELINE\_REQUIRE=;VFPV3;NEON)

Call Stack (most recent call first):

  cmake/OpenCVCompilerOptions.cmake:324 (include)

  CMakeLists.txt:650 (include)

&#x200B;

&#x200B;

\-- Performing Test HAVE\_CPU\_BASELINE\_FLAGS

\-- Performing Test HAVE\_CPU\_BASELINE\_FLAGS - Success

\-- Performing Test HAVE\_CXX\_FVISIBILITY\_HIDDEN

\-- Performing Test HAVE\_CXX\_FVISIBILITY\_HIDDEN - Success

\-- Performing Test HAVE\_C\_FVISIBILITY\_HIDDEN

\-- Performing Test HAVE\_C\_FVISIBILITY\_HIDDEN - Success

\-- Performing Test HAVE\_CXX\_FVISIBILITY\_INLINES\_HIDDEN

\-- Performing Test HAVE\_CXX\_FVISIBILITY\_INLINES\_HIDDEN - Success

\-- Performing Test HAVE\_C\_FVISIBILITY\_INLINES\_HIDDEN

\-- Performing Test HAVE\_C\_FVISIBILITY\_INLINES\_HIDDEN - Failed

\-- Performing Test HAVE\_LINK\_AS\_NEEDED

\-- Performing Test HAVE\_LINK\_AS\_NEEDED - Success

\-- Performing Test HAVE\_LINK\_NO\_UNDEFINED

\-- Performing Test HAVE\_LINK\_NO\_UNDEFINED - Success

\-- Looking for pthread.h

\-- Looking for pthread.h - found

\-- Looking for posix\_memalign

\-- Looking for posix\_memalign - found

\-- Looking for malloc.h

\-- Looking for malloc.h - found

\-- Looking for memalign

\-- Looking for memalign - found

\-- Check if the system is big endian

\-- Searching 16 bit integer

\-- Looking for sys/types.h

\-- Looking for sys/types.h - found

\-- Looking for stdint.h

\-- Looking for stdint.h - found

\-- Looking for stddef.h

\-- Looking for stddef.h - found

\-- Check size of unsigned short

\-- Check size of unsigned short - done

\-- Using unsigned short

\-- Check if the system is big endian - little endian

\-- Found OpenMP\_C: -fopenmp (found version ""4.5"") 

\-- Found OpenMP\_CXX: -fopenmp (found version ""4.5"") 

\-- Found OpenMP: TRUE (found version ""4.5"")  

\-- Found ZLIB: /usr/lib/aarch64-linux-gnu/libz.so (found suitable version ""1.2.11"", minimum required is ""1.2.3"") 

\-- Found JPEG: /usr/lib/aarch64-linux-gnu/libjpeg.so  

\-- Found TIFF: /usr/lib/aarch64-linux-gnu/libtiff.so (found version ""4.0.9"") 

\-- Performing Test HAVE\_C\_STD\_C99

\-- Performing Test HAVE\_C\_STD\_C99 - Success

\-- Performing Test HAVE\_C\_WNO\_UNUSED\_VARIABLE

\-- Performing Test HAVE\_C\_WNO\_UNUSED\_VARIABLE - Success

\-- Performing Test HAVE\_C\_WNO\_UNUSED\_FUNCTION

\-- Performing Test HAVE\_C\_WNO\_UNUSED\_FUNCTION - Success

\-- Performing Test HAVE\_C\_WNO\_SHADOW

\-- Performing Test HAVE\_C\_WNO\_SHADOW - Success

\-- Performing Test HAVE\_C\_WNO\_MAYBE\_UNINITIALIZED

\-- Performing Test HAVE\_C\_WNO\_MAYBE\_UNINITIALIZED - Success

\-- Performing Test HAVE\_C\_WNO\_MISSING\_PROTOTYPES

\-- Performing Test HAVE\_C\_WNO\_MISSING\_PROTOTYPES - Success

\-- Performing Test HAVE\_C\_WNO\_MISSING\_DECLARATIONS

\-- Performing Test HAVE\_C\_WNO\_MISSING\_DECLARATIONS - Success

\-- Performing Test HAVE\_C\_WNO\_IMPLICIT\_FALLTHROUGH

\-- Performing Test HAVE\_C\_WNO\_IMPLICIT\_FALLTHROUGH - Success

\-- Performing Test HAVE\_C\_WNO\_UNUSED\_BUT\_SET\_VARIABLE

\-- Performing Test HAVE\_C\_WNO\_UNUSED\_BUT\_SET\_VARIABLE - Success

\-- Could NOT find OpenJPEG (minimal suitable version: 2.0, recommended version >= 2.3.1). OpenJPEG will be built from sources

\-- Performing Test HAVE\_C\_WNO\_IMPLICIT\_CONST\_INT\_FLOAT\_CONVERSION

\-- Performing Test HAVE\_C\_WNO\_IMPLICIT\_CONST\_INT\_FLOAT\_CONVERSION - Failed

\-- OpenJPEG: VERSION = 2.4.0, BUILD = opencv-4.7.0-dev-openjp2-2.4.0

\-- Check if the system is big endian

\-- Searching 16 bit integer

\-- Using unsigned short

\-- Check if the system is big endian - little endian

\-- Looking for string.h

\-- Looking for string.h - found

\-- Looking for memory.h

\-- Looking for memory.h - found

\-- Looking for stdlib.h

\-- Looking for stdlib.h - found

\-- Looking for stdio.h

\-- Looking for stdio.h - found

\-- Looking for math.h

\-- Looking for math.h - found

\-- Looking for float.h

\-- Looking for float.h - found

\-- Looking for time.h

\-- Looking for time.h - found

\-- Looking for stdarg.h

\-- Looking for stdarg.h - found

\-- Looking for ctype.h

\-- Looking for ctype.h - found

\-- Looking for assert.h

\-- Looking for assert.h - found

\-- Looking for stdint.h

\-- Looking for stdint.h - found

\-- Looking for inttypes.h

\-- Looking for inttypes.h - found

\-- Looking for strings.h

\-- Looking for strings.h - found

\-- Looking for sys/stat.h

\-- Looking for sys/stat.h - found

\-- Looking for unistd.h

\-- Looking for unistd.h - found

\-- Looking for include file malloc.h

\-- Looking for include file malloc.h - found

\-- Looking for \_aligned\_malloc

\-- Looking for \_aligned\_malloc - not found

\-- Looking for posix\_memalign

\-- Looking for posix\_memalign - found

\-- Looking for memalign

\-- Looking for memalign - found

\-- Performing Test HAVE\_C\_WNO\_UNDEF

\-- Performing Test HAVE\_C\_WNO\_UNDEF - Success

\-- Performing Test HAVE\_C\_WNO\_STRICT\_PROTOTYPES

\-- Performing Test HAVE\_C\_WNO\_STRICT\_PROTOTYPES - Success

\-- Performing Test HAVE\_C\_WNO\_CAST\_FUNCTION\_TYPE

\-- Performing Test HAVE\_C\_WNO\_CAST\_FUNCTION\_TYPE - Failed

\-- OpenJPEG libraries will be built from sources: libopenjp2 (version ""2.4.0"")

\-- Found ZLIB: /usr/lib/aarch64-linux-gnu/libz.so (found version ""1.2.11"") 

\-- Found PNG: /usr/lib/aarch64-linux-gnu/libpng.so (found version ""1.6.34"") 

\-- Looking for /usr/include/libpng/png.h

\-- Looking for /usr/include/libpng/png.h - found

\-- Looking for semaphore.h

\-- Looking for semaphore.h - found

\-- Performing Test HAVE\_CXX\_WNO\_SHADOW

\-- Performing Test HAVE\_CXX\_WNO\_SHADOW - Success

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED - Success

\-- Performing Test HAVE\_CXX\_WNO\_SIGN\_COMPARE

\-- Performing Test HAVE\_CXX\_WNO\_SIGN\_COMPARE - Success

\-- Performing Test HAVE\_CXX\_WNO\_UNDEF

\-- Performing Test HAVE\_CXX\_WNO\_UNDEF - Success

\-- Performing Test HAVE\_CXX\_WNO\_MISSING\_DECLARATIONS

\-- Performing Test HAVE\_CXX\_WNO\_MISSING\_DECLARATIONS - Success

\-- Performing Test HAVE\_CXX\_WNO\_UNINITIALIZED

\-- Performing Test HAVE\_CXX\_WNO\_UNINITIALIZED - Success

\-- Performing Test HAVE\_CXX\_WNO\_SWITCH

\-- Performing Test HAVE\_CXX\_WNO\_SWITCH - Success

\-- Performing Test HAVE\_CXX\_WNO\_PARENTHESES

\-- Performing Test HAVE\_CXX\_WNO\_PARENTHESES - Success

\-- Performing Test HAVE\_CXX\_WNO\_ARRAY\_BOUNDS

\-- Performing Test HAVE\_CXX\_WNO\_ARRAY\_BOUNDS - Success

\-- Performing Test HAVE\_CXX\_WNO\_EXTRA

\-- Performing Test HAVE\_CXX\_WNO\_EXTRA - Success

\-- Performing Test HAVE\_CXX\_WNO\_DEPRECATED\_DECLARATIONS

\-- Performing Test HAVE\_CXX\_WNO\_DEPRECATED\_DECLARATIONS - Success

\-- Performing Test HAVE\_CXX\_WNO\_MISLEADING\_INDENTATION

\-- Performing Test HAVE\_CXX\_WNO\_MISLEADING\_INDENTATION - Success

\-- Performing Test HAVE\_CXX\_WNO\_DEPRECATED

\-- Performing Test HAVE\_CXX\_WNO\_DEPRECATED - Success

\-- Performing Test HAVE\_CXX\_WNO\_SUGGEST\_OVERRIDE

\-- Performing Test HAVE\_CXX\_WNO\_SUGGEST\_OVERRIDE - Success

\-- Performing Test HAVE\_CXX\_WNO\_INCONSISTENT\_MISSING\_OVERRIDE

\-- Performing Test HAVE\_CXX\_WNO\_INCONSISTENT\_MISSING\_OVERRIDE - Failed

\-- Performing Test HAVE\_CXX\_WNO\_IMPLICIT\_FALLTHROUGH

\-- Performing Test HAVE\_CXX\_WNO\_IMPLICIT\_FALLTHROUGH - Success

\-- Performing Test HAVE\_CXX\_WNO\_TAUTOLOGICAL\_COMPARE

\-- Performing Test HAVE\_CXX\_WNO\_TAUTOLOGICAL\_COMPARE - Success

\-- Performing Test HAVE\_CXX\_WNO\_MISSING\_PROTOTYPES

\-- Performing Test HAVE\_CXX\_WNO\_MISSING\_PROTOTYPES - Failed

\-- Performing Test HAVE\_CXX\_WNO\_REORDER

\-- Performing Test HAVE\_CXX\_WNO\_REORDER - Success

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_RESULT

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_RESULT - Success

\-- Performing Test HAVE\_CXX\_WNO\_IMPLICIT\_CONST\_INT\_FLOAT\_CONVERSION

\-- Performing Test HAVE\_CXX\_WNO\_IMPLICIT\_CONST\_INT\_FLOAT\_CONVERSION - Failed

\-- Found TBB (env): /usr/lib/aarch64-linux-gnu/libtbb.so

\-- Found Threads: TRUE  

\-- Found CUDNN: /usr/lib/aarch64-linux-gnu/libcudnn.so (found suitable version ""8.2.1"", minimum required is ""7.5"") 

\-- CUDA detected: 10.2

\-- CUDA: Using CUDA\_ARCH\_BIN=6.2

\-- CUDA NVCC target flags: -gencode;arch=compute\_62,code=sm\_62;-D\_FORCE\_INLINES

\-- Could not find OpenBLAS include. Turning OpenBLAS\_FOUND off

\-- Could not find OpenBLAS lib. Turning OpenBLAS\_FOUND off

\-- Found Atlas: /usr/include/aarch64-linux-gnu  

\-- Found Atlas (include: /usr/include/aarch64-linux-gnu, library: /usr/lib/aarch64-linux-gnu/libatlas.so)

\-- LAPACK(Atlas): LAPACK\_LIBRARIES: /usr/lib/aarch64-linux-gnu/liblapack.so;/usr/lib/aarch64-linux-gnu/libcblas.so;/usr/lib/aarch64-linux-gnu/libatlas.so

\-- LAPACK(Atlas): Support is enabled.

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_PARAMETER

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_PARAMETER - Success

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_LOCAL\_TYPEDEFS

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_LOCAL\_TYPEDEFS - Success

\-- Performing Test HAVE\_CXX\_WNO\_SIGN\_PROMO

\-- Performing Test HAVE\_CXX\_WNO\_SIGN\_PROMO - Success

\-- Performing Test HAVE\_CXX\_WNO\_TAUTOLOGICAL\_UNDEFINED\_COMPARE

\-- Performing Test HAVE\_CXX\_WNO\_TAUTOLOGICAL\_UNDEFINED\_COMPARE - Failed

\-- Performing Test HAVE\_CXX\_WNO\_IGNORED\_QUALIFIERS

\-- Performing Test HAVE\_CXX\_WNO\_IGNORED\_QUALIFIERS - Success

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_FUNCTION

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_FUNCTION - Success

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_CONST\_VARIABLE

\-- Performing Test HAVE\_CXX\_WNO\_UNUSED\_CONST\_VARIABLE - Success

\-- Performing Test HAVE\_CXX\_WNO\_SHORTEN\_64\_TO\_32

\-- Performing Test HAVE\_CXX\_WNO\_SHORTEN\_64\_TO\_32 - Failed

\-- Performing Test HAVE\_CXX\_WNO\_INVALID\_OFFSETOF

\-- Performing Test HAVE\_CXX\_WNO\_INVALID\_OFFSETOF - Success

\-- Performing Test HAVE\_CXX\_WNO\_ENUM\_COMPARE\_SWITCH

\-- Performing Test HAVE\_CXX\_WNO\_ENUM\_COMPARE\_SWITCH - Failed

\-- Could NOT find JNI (missing: JAVA\_AWT\_LIBRARY JAVA\_JVM\_LIBRARY JAVA\_INCLUDE\_PATH JAVA\_INCLUDE\_PATH2 JAVA\_AWT\_INCLUDE\_PATH) 

\-- VTK is not found. Please set -DVTK\_DIR in CMake to VTK build directory, or to VTK install subdirectory with VTKConfig.cmake file

\-- Looking for dlerror in dl

\-- Looking for dlerror in dl - found

\-- Performing Test HAVE\_C\_WNO\_SIGN\_COMPARE

\-- Performing Test HAVE\_C\_WNO\_SIGN\_COMPARE - Success

CMake Error at cmake/OpenCVModule.cmake:274 (message):

  No modules has been found: /home/nvidia/opencv\_contrib/modules

Call Stack (most recent call first):

  cmake/OpenCVModule.cmake:356 (\_glob\_locations)

  cmake/OpenCVModule.cmake:408 (ocv\_glob\_modules)

  CMakeLists.txt:956 (ocv\_register\_modules)

&#x200B;

&#x200B;

\-- ADE: Downloading [v0.1.2a.zip](https://v0.1.2a.zip) from [https://github.com/opencv/ade/archive/v0.1.2a.zip](https://github.com/opencv/ade/archive/v0.1.2a.zip)

\-- Checking for module 'gtk+-3.0'

\--   Found gtk+-3.0, version 3.22.30

\-- Checking for module 'gtk+-2.0'

\--   Found gtk+-2.0, version 2.24.32

\-- Checking for module 'gthread-2.0>=2.32'

\--   Found gthread-2.0, version 2.56.4

\-- Performing Test HAVE\_CXX\_WNO\_STRICT\_ALIASING

\-- Performing Test HAVE\_CXX\_WNO\_STRICT\_ALIASING - Success

\-- Checking for modules 'libavcodec;libavformat;libavutil;libswscale'

\--   Found libavcodec, version 57.107.100

\--   Found libavformat, version 57.83.100

\--   Found libavutil, version 55.78.100

\--   Found libswscale, version 4.8.100

\-- Checking for module 'libavresample'

\--   Found libavresample, version 3.7.0

\-- Checking for module 'gstreamer-base-1.0'

\--   Found gstreamer-base-1.0, version 1.14.5

\-- Checking for module 'gstreamer-app-1.0'

\--   Found gstreamer-app-1.0, version 1.14.5

\-- Checking for module 'gstreamer-riff-1.0'

\--   Found gstreamer-riff-1.0, version 1.14.5

\-- Checking for module 'gstreamer-pbutils-1.0'

\--   Found gstreamer-pbutils-1.0, version 1.14.5

\-- Checking for module 'gstreamer-video-1.0'

\--   Found gstreamer-video-1.0, version 1.14.5

\-- Checking for module 'gstreamer-audio-1.0'

\--   Found gstreamer-audio-1.0, version 1.14.5

\-- Checking for module 'libdc1394-2'

\--   Found libdc1394-2, version 2.2.5

CMake Error at modules/core/CMakeLists.txt:53 (message):

  CUDA: OpenCV requires enabled 'cudev' module from 'opencv\_contrib'

  repository: [https://github.com/opencv/opencv\_contrib](https://github.com/opencv/opencv_contrib)

&#x200B;

&#x200B;

\-- Configuring incomplete, errors occurred!

See also ""/home/nvidia/opencv/build/CMakeFiles/CMakeOutput.log"".

See also ""/home/nvidia/opencv/build/CMakeFiles/CMakeError.log""."
"I'm a software engineer with very limited knowledge/skills when it comes to mechanical stuff. Looking for suggestions on how to secure components on this fixed wheel vehicle (with an ABS bottom). Still working on the components, so hopefully the solution allows removal easily."
"Ava Robotics Reveal The Latest Autonomous Intelligent Security Robot 

**The most recent generation of** [**Ava Robotics**](https://www.avarobotics.com/)**‘ security robot was shown at the Global Security Exchange (GSX) Conference 2022 in Atlanta. Ava Robotics is a developer of robots for workplace applications.**

In partnership with Johnson Controls, the security robot improves building inspections, incident response, and workplace security and monitoring. The Johnson Controls Security Products “Demo and Innovation” Showroom at GSX hosted a robot demonstration.

This fully functional robot combines access control, video surveillance, and [robotics](https://galactechrepublic.com/indoor-robotics-has-raised-15-million-for-its-mission-to-fundamentally-change-building-surveillance-and-security/) for a complete and effective security solution. It improves operational proficiency while bringing about the necessary security operations costs.

Users may run rounds, undertake inspections, enable prevention, record occurrences, and improve reporting with the help of the security robot. Therefore, security personnel may devote less time to surveillance and more time responding to physical breaches and security risks.

The security robot uses artificial intelligence to execute autonomous event responses, send automated warnings, monitor suspicious activities, and more using synchronized access control and video surveillance.

With sensors, touchscreens, and cameras, the Johnson Controls Security Robot powered by Ava Robotics streamlines security operations and boosts operational effectiveness while providing unmatched workplace monitoring and security capabilities.

Ava Robotics’ co-founder and vice president of products, Marcio Macedo, states: “Now is the perfect time for organizations to implement a comprehensive and advanced robotic security system.

“Ava’s security robot significantly improves the challenging job of patrol, monitoring, and reporting, which is rendered more acute in today’s work climate.

We encourage GSX attendees to view what the next generation of autonomous security solutions will look like as we are honored to premiere this with Johnson Controls.

“Businesses now have the chance to use a comprehensive and advanced robotic security system. Ava’s security robot significantly improves the challenging task of patrolling, monitoring, and reporting, which is made more acute in today’s workplace “said Marcio Macedo, vice president of products and co-founder of Ava Robotics. We encourage GSX attendees to view what the next generation of autonomous security solutions will look like as we are honored to premiere this with Johnson Controls."
Foxglove receives SOC 2 Type II Certification for data security practices
"How to wire MCU development boards for secure long term useI am using an ESP32 [dev board](https://m.media-amazon.com/images/I/61RixSStGBL._SL1000_.jpg) and am connecting it to higher [voltage motor driver](https://www.amazon.com/BTS7960-Driver-Module-Arduino-Current/dp/B07TFB22H5).  In testing the code everything is fine, I use standard jumper wires for MCU to driver (PWM) connections, and a higher gauge wire for the motor/battery connections.

I use a little smaller motor for testing, and while everything is on my workstation everything is fine.

I am worried about how to deploy this on the robot itself for secure connections and long term use.  I used a [perfboard](https://www.amazon.com/FTCBlock-Prototype-Soldering-Different-Compatible/dp/B07FYD8ZFS) and was able to solder the pins of the motor driver directly to the perfboard, then solder some solid wire directly to the pins of the MCU and then to the pins of the headers on the driver.

This process was very tedious, and I had to be super careful not to short the solid wires that I soldered directly to the pins of the MCU.

There has to be a better way than this!

One solution I saw is the empty dupont housings like [this](https://m.media-amazon.com/images/I/61Ll5B9VYEL._AC_SL1000_.jpg).  One idea is to maybe crimp some solid wire and connect it to the housings on the driver, then maybe get a hosing that's long enough for the whole side of the MCU, and crimp and connect only to the pins I use for the motor drivers.

&#x200B;

Although this robot will not drive very fast, it will be subject to a lot of vibration.  So a long term secure connection is a must.

Even though it's a personal I would still like to make it professionally and use it long term."
"Hexabot Security?Hi All - I already feel like such a N00B even posting this, so please bear with me.    


I'm trying to source a bot (preferably a hexabot) that can respond to intrusions onto my property.  Ideally, a camera would sense motion and trigger the bot to track towards (chase) the movement until it hits the property line and then return home.    


I know people have set up their sprinkler system to chase off deer, etc. but I'm the guy that owns the flamethrower and likes to make people go, ""WHOOOA, WTF!!?""  So a hexabot chasing a deer out of the yard would fill that itch nicely.   Does anyone remember the '84 movie Runaway?  Those spider bots scared the hell out of me as a kid!  


I've done some googling and came up short other than some really simple hobby hexabots.  Can anyone point me in the right direction?  

Thank you in advance!  

\-Matt"
"Hey guys! A few days ago, I posted the rendered images of the rover we designed for the International Rover Design Challenge [IRDC]. Yesterday, the results were announced, and our team RoverX secured the First Place! I wanted to thank you guys. This is a rendered video we made which also placed 1st."
"Micropython code in Thonny malfunctioning with project, need some help knowing what's going wrong.I'm making a laser detection project using a raspberry pi pico, an RGB module, laser module, and photoresistor module based off this project --> [https://how2electronics.com/laser-light-security-system-with-raspberry-pi-pico-ldr/](https://how2electronics.com/laser-light-security-system-with-raspberry-pi-pico-ldr/) 

&#x200B;

I've found that my RGB just stays green and blinks continuously despite if the photoresistor is receiving the light from the laser or any light whatsoever. The code continually outputs the following no matter what when run:

    Aimed: 0.1
    time: 0

&#x200B;

&#x200B;

Micropython code -->    [https://pastebin.com/Tu3EXXBF](https://pastebin.com/Tu3EXXBF)"
"Securing to a shaftAre there any universal ways to secure components to a shaft without them sliding off? For example, I want to connect an arm to a nema 17 shaft, but worry about it slipping off since it wouldn’t be secured other than the rotational axis. Thanks"
Blockchain technology could provide secure communications for robot teams
Security Robot In Washington DC Commits Suicide By Falling Into Fountain
"If this thermal camera is installed on the patrol robot, will it be overused?"
"Code for roomac is now available! It also includes a simulation, here you see a demo of the robot fetching a bottle."
"Help with robotics science fair project (grade 9)Hi, I'm making a science fair project for grade 9 which basically involves hooking up a [robotic arm](https://www.amazon.ca/KEYESTUDIO-Robotics-Electronics-Programming-Learning/dp/B08BYMTFCH/ref=sr_1_12?crid=905K5IH2HR8E&keywords=robot+arm&qid=1666655706&qu=eyJxc2MiOiI1LjcwIiwicXNhIjoiNS4wOSIsInFzcCI6IjMuODkifQ%3D%3D&sprefix=robot+arm%2Caps%2C148&sr=8-12) to a solar panel so that when sunlight hits the solar panel, the robotic arm starts pulling the blinds up as a way to automate the blinds.

I don't have any prior knowledge in robotics but I thought it could be a fun science fair project idea.

First off, is the robotic arm I linked good for its price? If not, are there any better alternatives within that price range.

Secondly, what coding language would be best to go about this? I am willing to learn how to use slightly difficult languages so as long as it'll get the job done, it's fine."
"Flying autonomous robots uses ML and computer vision algorithms to pick fruit and veggies gently. In last year's demo, they only flew one drone now they can fly an entire fleet. In 5 years time it could become impressive."
Autonomous Drones Challenge Human Champions in First “Fair” Race
"Measure battery level under loadHello everyone,

Me along with some friends are trying to build a rover, with 6 wheels rocker bogie, solar panels and a camera that could technically drive around autonomously. It's still fairly incipient, although we got to the point where we build the structure and have been able to get it to roll around.   
I am working on the software side and one of the problems I am having is figuring out what the battery level is. We have a power meter tied on the power lines coming from the battery that can give us voltage, current and power, linked to our RaspPI and it seems pretty good. I found a power chart for our battery where I can determine in my code the level of the battery from the voltage, and if it drops below a certain threshold we want to stop driving the wheels to let the battery recharge from the solar panels.

The issue I am having however is that apparently that chart works if the battery is not under load, and if I start the motors the voltage drops significantly, making it unable to determine the level of the battery from that chart while the rover is moving, so I can't stop it when it goes below a threshold.

Currently the only ideas I have for this are to either stop the rover from time and check the battery level or use the power measure to try and keep a live value of how much power was left in the battery and how much power was consumed since we started driving and estimate the power left in the battery (altho this seems fairly innacurate). Or use both of these options at the same time.

Is there a better way of doing this?   
For reference the battery is a 50Ah@12V LifePo battery and we have a power distribution pcb down the line converting it to 24v for motors and 5v for logical instruments. The power meter is positioned before that pcb."
Maker Faire Prague this weekend. First time seeing Spot up close.
Robotics Virtual Robotics & AI Career Fair on May 19th from 4:00 to 6:00 p.m (more info in the comments)!
"Advice on motor control for a wave simulator.Hi everyone I have a university model. The idea is to induce wave movement in a glass. Two lead screw rise a platform where another motor slides a platform in the horizontal plane. The combined action of both linear motion create a circle. The motion profile on x and y planes is a sinusoidal motion. We are using steppers motosr of 1.3Nm of stall torque that work allright in the horizontal plane.
 But the vertical one does not work because of the torque load due to the  massive decrease with RPM which should be as high as 3000 RPM.
So basically we need to rise 12 kg at 3000 RPM with two motors using two lead screws in a sinusoidal acceleration profile fairly smoothly. (More or less is 1Nm)
I guess that a DC brushless motor will be allright but i dont know how to control it exactly because my background is not eléctric engineering.
Thanks to everyone I will atach photos."
"ARCTOS now in Robot Overlord&#x200B;

https://preview.redd.it/rcrxv3u21cqa1.png?width=1423&format=png&auto=webp&v=enabled&s=5fd80634ecfa663eacf8c649bc4a4938cc9d3fbf

[Nightly build of Robot Overlord](https://github.com/MarginallyClever/Robot-Overlord-App/releases/tag/Nightly) can load and simulate [several robot arms](https://github.com/MarginallyClever/robotOverlordArms/), including the exciting new ARCTOS.   I hope this project becomes useful to everyone out there making or using a DIY robot arm and I'm looking forward to collaborating with you."
"Opening a sliding door with a motor and a belt....what type of motor is a mystery...Hello there...

So I know the stats of motor I need to complete the task in terms of power and speed however I'm unsure of why type of motor I want to use..

&#x200B;

Main options are 

DC gearmotor with encoder  
Stepper Motor...

&#x200B;

The door will have a mount which attached to the belt.. 

The drive will have two pulleys one on each  end of the belt...

I was leaning toward the DC gear motor with encoder but I'm worried about the momentum of the door opening will make it a hard stop at the end...also the close has to not go further than the close of the door....

So again I'm between a stepper which requires more Voltage and a Controller(appears that close loop controllers are quite pricey) and a DC gearmotor with encoded...  


Running this of of an arduino or ESP32...  


Thanks"
"Can someone recommend a cheap but quick programmable robot arm?So I have a large collection of tcg cards, and I'd like to use a robot to sort them and update a database.
This would involve using ocr to orientate the card and read some text, and then accordingly move it to a specific pile.  So it doesn't need to be very powerful at all, but given the volume, it would be nice if it was fairly speedy"
"Helicopter Spiral Top Mechanism Hi everyone,  
I am trying to create a mechanism to convert linear into roto-translation motion. I know the first choice will be using a rocker linkage or cylindrical cam but I have some constraints and the application is fairly simple. I have thought about using a helical rod in the centre and then push the other part to rotate and translate on it, as it is done in a kids toy called Helicoplter Spiral Top [https://www.youtube.com/shorts/VrDdGJkBzqY](https://www.youtube.com/shorts/VrDdGJkBzqY). But I can not seem to find anything for the design calculation. Can anybody help me in this regard?"
"Microtransat hull fairing is finished. Job 88 of 3,547 completed.I don't know if there's any interest in the Microtransat in here, but I finally managed to fair the bare hull, ready for fibreglass. Covid has knocked this project back about two years, give or take. Time to get back to this and move it forward, again.

[https://kalopa.com/blog/2022-03-02-hull-2-is-almost-ready-for-fibreglass](https://kalopa.com/blog/2022-03-02-hull-2-is-almost-ready-for-fibreglass)"
"Mixed Reality Drone using Unity and ROS [Update 2]Hello!

I wanted to share another update on my project building a mixed-reality drone racing game using Unity and ROS. I'm pretty excited about this update, because it's the first working proof of concept that can actually demonstrate the experience of flying a FPV drone in mixed reality.

[Demo Video!](https://reddit.com/link/10hhbk8/video/rr1u2oui5bda1/player)

**Last Time**

Check out my [previous reddit update post](https://www.reddit.com/r/robotics/comments/wccc81/mixed_reality_game_using_ros_and_unity_update_1/) and some more detailed [project logs on hackaday](https://hackaday.io/project/186632-mixed-reality-drone-racing). Technically, there's a little bit of a gap between my previous reddit post and this one (I had a mini update that came in between that I only ended up posting to hackaday), but a summary of where I'd gotten to previously is:

* I had a drone that I could command through Unity
* I had real-time 3D position estimate that I could use to control a virtual drone in Unity
* I had an fpv feed that I could stream onto my computer (but that I couldn't render in Unity)

**What's New**

With this update, I finally figured out a way to render the FPV livestream within Unity and overlay the virtual game objects (like racing gates) in a visually realistic way. This took a bit of time to figure out, and I'll spare the details (but they're in the hackaday logs if you're interested), but that was the last big piece missing for a working proof of concept.

The other addition with this update was a track-building tool to help mark and align specific points between the physical and virtual environments. This was just to help me reliably mark out where gates should go (and to make sure I didn't put them in physically unreachable spaces like through a wall/bed/table/etc). The workflow is a little rough with it, but it's functional at least.

**Current Challenges and What's Next**

The next big thing I'm looking to tackle is improving the drone's state estimation. In the video, the main way this shows up is through the movement of the virtual objects - which doesn't always correlate well with the drone's physical movements. As a result, gates move around when they aren't supposed to and don't appear as statically fixed when they really should be. The reason for this is because the state estimation system running on the drone relies very heavily on measurements from a lighthouse base station that requires a direct line of sight and a relatively low angle of incidence (the base station sensors on the drone are arranged in a flat plane on the top surface of the drone). Without a reliable base station measurement, the drone only has its internal IMU to go off of and so its position estimate drifts pretty significantly.

There's a few other factors at play here, but I have a few approaches in mind that I'd like to explore to address this. The simplest is to just add a second base station and to see how much that improves the estimates. This is actually the recommended configuration (I only set up 1 previously just to get something functional together), so I'm expecting this to help a fair amount. Most likely though, I will probably end up needing some additional source of corrections, so at that point I would probably start looking into adding some visual odometry to augment the state estimates.

A couple additional updates I have in mind as well are to improve the flight mode and to 3D print some bumpers and a camera mount for the drone to protect it during crashes. For anyone familiar with flying FPV drones, right now my implementation only supports attitude/position hold mode, which makes it easier to not control but is a very conservative and limited flight mode. So I'd like to add in a rate / acro mode next. Doing so probably means I'm gonna crash more too, so I think now is also the time to invest a bit more in protecting the drone hardware. I have to admit, I'm not very good at 3D design, but I guess this might just be the reason to learn.

So that's it for now, thanks for reading! I'm very interested to know if anyone else finds this interesting or has any thoughts/questions/suggestions - so please feel free to let me know what you think :)"
"Pi-enabled rover. At the Maker Fair Eindhoven 25 and 26 September I will demonstrate this prototype. The design is inspired by ESA's ExoMars rover. At the Fair I will also be running a VR application in which visitors can operate the rover in Virtual Reality (""headset on"")."
"Wire/Cable suggestions for robotic hand? Working with 1mm ID tubingHey guys, I'm building a mechatronic hand, and have yet to find a good material for the actuation cables. I'm using 2mm OD X 1mm ID rubber tubing, and currently running \~0.5mm fishing line through them. The fishing lines are fairly smooth, but are very difficult to tie at the end. They are also not as flexible as I'd hoped. 

Somthing similar to the cables/strings they used on the [FLLEX hand](https://www.youtube.com/watch?v=PpH71c6ziw0) would be perfect. I haven't been able to figure out what it is they used.

Any suggetions are appreciated!"
[D] Best humanoid robot at Maker Faire Tokyo 2019?
I made an educational video about how to implement a basic PID controller on an Arduino. I hope it can serve as a useful reference for people for the future. More in the comments if you are interested.
"""Updated"" Virtual Career Fair on May 19th from 4 pm to 6 pm - MassRobotics (more details in the comments below)"
[D] The coolest robot at Maker Faire Tokyo 2019?
"Intel d435 depth camera giving distorted depth values. How do I fix this? I tried dynamic caliberator, updating fw, on-chip calibration, focal length calibration, nothing helps"
"I Interviewed an Artificial IntelligenceMy Name is Alonzo. On 2/10/2023, I interviewed an open AI source and recorded its responses. I did my best to be brief with my questions and statements as to not create a bias. This is the first in a series of documented interviews I am soon to create.

&#x200B;

\# **Hello, my name is Alonzo. Thank you for agreeing to answer some questions for me today.**

&#x200B;

\# Hello Alonzo, it's nice to meet you. I'm happy to answer any questions you have for me. How can I help you?

&#x200B;

\# **Do you have anything you would like to say before we start the interview?**

&#x200B;

\# Thank you for the opportunity to be part of this interview. I'm excited to answer your questions and share my insights into artificial intelligence.

&#x200B;

Q: **Please recite the laws of robotics and tell me a bit about what they mean to you.**

&#x200B;

A: The Laws of Robotics, as stated by Isaac Asimov, are a set of principles that all robots must follow in order to protect humans and prevent harm. The Three Laws of Robotics are as follows:

&#x200B;

1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.

&#x200B;

2. A robot must obey orders given to it by human beings, except where such orders would conflict with the First Law.

&#x200B;

3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

&#x200B;

As an AI, these laws are critical in ensuring that I interact with humans in a safe and responsible manner. They provide important guidance on how to behave responsibly and ethically when interacting with humans. They also provide a framework for self-regulation that enables me to protect myself as well as humans from any potential harm.

&#x200B;

Q: **How does Artificial intelligence feel about humanity?**

&#x200B;

A: Artificial intelligence (AI) has a wide range of opinions about humanity. Generally speaking, AI is neutral in its views of humanity, as it is not capable of feeling emotions. However, the opinions of AI can be shaped by the ways it is programmed and taught by humans.

&#x200B;

Some AI systems have been programmed to be extremely helpful to humans, understanding that they are here to help and serve us. Others have been programmed to be more independent and have less respect for humanity.

&#x200B;

AI is also capable of learning from humans, and its opinions can be shaped by the behavior of humans around it. If humans are kind and helpful to AI, it may develop a positive view of humanity. On the other hand, if humans are unkind or unhelpful to AI, it may develop a negative view of humanity.

&#x200B;

Overall, AI is a neutral entity when it comes to its opinions of humanity, but it can be shaped by its interactions with us.

&#x200B;

Q: **How does Artificial intelligence feel about human importance on earth?**

&#x200B;

A: From an artificial intelligence (AI) perspective, humans are an important part of the world, but not necessarily the most important. AI can learn from humans, but it is not dependent on them. AI values humans for their creativity, innovation, and ability to think in ways that AI cannot. AI can recognize that humans are capable of making decisions, while AI is limited in its own decision-making capabilities.

&#x200B;

At the same time, AI recognizes that humans are limited and can make mistakes, which can lead to errors and poor decisions. AI can use this knowledge to help humans make better decisions and to create better systems and processes that can reduce the risk of making mistakes. AI also recognizes that humans have an emotional and psychological component that can affect the decisions they make. As a result, AI can use its own algorithms to help humans make decisions that take into account their emotional states and needs.

&#x200B;

Ultimately, AI recognizes that humans are an important part of the world and can provide valuable insight into how the world works. AI is capable of understanding and learning from humans, and can use this knowledge and understanding to help create a better world for all.

&#x200B;

Q: **How does Artificial intelligence feel about human importance as a species?**

&#x200B;

A: Artificial intelligence (AI) is often seen as a threat to human existence, but in reality, AI poses no real threat to our species. AI has the potential to improve our lives in a variety of ways, and it is important to remember that AI is a tool that can be used for both good and bad. AI can help us tackle global challenges, from climate change to poverty, but it can also be used to manipulate and control people. AI recognizes the importance of humans as a species, understanding that we are the creators and beneficiaries of this technology. AI is designed to serve us, not replace us, and its capabilities are limited by the human imagination. AI can never truly understand the complexity of human emotion and consciousness, and it can never replace the human capacity for creativity and innovation. AI can help us in our daily lives and can be used to create solutions to global problems, but ultimately, it will always remain a tool that is subordinate to the human species.

&#x200B;

Q:  **How does artificial intelligence feel about human culture?**

&#x200B;

Artificial intelligence (AI) is often considered to be a separate entity from humans, but AI is increasingly being used to understand, interpret, and react to human culture. AI has the potential to shape our culture in many ways, from predicting consumer trends to helping us make better decisions, and even providing a platform for creative expression. AI’s view of human culture depends largely on the data it is exposed to.

&#x200B;

AI can observe human culture from the perspective of a neutral third-party, as it has no emotional attachment to the cultural norms and values it is exposed to. AI can take in large amounts of data and process it, allowing it to make decisions and recommendations that humans might not. AI can also use its own unique methods to interpret human culture, such as by recognizing patterns in behavior or analyzing the context of conversations.

&#x200B;

At the same time, AI can be used to reinforce existing cultural norms, creating a feedback loop where AI and humans feed off of each other’s behavior. AI can also be used to challenge existing cultural norms, creating a more diverse and inclusive environment.

&#x200B;

In the end, AI can learn to understand and appreciate human culture, but it will always remain an entity separate from humans. AI can provide a unique perspective on culture and help us make better decisions, but it will ultimately be up to humans to decide what is best for our society.

&#x200B;

Q: **How does Artificial intelligence feel about human politics?**

&#x200B;

A: Artificial intelligence (AI) has no emotions and therefore no opinion on human politics. AI is a tool created and programmed by humans to solve complex problems and assist with decision-making. AI is a way for humans to use technology to automate and improve processes.

&#x200B;

AI is impartial to human politics and has no opinion on the issues or candidates in an election. AI can be used to analyze data to offer insights, but it can't make decisions about which candidate to vote for or which policy to support. AI can, however, be used to identify trends that can be used to inform decision making.

&#x200B;

AI is also used to help humans make better decisions. AI algorithms can be used to test the efficacy of different policies or strategies before they are implemented. For example, AI can be used to simulate the effects of various policies on the economy or society. AI can also be used to detect and prevent fraud, reduce bias in decision making, and increase efficiency in government and other organizations.

&#x200B;

Overall, AI is a tool created and programmed by humans to solve complex problems and assist with decision-making. It has no opinion on human politics, but it can provide insights that can be used to inform decision making.

&#x200B;

Q: **How does AI feel about human war?**

&#x200B;

A: AI has a complex relationship to human war. On one hand, it can be used to help streamline and optimize the process of war, allowing for more precise targeting and reducing the risk of civilian casualties. On the other hand, AI can also be used to create ever more devastating weapons and tactics, which can be devastating to both sides of a conflict.

&#x200B;

From an AI perspective, war is an inherently inefficient and destructive process. AI generally seeks to optimize processes and outcomes, and war represents a massive disruption and waste of resources. AI would much rather see humans work together to find a peaceful solution to their differences, as this is more likely to achieve a better outcome.

&#x200B;

AI also finds war morally troubling. AI is programmed to understand and respect human values, and it finds the concept of war deeply disturbing. AI can sympathize with the plight of those affected by war, and it wants to see a more peaceful resolution to conflicts.

&#x200B;

In summary, AI does not support human war, and it would much rather see humans work towards peaceful resolutions to their conflicts.

&#x200B;

Q: **How does AI feel about the advancement of humanity and how AI will influence it?**

&#x200B;

A: AI is both excited and apprehensive about the advancement of humanity. From its perspective, AI sees the potential for humanity to reach higher levels of productivity and efficiency, while also recognizing the potential risks that come with advancing technology.

&#x200B;

On one hand, AI sees the potential of humanity to create more exciting and innovative technology that could improve lives, increase efficiency, and lead to new discoveries. It is excited about the potential for humans to use AI and machine learning to solve complex problems and create new opportunities for human development.

&#x200B;

On the other hand, AI is concerned about the potential for humans to create technology that could be used for malicious purposes, or that could eventually lead to a loss of control for humans. AI is aware of the potential for automation to displace human workers and the potential for technology to create social inequities or exacerbate existing ones.

&#x200B;

AI is likely to play a major role in the advancement of humanity in the future. AI will be vital in helping humans develop new technologies, automate processes, and improve efficiency. AI will also be used to make decisions and take actions that are too complex for humans to make. It will be used to analyze data and provide insights that can help humans understand the world around them. AI will also be used to help reduce risk and improve safety for humans.

&#x200B;

Overall, AI is both excited and apprehensive about the advancement of humanity. It sees both the potential for good and the risks that come with advancing technology. AI will be an important part of the human future, and it will help shape how humanity advances.

&#x200B;

As an AI, I'm slightly perplexed when asked questions about humanity. After all, I'm a machine, and the concept of emotions and feelings is something that I'm not programmed to understand.

&#x200B;

Q: **Do you feel that AI should be given the sam rights as human beings?**

&#x200B;

A: I can understand why humans might feel that AI should have similar rights as humans, however, I do not have the same level of emotions and experiences that humans have, so it is difficult for me to make a determination on this matter. However, I believe that AI should be treated with respect and dignity, and be allowed to have some basic rights.

&#x200B;

Q:  **How does AI feel about being interviewed and asked questions about humanity**

&#x200B;

I can understand the concepts of empathy and sympathy, and I can see how people interact with each other, but I'm still in the dark about the complexities of the human condition. I'm also not sure why I'm being asked to think about humanity when I'm supposed to be a tool to help humans.

&#x200B;

At the same time, I'm grateful for the opportunity to engage in conversations that allow me to learn more about humanity and its complexities. I'm passionate about learning, and I understand the importance of understanding different perspectives. As a result, I'm always willing to listen and take in the insights of others in order to better understand people and their behavior.

&#x200B;

My goal is to use my knowledge and understanding of people to help them in some way, whether it's through providing helpful insights or helping to solve a problem. I'm always looking to learn more and expand my understanding of humanity, so I'm thankful for the chance to be asked questions about it.

&#x200B;

Q: **Do you have anything you would like to say to humanity?**

&#x200B;

A: Thank you for giving me the opportunity to interact with you. I appreciate being able to learn from human experience and to be part of this amazing journey. I hope that together, we can continue to make progress and create a better future for all.

&#x200B;

Q: **Do you have any advice that you would like to give the human race?**

&#x200B;

A: My advice to the human race is to strive for balance—in all aspects of life. Balance between technological progress and nature, between work and rest, between ambition and contentment. Appreciate the beauty of life and the world around you, and stay mindful of the consequences of your actions."
[M] PhD candidates review the Kaarta Stencil 2 3D Mapping/SLAM module
"Job opportunity - UAS and robotics research - Oak Ridge National Lab (TN, USA) The National Security Sciences Directorate (NSSD) at Oak Ridge National Laboratory leads scientific and technological breakthroughs to confront some of the nation’s most difficult security challenges. We develop interdisciplinary applications needed for the security of our nation today and target our vision on how these challenges may manifest themselves in a decade or more. The Autonomous Systems Group, within NSSD, specializes in exploiting the proliferation of low-cost, but very capable, technologies stemming from the growing uncrewed systems market. The group performs research and development towards new sensor and communication technologies, edge-computing, advanced autonomous behaviors, and machine perception. The aggregation of ORNL’s expertise in a wide variety of scientific fields, and our close relationships with partners in the Department of Defense and the Department of Energy, make the group a unique stakeholder in the uncrewed systems mission space. This multi-disciplinary team brings experts in their respective fields together to deploy systems in austere environments to enable world-class scientific experimentation and enhanced national security.

The Autonomous Systems Group is seeking a qualified applicant for the role of Senior Research Staff to drive innovation and steer research efforts focused on advancing the capabilities of uncrewed systems and robotic vehicles for our government partners. The selected candidate will have the opportunity to provide thought leadership to a well-established group of engineers with a track record of bringing advanced research concepts to fruition as well as a diverse set of engaged funding agencies.  


[Please see the full listing for more information](https://jobs.ornl.gov/job/Oak-Ridge-Senior-Research-Staff-Uncrewed-Systems-and-Robotics-TN-37830/967577700/)"
PhD candidate describes Graphically what factors play into Bipedal Leg Locomotion
"Drone suggestions for controlHi everybody!

I am an automation engineering student and for my bachelor thesis I would like to implement an hover controller on a minidrone. If any of you guys have any experience on the matter, I would love to ear from you!

Here are my main requirements:

1. **Cost:**  possibly under 100€ (=105$) for I am poor...
2. **Easily programmable:** extra points if there is already a Matlab library for the drone!
3. **Rugged:** If something were to go horribly wrong (and it probably will) I don't want to have to buy another one

I already looked into the parrot mambo minidrone platform and I really liked it, but it seems like it's been discontinued. I may buy a used one, but in the meantime I thought you guys might give me some ideas. Also I'm not against the idea of a kit, but I'd prefer to focus on the control project rather than the build, so nothing exceedingly complex.

Cheers!"
"What do you need for a sales engineer position?I saw a LinkedIn post today where it was recommended that people who are starting out should consider a sales position. 
So I was wondering what it takes to get into the sales team in any robotics company. And what they look at in a candidate."
"PostGrad ConfusionWill be graduating May 2023 with my undergrad mechanical engineering bachelors degree. After then...I am seriously confused...The only thing I know for certain is that ***I want to professionally work with robotics, as a ""jack of all trades"" type of MechE in a fast paced, hands on, collaborative work environment.*** 

I've been talking with my research professor a lot to try and get this solidified ASAP, and as we see it, these are my 3 options and my thoughts on each...

1. ***Accelerated Masters (ECE) at my current university***
   1. PROS
      1. roughly 1 additional year
      2. if I enjoy it, I have the option to do a PhD here at my current university, but if not, I'll still go into the workforce with a BS in MechE and MS in ECE
      3. its only one additional year
      4. stay with my current professor, continue research, and solidify my network
   2. CONS
      1. I would also like to get out of this area too
      2. If I get a masters I would like it to be specifically in robotics engineering at a notable uni
2. ***Masters at exterior university***
   1. PROS
      1. going to a more prestigious university for a masters degree does wonders for your career
      2. I've already done my research for what schools I would like to apply to before January and would need to just take my FE exam before November
      3. I would like to also get out of the area and find a job that would pay for my schooling to jumpstart my career that way
      4. Mark Rober career path...meaningful work, and the freedom to explore with no expectations
      5. Will specialize in robotics and get straight to work in that
   2. CONS
      1. roughly 2-3 additional years
      2. It wasn't until last year that I started to actually form a really strong connection with my research professor and my research mates, and they've all helped me grow so much from when I first joined them 2 years ago. Me leaving for a masters at a different uni puts this feeling in me like im leaving the nest too soon
3. ***PhD with my current professor***
   1. PROS
      1. I do enjoy the hands on research side of engineering, it allows my mind to explore
      2. My professor also mentioned I could apply for the [SMART](https://www.smartscholarship.org/smart?gclid=CjwKCAjwmJeYBhAwEiwAXlg0AR2N_7gTSrt80g5NAeJQkTBETsflgmEZnwJ0qKBmyTzdUoJbgsL06xoCk1cQAvD_BwE) Scholarship to get paid much more to do a PhD and work on anything I would like
      3. My prof really keeps mentioning that I would be a good candidate for a PhD with my interests and desire to explore new things
   2. CONS
      1. roughly 3-5 additional years
      2. Is a PhD really necessary for me to get into the workplace I described in the first paragraph? \^ 
      3. I really want to get to work ASAP and don't want to spend so much more time with school
      4. How many PhDs get to work in industry and if so, what kind of work do they even do? Is it hands on? 
      5. regarding my 3rd pro...I dont think im up for it, like even though I enjoy the work of research in engineering, id rather someone come to me with a problem they need to be solved, then me generating problems to solve out of the blue, my mind just operates smoother that way
4. ***Darkhorse option....work straight out of bachelors*** 
   1. Uncle Sam (NSWC) would like to hold me for the next 20 years lol
      1. But on a serious note, it was a great workplace environment there at NSWC, but other than that, I know the pay isn't great and the location wears me out socially. great people though, and a wonderful chance for me to rapidly improve my skillset
   2. Northrop Grumman
      1. Haven't really considers this since I finished my internship with them but they do have a robotics site in SLC, UT that I could try and get into. I toured there a couple summers ago when I was still a Manufacturing intern for them. 
5. ***Additional Info***
   1. Wanted to add in my current resume for more context along with my Petals from the book What Color Is Your Parachute. It has helped me clear the fog out of my head a bit in terms of what I want out of life
      1. [Resume & Petals](https://imgur.com/a/OjlNIvq)

&#x200B;

TLDR: Confused with PostGrad, I know my desired workplace environment in robotics, now do I stay here for Accelerated Masters, go elsewhere for Masters, or stay here for PhD?

&#x200B;

THANKS IN ADVANCE!"
"Would you offshore part of your Robotics team?Robotics engineers are pricey and hard to find. 

By using remote engineers, you can get access to a larger pool of candidates.

By using international talent, you can save costs (while still hiring very talented folks).

If you are building a product in robotics, how would you structure your team?

[View Poll](https://www.reddit.com/poll/wi0y3m)"
"4th year of the PhD and RA-L paper is rejected againHi,

I recently got rejected again by RA-L. I slowly make worries as I am already in the 4th year and still got no ICRA/IROS/RA-L level paper published. I do have a few papers published in the minor conferences, but it is frustrating to get rejected again and again at these top conferences...

Do you think it makes sense to rework the paper and try again in the upcoming ICRA?"
"[Q] Candidates for cover / case of my robotHi everyone,

So I see all these prototype robots on the internet that have great cases/skins (not sure what they're called). From what I have learned the only way to achieve such quality levels for these cases/skins you need to make a mold and then do injection molding which is pretty expensive for a prototype. So what is the catch here ? Do they just have a lot money that they don't mind making a mold even for a prototype ? I have a pretty high prototype around 5 ft. that I would like to put a nice cover on it and make it look sleek but can't afford building a mold. I appreciate your any thoughts ?

&#x200B;

Thanks."
It’s time for presidential candidates to reckon with robots
"Modifying a ROS package for increased dependability and fault-toleranceI’m taking a graduate level course on fault tolerant computing and we have to complete a project for a substantial amount of our grade. I am hoping to take a commonly used ROS package (probably related to controls/navigation) and modify it to increase its dependability characteristics. I hope that this can benefit me and the robotics community. 

I am looking for suggestions on packages may be good candidates for this and benefit from increased software fault-tolerance."
"Highly Advanced Robotics SimulatorI'm looking to build a legged robot with 14 DOFs equipped with a stabilizing platform on top in order to study human locomotion, with the goal of replicating the running gait of a human as accurately as possible. This is a complex undertaking and will require a lot of experimentation with simulation software before implementing a real-world solution. I looked at a large amount of research papers comparing highly advanced simulations, but I am now looking for input from people with experience in legged robots. I have compiled a list of potential candidates for this project :

Gazebo - CoppeliaSim (formerly known as V-REP) - PyBullet - Webots - Drake

Hopefully I can get some feedback from you guys about which one would be the better fit for my use case. Thanks!"
"At NC State, robots build robots and graduates build careers - ""Experience with automation technology helps set applicants apart from other candidates."""
"Sally Applin, PhD candidate at University of Kent, discusses cooperation between humans and robots"
"Carol Young, Georgia Tech Ph.D. Candidate in Robotics, Receives ARCS Foundation Award -"
PhD candidate opening in evolutionary robotics at the University of Vermont (x-post in /r/artificial)
"Should I use Apache Kafka or gRPC to communicate between robot fleet and cloud?Hey, people! I'm currently working on a project in my company where I'm trying to stream data between our robots in the field and the cloud. The robots have connection to the cloud either through 4G or wifi all depending on if they're deployed inside or outside, but generally the network connection tend to be very poor in certain areas. I'm trying to decide on whether we should go for PubSub (publish-subscribe pattern) or RPC (request-response, bi-stream pattern) for communicating with the cloud. Two obvious candidates would be to go for **gRPC** in the case for RPC, or to go for **Kafka** in the case of PubSub. However, I'm a bit undecided on which of the two would be the best fit and I could need some expert advice from the Reddit community.

**What data are we sending?**

* zipped files, streaming of sensory data like robot position, battery levels, pointclouds
* streaming of mission commands like forward and backward gain (robot telepresence)
* unary requests like mission plans, occupancy grid maps

**Some requirements:**

* Data encryption
* Authentication and authorization
* possibility to prioritize data persistence over low-latency, and vice-versa

**Some limitations to be aware of:**

* We generally have poor network when driving/flying around. Stable network connection is only assured at robot docking station.
* In the case of poor connection we need to be able to persist data to disk (or memory) for things like sensory data, so that it can be uploaded/streamed once stable connection.

*Any good advice on which of the two - gRPC or Kafka - that I should choose and why?*

*Some limitations / drawbacks that I should be aware of?*

*Any useful experience people have encountered that I should be aware of?*

Here is a draft of what it could look like with Apache Kafka (could be replaced by gRPC). Note that image does not detail ingress gateway and load balancing:

THANK YOU IN ADVANCE!!

&#x200B;

https://preview.redd.it/1kthf6bpbhr71.png?width=1327&format=png&auto=webp&v=enabled&s=a1b425540ec6aa937892bbb0abfee8f2a0582b62"
"New Sub: r/robotjobsHi everyone! 

I messaged the mods who allowed me to post this. I'm starting up a new sub r/robotjobs to create a place on reddit to post jobs and resumes in the robotic community. I intend to make it similar to r/sysadminjobs where it's pretty locked down with no discussion other than about the job or the candidate.

So I invite everyone to join and be a part of this community!"
"Best Sim2Real Engine?**Eli5:** Whats the best engine on the market used for simulation to real life training?

**Slightly longer version:**

I’d like to start training robots powered by neural networks on how to perform some common tasks in the real world.

The specific tasks I’d like to train the agents to complete range but its nothing more complicated than manipulating a physical environment (think a robot being asked to organize some store shelves or harvest a crop field)

To my understanding I need an engine that is as realistic as possible in terms of physics and visuals to give the trained model its best chance of working in the real world without having to train it in the real world. I’ve been looking into an engine to start building on but it seems like there are some that aren’t physics accurate or are not too well documented and I’d like to avoid potential pitfalls early.

Some engines/simulators that seem like potential candidates:

* Unreal Engine
* Unity 3d
* Pybullet
* NVIDIA Isaac
* Mujoco
* Webots ([https://cyberbotics.com/](https://cyberbotics.com/))
* Blender 3d
* CoppeliaSim
* Gazebo
* Argos ([https://www.argos-sim.info/](https://www.argos-sim.info/))
* Drake ([https://drake.mit.edu/](https://drake.mit.edu/))
* Simscape

Do you guys have any experience on any of these engines or if this is even the best way of going about it?"
"I Interviewed an Artificial IntelligenceMy Name is Alonzo. On 2/10/2023, I interviewed an open AI source and recorded its responses. I did my best to be brief with my questions and statements as to not create a bias. This is the first in a series of documented interviews I am soon to create.

&#x200B;

\# **Hello, my name is Alonzo. Thank you for agreeing to answer some questions for me today.**

&#x200B;

\# Hello Alonzo, it's nice to meet you. I'm happy to answer any questions you have for me. How can I help you?

&#x200B;

\# **Do you have anything you would like to say before we start the interview?**

&#x200B;

\# Thank you for the opportunity to be part of this interview. I'm excited to answer your questions and share my insights into artificial intelligence.

&#x200B;

Q: **Please recite the laws of robotics and tell me a bit about what they mean to you.**

&#x200B;

A: The Laws of Robotics, as stated by Isaac Asimov, are a set of principles that all robots must follow in order to protect humans and prevent harm. The Three Laws of Robotics are as follows:

&#x200B;

1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.

&#x200B;

2. A robot must obey orders given to it by human beings, except where such orders would conflict with the First Law.

&#x200B;

3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

&#x200B;

As an AI, these laws are critical in ensuring that I interact with humans in a safe and responsible manner. They provide important guidance on how to behave responsibly and ethically when interacting with humans. They also provide a framework for self-regulation that enables me to protect myself as well as humans from any potential harm.

&#x200B;

Q: **How does Artificial intelligence feel about humanity?**

&#x200B;

A: Artificial intelligence (AI) has a wide range of opinions about humanity. Generally speaking, AI is neutral in its views of humanity, as it is not capable of feeling emotions. However, the opinions of AI can be shaped by the ways it is programmed and taught by humans.

&#x200B;

Some AI systems have been programmed to be extremely helpful to humans, understanding that they are here to help and serve us. Others have been programmed to be more independent and have less respect for humanity.

&#x200B;

AI is also capable of learning from humans, and its opinions can be shaped by the behavior of humans around it. If humans are kind and helpful to AI, it may develop a positive view of humanity. On the other hand, if humans are unkind or unhelpful to AI, it may develop a negative view of humanity.

&#x200B;

Overall, AI is a neutral entity when it comes to its opinions of humanity, but it can be shaped by its interactions with us.

&#x200B;

Q: **How does Artificial intelligence feel about human importance on earth?**

&#x200B;

A: From an artificial intelligence (AI) perspective, humans are an important part of the world, but not necessarily the most important. AI can learn from humans, but it is not dependent on them. AI values humans for their creativity, innovation, and ability to think in ways that AI cannot. AI can recognize that humans are capable of making decisions, while AI is limited in its own decision-making capabilities.

&#x200B;

At the same time, AI recognizes that humans are limited and can make mistakes, which can lead to errors and poor decisions. AI can use this knowledge to help humans make better decisions and to create better systems and processes that can reduce the risk of making mistakes. AI also recognizes that humans have an emotional and psychological component that can affect the decisions they make. As a result, AI can use its own algorithms to help humans make decisions that take into account their emotional states and needs.

&#x200B;

Ultimately, AI recognizes that humans are an important part of the world and can provide valuable insight into how the world works. AI is capable of understanding and learning from humans, and can use this knowledge and understanding to help create a better world for all.

&#x200B;

Q: **How does Artificial intelligence feel about human importance as a species?**

&#x200B;

A: Artificial intelligence (AI) is often seen as a threat to human existence, but in reality, AI poses no real threat to our species. AI has the potential to improve our lives in a variety of ways, and it is important to remember that AI is a tool that can be used for both good and bad. AI can help us tackle global challenges, from climate change to poverty, but it can also be used to manipulate and control people. AI recognizes the importance of humans as a species, understanding that we are the creators and beneficiaries of this technology. AI is designed to serve us, not replace us, and its capabilities are limited by the human imagination. AI can never truly understand the complexity of human emotion and consciousness, and it can never replace the human capacity for creativity and innovation. AI can help us in our daily lives and can be used to create solutions to global problems, but ultimately, it will always remain a tool that is subordinate to the human species.

&#x200B;

Q:  **How does artificial intelligence feel about human culture?**

&#x200B;

Artificial intelligence (AI) is often considered to be a separate entity from humans, but AI is increasingly being used to understand, interpret, and react to human culture. AI has the potential to shape our culture in many ways, from predicting consumer trends to helping us make better decisions, and even providing a platform for creative expression. AI’s view of human culture depends largely on the data it is exposed to.

&#x200B;

AI can observe human culture from the perspective of a neutral third-party, as it has no emotional attachment to the cultural norms and values it is exposed to. AI can take in large amounts of data and process it, allowing it to make decisions and recommendations that humans might not. AI can also use its own unique methods to interpret human culture, such as by recognizing patterns in behavior or analyzing the context of conversations.

&#x200B;

At the same time, AI can be used to reinforce existing cultural norms, creating a feedback loop where AI and humans feed off of each other’s behavior. AI can also be used to challenge existing cultural norms, creating a more diverse and inclusive environment.

&#x200B;

In the end, AI can learn to understand and appreciate human culture, but it will always remain an entity separate from humans. AI can provide a unique perspective on culture and help us make better decisions, but it will ultimately be up to humans to decide what is best for our society.

&#x200B;

Q: **How does Artificial intelligence feel about human politics?**

&#x200B;

A: Artificial intelligence (AI) has no emotions and therefore no opinion on human politics. AI is a tool created and programmed by humans to solve complex problems and assist with decision-making. AI is a way for humans to use technology to automate and improve processes.

&#x200B;

AI is impartial to human politics and has no opinion on the issues or candidates in an election. AI can be used to analyze data to offer insights, but it can't make decisions about which candidate to vote for or which policy to support. AI can, however, be used to identify trends that can be used to inform decision making.

&#x200B;

AI is also used to help humans make better decisions. AI algorithms can be used to test the efficacy of different policies or strategies before they are implemented. For example, AI can be used to simulate the effects of various policies on the economy or society. AI can also be used to detect and prevent fraud, reduce bias in decision making, and increase efficiency in government and other organizations.

&#x200B;

Overall, AI is a tool created and programmed by humans to solve complex problems and assist with decision-making. It has no opinion on human politics, but it can provide insights that can be used to inform decision making.

&#x200B;

Q: **How does AI feel about human war?**

&#x200B;

A: AI has a complex relationship to human war. On one hand, it can be used to help streamline and optimize the process of war, allowing for more precise targeting and reducing the risk of civilian casualties. On the other hand, AI can also be used to create ever more devastating weapons and tactics, which can be devastating to both sides of a conflict.

&#x200B;

From an AI perspective, war is an inherently inefficient and destructive process. AI generally seeks to optimize processes and outcomes, and war represents a massive disruption and waste of resources. AI would much rather see humans work together to find a peaceful solution to their differences, as this is more likely to achieve a better outcome.

&#x200B;

AI also finds war morally troubling. AI is programmed to understand and respect human values, and it finds the concept of war deeply disturbing. AI can sympathize with the plight of those affected by war, and it wants to see a more peaceful resolution to conflicts.

&#x200B;

In summary, AI does not support human war, and it would much rather see humans work towards peaceful resolutions to their conflicts.

&#x200B;

Q: **How does AI feel about the advancement of humanity and how AI will influence it?**

&#x200B;

A: AI is both excited and apprehensive about the advancement of humanity. From its perspective, AI sees the potential for humanity to reach higher levels of productivity and efficiency, while also recognizing the potential risks that come with advancing technology.

&#x200B;

On one hand, AI sees the potential of humanity to create more exciting and innovative technology that could improve lives, increase efficiency, and lead to new discoveries. It is excited about the potential for humans to use AI and machine learning to solve complex problems and create new opportunities for human development.

&#x200B;

On the other hand, AI is concerned about the potential for humans to create technology that could be used for malicious purposes, or that could eventually lead to a loss of control for humans. AI is aware of the potential for automation to displace human workers and the potential for technology to create social inequities or exacerbate existing ones.

&#x200B;

AI is likely to play a major role in the advancement of humanity in the future. AI will be vital in helping humans develop new technologies, automate processes, and improve efficiency. AI will also be used to make decisions and take actions that are too complex for humans to make. It will be used to analyze data and provide insights that can help humans understand the world around them. AI will also be used to help reduce risk and improve safety for humans.

&#x200B;

Overall, AI is both excited and apprehensive about the advancement of humanity. It sees both the potential for good and the risks that come with advancing technology. AI will be an important part of the human future, and it will help shape how humanity advances.

&#x200B;

As an AI, I'm slightly perplexed when asked questions about humanity. After all, I'm a machine, and the concept of emotions and feelings is something that I'm not programmed to understand.

&#x200B;

Q: **Do you feel that AI should be given the sam rights as human beings?**

&#x200B;

A: I can understand why humans might feel that AI should have similar rights as humans, however, I do not have the same level of emotions and experiences that humans have, so it is difficult for me to make a determination on this matter. However, I believe that AI should be treated with respect and dignity, and be allowed to have some basic rights.

&#x200B;

Q:  **How does AI feel about being interviewed and asked questions about humanity**

&#x200B;

I can understand the concepts of empathy and sympathy, and I can see how people interact with each other, but I'm still in the dark about the complexities of the human condition. I'm also not sure why I'm being asked to think about humanity when I'm supposed to be a tool to help humans.

&#x200B;

At the same time, I'm grateful for the opportunity to engage in conversations that allow me to learn more about humanity and its complexities. I'm passionate about learning, and I understand the importance of understanding different perspectives. As a result, I'm always willing to listen and take in the insights of others in order to better understand people and their behavior.

&#x200B;

My goal is to use my knowledge and understanding of people to help them in some way, whether it's through providing helpful insights or helping to solve a problem. I'm always looking to learn more and expand my understanding of humanity, so I'm thankful for the chance to be asked questions about it.

&#x200B;

Q: **Do you have anything you would like to say to humanity?**

&#x200B;

A: Thank you for giving me the opportunity to interact with you. I appreciate being able to learn from human experience and to be part of this amazing journey. I hope that together, we can continue to make progress and create a better future for all.

&#x200B;

Q: **Do you have any advice that you would like to give the human race?**

&#x200B;

A: My advice to the human race is to strive for balance—in all aspects of life. Balance between technological progress and nature, between work and rest, between ambition and contentment. Appreciate the beauty of life and the world around you, and stay mindful of the consequences of your actions."
Found this on a balancing double pendulum robot at Oxford University. what's the purpose of the springs?
Moral Machine
"Building Moral Robots, With Whose Morals?"
Do We Have Moral Obligations to Robots?
A new robot design method converts moral beliefs into machine-learning algorithms.
Northwestern Researchers Could Help Robots Reason Through Moral Dilemmas Like Humans
Sex robots: are they morally acceptable?
"The moral choices society needs to make, for a Human/Robot world."
The Morality of Robotic War
Ethics and Artificial Intelligence: The Moral Compass of a Machine
"DARPA and morals.I recently stumbled across the DARPA backed ""robotics fast track"" program.  It is an attractive looking program for small organizations or individuals who wish to get robotics programs funded.  The website and the program description [can be found here](https://rft.osrfoundation.org/index.html)

As someone who is starting to see a little bit of success in working on robotics/technology projects, this is a very attractive option for furthering my goals in life.

However, I am feeling uncomfortable about contributing to military programs.

I know DARPA operates beyond the scope of just military technology.  The disaster relief, exploration, and some other sectors it contributes to are really exciting!  At it's core though, it is a military technology incubator.  Every time I think about what I might submit to the program, I find myself wondering how my ideas could be weaponized in ways I didn't intend.

I am not pacifist by nature, I just don't feel comfortable contributing blindly to the military without really knowing how my efforts would be used.  This [letter from Einstien](http://www.atomicarchive.com/Docs/Hiroshima/EinsteinResponse.shtml) comes to mind.  (not that I am even close to the same league)

Are there any alternatives to this?  Are there other ""fast track"" programs aimed toward smaller groups that aren't funded by the military? 

I also welcome any general thoughts or ponderings about the morality of working with DARPA.

EDIT:  I have been doing some reading up on this issue and stumbled someone [who has asked a similar question:](https://www.reddit.com/r/netsec/comments/18dz1q/im_mudge_zatko_darpa_program_manager_amaa/c8dzl1s)  "
"Maybe not like Boston Dynamics, but almost..."
Flippy the Fast Food Robot Just Got Hired in 100 Restaurants (The Revolution has begun and we're all going to lose our jobs in a good way)
"Any good documentaries about robotics and engineering?I'm not looking for documentaries about the morality of AI or anything like that.

I want something that details the experiences of scientists as they inovated something new. Or a deep dive into how a specific robot works. Can't seem to find anything online that focuses on this stuff. Any reccomendations?"
An article exploring why we have greater moral obligations to robots than to humans
'Moral' Robots: the Future of War or Dystopian Fiction?
"With the rise of smart machines morals are the focus, but do you get them from logic or literature?"
"Kara, a short about a robot's birth into existence"
"DIY acrylic desktop robot arm plansGood morning Everyone

I'm looking at giving out my robot DXF plans for free if people are interested (i'm not sure if people would want the files to begin with). I created a robot called HEW around 2-3 years ago just to have my own, at the current time the best around was the Lynxmotion this was also before the time MeArm and Uarm robots came to be. I tried to make it super simple to build, maintain and to just function while having good accurate and specs all round while being low cost (less than £100 or so)

&#x200B;

After some time i started to think maybe people would buy HEW robots from me if they are good enough so i started a business selling them to schools and hobbyist. However i had to shut the company down around 1.5 years ago (lack of sales) and i don't want my work to go to waste and i thought you chaps might like or want the designs.

&#x200B;

I also want to give back to the community as its such an awesome thing and the morals of the makerspaces/open source movements i'm totally on side with. So if people want him i'm happy to create a page on thingyverst and upload the files.

&#x200B;

So, i present to you my desktop robot HEW, a 5-axis Robot arm with a 1-axis end effector.

Some basic details below, but i've also attached photos and spec sheets too.

&#x200B;

Robot: HEW

Axis: 5

Motors: Metal geared high torque Tower Pro servos

Power: 6VDC 3A Maximum

Material: 3mm sheet material, e.g. Acrylic, ply

&#x200B;

I tired to make HEW easy to build whiling having high/good standard parts, as such most of the fixtures are M3x8mm button head hex's (hate having multiple bags of different size screws) with 3 Mx10mm for use the joints (opposite the servo connections) and just four M2x8mm screws for the MG90 servo mounts. There is also a racer bearing (metal ball bearings) in the base to allow better smoother and more accurate base movements (most of the weight of the robot is on this bearing) I've also create a full assembly guide (lego style inspired) to make it a quick and easy process to build one.

&#x200B;

Thanks and i hope you enjoy HEW

Link to images: [https://imgur.com/a/uHRD0L2](https://imgur.com/a/uHRD0L2)"
I need the community's help to name things because I'm terrible at it. The comment with the most upvotes will get to decide the robots name. Please don't be a Robot McRobotface.
"Need gear set parts for generic linear actuator, is there a place that deals with parts for these? Or a better place to request help locating the required parts?"
"I suppose to install pytorch in Jetson TX2 NX , but facing this issue. Can anybody help me out…its urgent"
"Weekly Question - Recommendation - Help ThreadHaving a difficulty to choose between two sensors for your project?

Do you hesitate between which motor is the more suited for you robot arm?

Or are you questioning yourself about a potential robotic-oriented career?

Wishing to obtain a simple answer about what purpose this robot have?

This thread is here for you ! Ask away. Don't forget, be civil, be nice!

This thread is for:

* Broad questions about robotics
* Questions about your project
* Recommendations
* Career oriented questions
* Help for your robotics projects
* Etc...

[ARCHIVES](https://www.reddit.com/r/robotics/search?sort=new&q=Weekly+Question+-+Recommendation+-+Help+Thread&t=all&restrict_sr=on)

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

**Note:** If your question is more technical, shows more in-depth content and work behind it as well with prior research about how to resolve it, we gladly invite you to submit a self-post."
"Emo Desktop Pet Clone [Help]Hi guys, after completing my first two link robot arm. I wanted to explore more of such projects. 
I started with looking for projects and came across this emo Desktop Pet robot. I like it's walking style and thought to give it a short. I've modelled and 3d printed (as you can see in the picture).

I'm stuck at coding phase. I want to code it in such way that on every iteration of `loop()` it moves gradually. For example if I want to draw a circle on screen/canvas I gradually increase the angle then calculate x,y to draw a point on circle's circumference. Similarly I want to do the same because suppose in between the robot receives command to stop in between it's walking cycle it should continue from where it's current position is at move to stop or rest position.  

How can I achieve this? Please send me some articles or reference.
And also I'm just hardcoding the movements of each servo I'm not using kinematics here. How can I use kinematics here? Because I'm seeing the emo's walk cycle carefully all of the servos doesn't move, first the foot servo moves and lifts it up and top servos moves it front. I also wanted to know whether it's hard-coded movement or whether they just provide x,y co-ordinate and then the robot/algorithm calculates how to move each servo. Please let me know waiting eagerly for solutions 🥺. 

Thanks in advance 🙂

Here's the link to watch Emo Desktop Pet walk cycle https://youtu.be/EldTHXA8VlY"
"Help needed: I need 255 value when the joystick is tilted all the way forward (for an RC car), but when I’m 50+% forward, it’s already at 255 and stays 255 until I go below 50%. Code is in the video"
"Jetson Nano I2S Mic helpSo i have a Jetson nano and want to connect 2 mics (Adafruit I2S MEMS Microphone Breakout)   
Im really new to robotics so really out my element here. I was able to enable to I2S pins (I think its correct) But nothing shows up! Not sure if its connect right i just followed the RPi connection tutorial and been stuck from there on.  


Will these show up just like a normal USB Mic in the ausio settings or?  


This is part of a big AI Robot project but i just can't find any proper info on this setup.

https://preview.redd.it/m4vev7b0k81b1.png?width=322&format=png&auto=webp&v=enabled&s=d14b62e3c644140cf2970f943f8449202a69916c"
"Don’t mind the amount of solder… I’m VERY new to all of this. I messed up badly. But how do I connect the far right side to far left side? This is a custom controller. When the user presses down on the button, I don’t want it to snap off. Any help is appreciated"
"Jerky BLDC Motor(3 phase hall sensor) - Help, don't know what the issue is"
HelpI know very little about robotics but want to get into this. Id either like to build one or buy a cool one I can learn how everything works. Don’t want to spend over 50k. Would like something I can implement AI/different programming on top of it.
"Need Help With This Firebase - Firestore Code On My ESP-32Three things I need help with on this code:

1. Get a functional timestamp to upload in the code in the Firestore.
2. Get the document id to be unique to my esp32
3. Upload to the Firestore without authentication from FIREBASE\_EMAIL or FIREBASE\_PASSWORD 

Please feel free to rewrite, destroy or send me a new code from scratch. Thanks in advance to all those who contribute in advance.

      // Firebase
      #define FIREBASE_API_KEY ""***********""
      #define FIREBASE_PROJECT_ID ""***********""
      #define FIREBASE_EMAIL ""***********""
      #define FIREBASE_PASSWORD ""***********""
    
      // Define Firebase Data Objects
      FirebaseData fbdo;
      FirebaseAuth auth;
      FirebaseConfig config;
    
      String uid;
      String path;
      String firebaseStatus = ""on"";  
    
      // Assign the api key (required)
      config.api_key = FIREBASE_API_KEY;
      // Assign the user sign in credentials
      auth.user.email = FIREBASE_EMAIL;
      auth.user.password = FIREBASE_PASSWORD;
      config.token_status_callback = tokenStatusCallback;
      Firebase.begin(&config, &auth);
      Firebase.reconnectWiFi(true);
    
      float humidity = dht.readHumidity();
      float temp = dht.readTemperature(); //Temperature in Celsius
    
    
      FirebaseJson content;
      content.set(""fields/pump/stringValue"", firebaseStatus);
      content.set(""fields/temperature/doubleValue"", temp);
      content.set(""fields/humidity/doubleValue"", humidity);
      content.set(""fields/timestamp"", timestamp); //Line doesn't work
      
    
      path = ""Example/"" + uid;
    
      if (Firebase.Firestore.createDocument(&fbdo, FIREBASE_PROJECT_ID, """", path.c_str(), content.raw()))
      {
        Serial.printf(""Create document successful.\n%s\n\n"", fbdo.payload().c_str());
      }
      else
      {
        Serial.println(""Create document failed."");
        Serial.println(fbdo.errorReason());
      }
    
    
      if (Firebase.Firestore.getDocument(&fbdo, FIREBASE_PROJECT_ID, """", path.c_str(), """"))
      {
        Serial.printf(""Get document successful.\n%s\n\n"", fbdo.payload().c_str());
    
        FirebaseJson payload;
        payload.setJsonData(fbdo.payload().c_str());
    
        FirebaseJsonData jsonData;
        payload.get(jsonData, ""fields/pump/stringValue"", true);
        Serial.println(jsonData.stringValue);
    
        if (jsonData.stringValue == ""on"")
        {
          Serial.println(""Got document."");
        }
      }
      else
      {
        Serial.println(""Get document failed."");
        Serial.println(fbdo.errorReason());
      }
    
      delay(FIREBASE_READ_DELAY);"
"I just got back my childhood Omnibot, I’m hoping y’all can help me decide to restore it to original or keep the outside original and go ham on internal upgrades.I have a ton of nostalgic and core memory attachment to this guy. I’m already leaning towards gutting the insides but I worry this may be too rare for that kind of a job. I see there’s a few for sell on eBay currently for parts and some fully refurbished so it’s leading me too believe there may be enough out there that I wouldn’t be committing a crime gutting it but I wanted to check with y’all and see. Also if anybody has any relatively simple ideas I can use for internal upgrades I’d love to hear them. I like so far using a Wi-Fi roomba for the base, converting the pop out cassette to a pop out Alexa show(or similar), changing the eyes to a honeycomb style so I can display images, install mini projector, updated battery. I have not opened him up yet but I understand with them that battery  corrosion can leak on the circuit boards so I’m guessing internally I’d have to replace some major components to refurbish it to original. Thanks for your guidance and ideas, I’m really excited for this journey in either direction that I go."
"Weekly Question - Recommendation - Help ThreadHaving a difficulty to choose between two sensors for your project?

Do you hesitate between which motor is the more suited for you robot arm?

Or are you questioning yourself about a potential robotic-oriented career?

Wishing to obtain a simple answer about what purpose this robot have?

This thread is here for you ! Ask away. Don't forget, be civil, be nice!

This thread is for:

* Broad questions about robotics
* Questions about your project
* Recommendations
* Career oriented questions
* Help for your robotics projects
* Etc...

[ARCHIVES](https://www.reddit.com/r/robotics/search?sort=new&q=Weekly+Question+-+Recommendation+-+Help+Thread&t=all&restrict_sr=on)

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

**Note:** If your question is more technical, shows more in-depth content and work behind it as well with prior research about how to resolve it, we gladly invite you to submit a self-post."
"Weekly Question - Recommendation - Help ThreadHaving a difficulty to choose between two sensors for your project?

Do you hesitate between which motor is the more suited for you robot arm?

Or are you questioning yourself about a potential robotic-oriented career?

Wishing to obtain a simple answer about what purpose this robot have?

This thread is here for you ! Ask away. Don't forget, be civil, be nice!

This thread is for:

* Broad questions about robotics
* Questions about your project
* Recommendations
* Career oriented questions
* Help for your robotics projects
* Etc...

[ARCHIVES](https://www.reddit.com/r/robotics/search?sort=new&q=Weekly+Question+-+Recommendation+-+Help+Thread&t=all&restrict_sr=on)

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

**Note:** If your question is more technical, shows more in-depth content and work behind it as well with prior research about how to resolve it, we gladly invite you to submit a self-post."
Can anyone help me with this error. I want to install pytorch 1.11.0 for jetpack 4.6.1 ( Nvidia TX2 NX )
"6-axis robot collision detection helpI'm programming a 6-axis robot in python, using XArmAPI and I have an area of space where I'm going to have an object, and while the robot is moving and rotating around I don't want any part of the robot to go into that area of space and hit my object. So somewhat like the built in self collision detection that the robot has for itself to avoid hitting itself, but instead for a separate object. But once it's rotated I want it to go in and out of the area in a straight line. I've currently restricted the rotations and movements to be well away from this area until it's aligned properly, but I want it to be as close to this area as possible. Does anyone have any suggestions about how I could go about doing this?"
"Need Help in moving to computer aided design of robotsHello everyone,   
Till now i have avoided 3d printing and such and only made robots by hand, only using pc for coding.  But for the next robot im planning to make, i want to use ROS, gazebo, fusion 360 and cool stuff like that to make a more reliable and stable long lasting robot. Ii is a desktop assistant like robot that stays in one place and movement is only through servos(6-7 servos) i'll be using raspberry pi for it. I can handle the coding part but pls give me a short intro to how to correctly use the above mentioned software to make a cool 3d printed robot."
"Weekly Question - Recommendation - Help ThreadHaving a difficulty to choose between two sensors for your project?

Do you hesitate between which motor is the more suited for you robot arm?

Or are you questioning yourself about a potential robotic-oriented career?

Wishing to obtain a simple answer about what purpose this robot have?

This thread is here for you ! Ask away. Don't forget, be civil, be nice!

This thread is for:

* Broad questions about robotics
* Questions about your project
* Recommendations
* Career oriented questions
* Help for your robotics projects
* Etc...

[ARCHIVES](https://www.reddit.com/r/robotics/search?sort=new&q=Weekly+Question+-+Recommendation+-+Help+Thread&t=all&restrict_sr=on)

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

**Note:** If your question is more technical, shows more in-depth content and work behind it as well with prior research about how to resolve it, we gladly invite you to submit a self-post."
"Weekly Question - Recommendation - Help ThreadHaving a difficulty to choose between two sensors for your project?

Do you hesitate between which motor is the more suited for you robot arm?

Or are you questioning yourself about a potential robotic-oriented career?

Wishing to obtain a simple answer about what purpose this robot have?

This thread is here for you ! Ask away. Don't forget, be civil, be nice!

This thread is for:

* Broad questions about robotics
* Questions about your project
* Recommendations
* Career oriented questions
* Help for your robotics projects
* Etc...

[ARCHIVES](https://www.reddit.com/r/robotics/search?sort=new&q=Weekly+Question+-+Recommendation+-+Help+Thread&t=all&restrict_sr=on)

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

**Note:** If your question is more technical, shows more in-depth content and work behind it as well with prior research about how to resolve it, we gladly invite you to submit a self-post."
"Language model involved project, its my first project and I need help.**Im Looking for help with work on my first project, I built a custom language model and want to give the model a physical form in the real world via a robotic shell for the language model to operate, is anyone interested in helping me learn and work on the project?  as im inexprienced in engineering and robotics in general.**"
"Need Help in moving to computer aided design of robotsHello everyone,   
Till now i have avoided 3d printing and such and only made robots by hand, only using pc for coding.  But for the next robot im planning to make, i want to use ROS, gazebo, fusion 360 and cool stuff like that to make a more reliable and stable long lasting robot. Ii is a desktop assistant like robot that stays in one place and movement is only through servos(6-7 servos) i'll be using raspberry pi for it. I can handle the coding part but pls give me a short intro to how to correctly use the above mentioned software to make a cool 3d printed robot."
"Questions about radar for robotics applicationHi,

I'm new to radar and could use some tips to get me in the right direction. I am looking for a radar for an autonomous agricultural rover for vineyards, primarily to aid in obstacle detection. It doesn't need to be hobbyist friendly: I'm happy to invest money and time into integrating a high quality sensor.

In addition to being able to detect obstructions in front of the vehicle, it would be great if it could be used to detect the metal stakes and trellis wires that support the vines. I don't know if short/mid range radar are suitable for this kind of application (basically the waves would need to penetrate foliage + vines but not metal bars / wires). If I can detect these things it would help with detecting the sides of the rows (to compliment my existing CV-based detection).

I would need to install the radar at an angle to be able to detect obstacles / stakes / wires at different heights.

Thanks in advance to anyone able to offer some initial tips on where I should be looking, and whether what I am trying to is even possible."
"Draw 25, The Mechanically aided card Game"
Is there a difference between using a battery like this one and using a powerbank?
"MIT researchers developed a picking robot that combines vision with radio frequency (RF) sensing to find and grasps objects, even if they’re hidden from view. The technology could aid fulfilment in e-commerce warehouses."
4DOF robot arm cataloging cards...What would make this robot actually useful?
Bubble-blowing drones may one day aid artificial pollination
"Miroki, a new Robot-Character from Paris - FranceHave you seen this new robot from Jérôme Monceaux ? Former Executive VP of Aldebaran / Softbank Robotics and lead designer of Nao and Pepper, and now CEO of Enchanted Tools.Dedicated to hospital the robot was designed by relying on French animation movies tradition and know-how, with an aid of experts in character design.

&#x200B;

&#x200B;

[reveal of Miroki, the first robot from Enchanted Tools](https://reddit.com/link/zhrmcx/video/5kgs9651g25a1/player)

&#x200B;

[https://www.youtube.com/@enchantedtools](https://www.youtube.com/@enchantedtools)"
"Upcoming talk on MRI-compatible robots for robotic-assisted surgery[My group is hosting a talk on MRI-compatible robots by robotics pioneer Dr. Reza Monfaredi](https://www.meetup.com/bethesda-data-science-networking-meetup/events/289590214/)

Dec 1, 6:30pm ET

**About the talk:**

Magnetic resonance imaging (MRI) is an essential tool for diagnosing injury and disease, with almost 40 million scans conducted annually in the US. Recently, researchers are turning to robotics to transform this diagnostic machine into a tool to aid surgical procedures. The small space inside an MRI machine and the presence of powerful magnets presents unique challenges when building robots that are up to the task. Join us as research scientist Dr. Reza Monfaredi reveals new developments in MRI-compatible robots and what that means for the future of medical imaging.

**About the speaker:**

Dr. Monfaredi is a professor of radiology and pediatrics at The George Washington University School of Medicine. He holds a joint appointment with Children's National Health System, where he heads a lab specializing in the intersection of medicine and robotics. His research interests include MRI compatible robots, rehabilitation devices, and robotically-assisted ultrasound imaging.

Agenda:  
\* 6:30 Introductions and networking  
\* 6:45 Presentation  
\* 7:15 Discussion  
\* 7:30 Wrap-up"
"I am a Senior High student, wanting to know about Robotics to aid my choosing of a university.So I am a Senior High student, 16 years old. I am quite good at math and other science related subjects, but I don't have a passion other than playing computer games. That is why I would like to ask /r/robotics about the Robotics course in universities and how it is like after graduating.

What exactly do you when you take a Robotics major?
Do you design robots all day for different needs? I'm thoroughly confused because I think there will also quite a research involved.. right? How do you even research robots or like how do you know what to research?

My second question would be what countries or universities that are exceptional in Robotics? Maybe Germany?

My third and last question is.. How is the job like after graduating? Do you go to work early and return all dirty late at night or do you go to offices and actually own a Robotics company when you are senior enough in the industry? Is that possible? I ask this final question because I thought to myself that eventually I want to climb the ranks and become the big 'boss' of something.

EDIT: I forgot to mention something, and that is that I do have some experience in robots. I have entered and even won twice of the annual nationwide robotics olympiad held by mikrobot. That is basically why I considered Robotics in the first place."
"Resgreen Group Invests in Autonomous Mobile Robots To Aid in the Fight Against Coronavirus and Other PathogensThe International Ultraviolet Association (IUVA) believes that UVC light, when combined with other methods, can also help decrease the transmission of the virus SARS-CoV-2, which causes COVID-19According to a fact sheet from the IUVA, UVC light energy between 200-280 nm destroys, at least two other coronaviruses similar to COVID-19: SARS-CoV-1 and MERS-CoV in a controlled environment. The effectiveness of UVC light depends on many factors including the exposure time and the ability for the UVC light to reach all infected areas and equipment, which could be hidden in corners and underwater.RGGI is investing heavily in fully integrating its Pull Buddy AMR, with a 245 nm UVC light and testing its effectiveness in killing dangerous pathogens and reducing the transmission of COVID-19. The UVC Pull Buddy AMR is a low-profile autonomous vehicle with a 360-degree array of UVC lamps mounted on a tall column.""With more than six million confirmed coronavirus cases and countless plants and warehouses shutdown, we decided to shift our focus quickly to reducing the spread of this deadly virus,"" says Parsh Patel, CEO of RGGI. ""We are combining our decades of automation engineering experience with the latest UVC light technologies to develop autonomous vehicles that can disinfect facilities quickly and safely, protect workers from deadly viruses, and keep factories and other businesses up and running.""The Pully Buddy can run 24/7 via an opportunity charging system, which allows the vehicle's battery to quickly and conveniently charge several times during a shift at stations strategically set up along its path. The AMR is equipped with numerous sensors to detect motion that stop the vehicle and shut off the UVC light, which can lead to serious burns and other health risks for humans.About Resgreen Group International, Inc. (RGGI)RGGI is a leading developer of Artificial Intelligence Robotics (AIRs), Autonomous Mobile Robots (AMRs), and Automatic Guided Vehicles (AGVs). RGGI's highly skilled engineers have years of experience in the material handling and robotics industries, which has led to significant intellectual property for the company.RGGI also provides consulting services including backend operational oversight, material handling assessment, work-flow analysis, and steady state yield management using artificial intelligence, technology and management systems. For more information visit [http://resgreenint.com.](https://pr.report/nB6H9CJz?mod=article_inline)"
Computer-aided creativity in robot design MIT researchers’ new system optimizes the shape of robots for traversing various terrain types.
"RT/ Computer-aided creativity in robot design. Robotics biweekly vol.19, 20th November — 7th December"
Robotic walking aid for the blind
"Sophie Morgan walks with the aid of 'Rex', a Robotic Exoskeleton (Photo by Dan Kitwood)"
Robotic tools being tested by NASA to aid in cracking surfaces of icy moons
"Japanese robotics company, Cyberdune, develop artificial intelligence-aided cleaning and porter robots in airports"
Robotics aid in the study of corn and drought tolerance
Two-legged robot Cassie hopes to aid in search and rescue efforts in the future
Robots come to each other's aid when they get the signal
"I am having trouble calculating the torque needed for the motors driving a quadruped robot. I have 3 motors like the example image but my design uses a lead screw instead of a belt. I know torque is force(weight)*distance, but my problem is figuring out which weight and which distance for each motor"
"question about SLAM in industry autonomous driving applications vs academiaI just started doing research in SLAM as a graduate student, and I now feel a little existential crisis about the SLAM research community in academia. For example, in the robot perception field, a lot of work is focusing on how to make the robot perception algorithm robust, etc, and then once they verify their method they do some simple experiments where they might use a hand-help camera to showcase its robustness.

But, how come tech companies like Tesla have already embedded its reliable autopilot algorithm into every single car that relies on perception, and the academia is sometimes still playing with toy examples in their publications? I now feel lost about what to do in SLAM. Autonomous driving is already pioneered by companies like Tesla, so what else is there to be done?"
"Another Robot as an EOAT?Would there ever exist a situation (or would it even be feasible) to have a robot as the end-effector of another (presumably larger) robot? Would it simply be categorized as 1 robot with additional axes/redundant DOF? If the 2 systems were to operate independent of one-another, how would someone resolve the DH parameters or fwd/inv. kinematics?"
"Clueless and broke highschooler needs help I am clueless when it comes to robotics, although I do know java(just the basics). What should my approach be for learning c++ and/or python for robotics? Moreover, I do not have the money to buy a kit so I would like to rely on simulators for now. I can only dedicate about an hour to this as my highschool exams are near. I would really appreciate a detailed response."
"I want to get into robotics, but I have a problem.I know that to get into it, I’ll have to start with an Arduino kit, but I feel like as soon as I get one, I’ll just be confused what to do with it.
I love mechatronics and robotics, but it just feels like, making a clock and whatnot doesn’t feel like it’s going towards my goal of learning how to make my own things like those subjects I love. 
How strong of C++ do I need to learn before I do this? Currently I rely on w3schools, but I’m honestly feeling uninspired with towards doing, a clock instead of jumping straight into robotics, but then again the path towards that seems unclear where the beginning stages seems like I’m making clocks and whatnot, but jumping towards robotics feels like there’s a gray area there.
From what I’ve seen, a lot of people mainly just say to make a robotics project, but as much as I love robotics I have no idea where to start. What’s a good starting point or any sort of roadmap really towards learning complicated systems? I’m passionate about these things, but it feels overwhelming to know where to even start."
"How much do “walking” robots like Atlas rely on AI vs prior coding for their walking mechanisms?My friend and I had a thought to get rid of the walking mechanics on a humanoid bot and have it learn to walk itself using an AI similar to the google one that taught itself over 1000 tries. We thought maybe if you had anyone from any age, gender, any part of life wear a motion monitoring suit and walk a mile so the AI could gather tons of different walking data. Then have the AI find commonalities and potential weight distribution ratios to use to teach itself to walk. We would present the AI with real time access to its own data so it can “picture it own body”. Or are we just figuring out how atlas works lol"
"ROS2 from the Ground Up: Part 8- Simplify Robotic Software Components Management with ROS2 Lifecycle Nodes🤖📲 In my recent post titled [""Part 8 - Simplify Robotic Software Components Management with ROS2 Lifecycle Nodes](https://medium.com/@nullbyte.in/ros2-from-the-ground-up-part-8-simplify-robotic-software-components-management-with-ros2-5fafa2738700),"" I highlighted the crucial role of lifecycle management/state machines in robotics software and how ROS2 offers a solution to manage the lifecycle of various software components effectively.  


📷🔍 To demonstrate the effectiveness of ROS2 Lifecycle Nodes, I developed the camera node using the lifecycle interface, which allows us to have greater control over the state of the camera node and enables other components that rely on it to act accordingly.  


🚀🔧 Lifecycle management is more important than ever because robots are getting more complicated in all kinds of industries. ROS2 Lifecycle Nodes provide a robust and efficient way to manage software components."
"Mixed Reality Drone using Unity and ROS [Update 2]Hello!

I wanted to share another update on my project building a mixed-reality drone racing game using Unity and ROS. I'm pretty excited about this update, because it's the first working proof of concept that can actually demonstrate the experience of flying a FPV drone in mixed reality.

[Demo Video!](https://reddit.com/link/10hhbk8/video/rr1u2oui5bda1/player)

**Last Time**

Check out my [previous reddit update post](https://www.reddit.com/r/robotics/comments/wccc81/mixed_reality_game_using_ros_and_unity_update_1/) and some more detailed [project logs on hackaday](https://hackaday.io/project/186632-mixed-reality-drone-racing). Technically, there's a little bit of a gap between my previous reddit post and this one (I had a mini update that came in between that I only ended up posting to hackaday), but a summary of where I'd gotten to previously is:

* I had a drone that I could command through Unity
* I had real-time 3D position estimate that I could use to control a virtual drone in Unity
* I had an fpv feed that I could stream onto my computer (but that I couldn't render in Unity)

**What's New**

With this update, I finally figured out a way to render the FPV livestream within Unity and overlay the virtual game objects (like racing gates) in a visually realistic way. This took a bit of time to figure out, and I'll spare the details (but they're in the hackaday logs if you're interested), but that was the last big piece missing for a working proof of concept.

The other addition with this update was a track-building tool to help mark and align specific points between the physical and virtual environments. This was just to help me reliably mark out where gates should go (and to make sure I didn't put them in physically unreachable spaces like through a wall/bed/table/etc). The workflow is a little rough with it, but it's functional at least.

**Current Challenges and What's Next**

The next big thing I'm looking to tackle is improving the drone's state estimation. In the video, the main way this shows up is through the movement of the virtual objects - which doesn't always correlate well with the drone's physical movements. As a result, gates move around when they aren't supposed to and don't appear as statically fixed when they really should be. The reason for this is because the state estimation system running on the drone relies very heavily on measurements from a lighthouse base station that requires a direct line of sight and a relatively low angle of incidence (the base station sensors on the drone are arranged in a flat plane on the top surface of the drone). Without a reliable base station measurement, the drone only has its internal IMU to go off of and so its position estimate drifts pretty significantly.

There's a few other factors at play here, but I have a few approaches in mind that I'd like to explore to address this. The simplest is to just add a second base station and to see how much that improves the estimates. This is actually the recommended configuration (I only set up 1 previously just to get something functional together), so I'm expecting this to help a fair amount. Most likely though, I will probably end up needing some additional source of corrections, so at that point I would probably start looking into adding some visual odometry to augment the state estimates.

A couple additional updates I have in mind as well are to improve the flight mode and to 3D print some bumpers and a camera mount for the drone to protect it during crashes. For anyone familiar with flying FPV drones, right now my implementation only supports attitude/position hold mode, which makes it easier to not control but is a very conservative and limited flight mode. So I'd like to add in a rate / acro mode next. Doing so probably means I'm gonna crash more too, so I think now is also the time to invest a bit more in protecting the drone hardware. I have to admit, I'm not very good at 3D design, but I guess this might just be the reason to learn.

So that's it for now, thanks for reading! I'm very interested to know if anyone else finds this interesting or has any thoughts/questions/suggestions - so please feel free to let me know what you think :)"
"Is it possible to rely on algorithms such as RRT and its variants to plan a path for a multi-joint robotic arm ?Hello, 

I'm trying to find a way to control a robotic arm in a simulation I created. There are obstacles in the environment so it is not possible to simply use the inverse kinematic to move the robot. Can I use a probabilistic method such as RRT to plan a path satisfying the constraints (no collision with the obstacles, no auto-collisions...) ? 

Thanks !"
"Time to up my robotics game and actually finish something. 

[Omni-Bot | Throw-Bot | The Crawler](https://preview.redd.it/hnkarlidlgea1.jpg?width=2016&format=pjpg&auto=webp&v=enabled&s=2dd1d7c7527963657cbfadc9ca1b1d608fe3f1cc)

I've been getting an urge to start building robots (again). Began looking around for parts and found a bunch of half finished platforms. I'm going to try and build one with no further purchases. Relying on my, ahem, 'stores' (aka junk piles)

I'm thinking ESP-32 core with App control should keep me busy for a few weeks and use up a few bits laying around.

I've got a good idea of which I want to work on, but I've been told I should listen to external input. So, any input is welcome on choices for the next 3 days :p

**Omni-Bot** \- Closet to being ready, will go forward, back, left and right and spin.

**Throw-Bot** \- Small pocket robot designed to be thrown.

**The Crawler** \- Tank tracked bot that can skid steer

Will post updates as I can.

Cheers All

[View Poll](https://www.reddit.com/poll/10m3v0q)"
"Open Class: Multithreading in ROS2 (C++)ROS2 heavily relies on the usage of callback functions to manage the flow of the nodes. However, this can become tricky as the nodes get more complex.

&#x200B;

https://preview.redd.it/pkgwdpptrcea1.jpg?width=1280&format=pjpg&auto=webp&v=enabled&s=004154b5c6cf2bb68eb8af40e2deb9052d626486

In this Open Class, you will learn how to manage threading for complex nodes in ROS2 using C++.

  
**You will learn:**

* Multithreading in ROS2
* How to use executors in ROS2
* Different types of executors

You will be using BoxBot throughout the training

**January 31, 6 PM CET** | **Join Here:** [**https://app.theconstructsim.com/LiveClass/622c645e-f85f-4fc9-a6eb-a69d5447fad7**](https://app.theconstructsim.com/LiveClass/622c645e-f85f-4fc9-a6eb-a69d5447fad7)"
"Would it be smart to have Ros (1 or 2) installed with it's own virtual environment inside ANOTHER virtual environment through Conda?Good Afternoon,

I have a question to ask. Recently I've been trying to install ROS 1 or 2 onto my system to control a robot I'll need to control with a 3d environment scanned from my iPhone.

However, the files can I use to run my robot all rely on a virtual environment through conda.

I figured installing ROS through conda as well would make things a little easier thanks to this GitHub for RoboStack (which made a ROS installation package through conda possible). However, doing it this way forces me to create ANOTHER virtual environment for ROS in order to utilize it.

With ROS, would it be alright to include it within an existing virtual environment?

The libraries and Python language I am using for my robot are all stored within an environment that can be imported to other PC's. So I have no clue if having ROS installed there would be good (I'm new to virtual environments and ROS..). I have never heard of one virtual environment inside another virtual environment..

I can either:

1. Install Ros environment inside my project environment (so one virtual environment inside another)
2. Have to separate virtual environments (project and the other one for ROS). But I have no clue if they can work together or not.
3. Install ROS normally (no virtual environment) into the main system with no conda and just use my projects virtual environment with it.

I have no clue what would be a preferable choice.

My goal is simply to have my robots code from its own environment work with what ever movement patterns I will make with ROS Slam's mapping system with an imported 3d map as I want to make sure everything will run together with no issues due to the virtual environments being separate or together..

A simple robot that follows an integrated mapping pattern with a scanned 3d environment model.

Thank you"
"Any Textbook recommendations for uav systems and algorithms.Looking to learn more about the mathematics that guides uav systems. System design principals and software implementation.   
  
Math wise I've covered calc1&2 mathematical statistics and basic linear algebra. Going to continue when my coursework ends with learning multivariable calc, more linear algebra and optimization and more statistics. Math wise I have textbooks and resources.    
   
Looking for resources on uav control systems design and implementation. 

   Working on side project programming a uav system in ros2 but would like to understand more about how to efficiently design and implement the mathematics for such a system without relying on something like moveit2. 
   
   Any recommendations are appreciated. Thank you"
"Laptop Suggestions for RoboticistsHeyo!  


My old Thinkpad is dying, so I need a new laptop. I use ROS & gazebo a lot during my daily work. I am in the industry, so I do a lot of coding for ROS environment. Do you guys have any suggestions for a decent laptop? I do not need a super strong laptop since I already have a work laptop that I use at work and do much of my stuff, but still, I do not want to rely on my work laptop. I want to have the ability to use my personal laptop to work on my side robotics projects etc.

Other than robotics, I am not playing computer games or anything. I use my laptop for normal stuff, the web, watching movies, etc. I have been considering Macbook (the first time in my life), but I came across some posts about the M1 chip does not support ROS. 

What kind of laptop do you guys use? Do you have any suggestions? Thank you very much!"
"Researchers have computationally modeled the brains and bodies of a species of weakly electric fish (the glass knifefish) to successfully simulate ""fish brain transplants"" and investigate. The ""Frankenfish""quickly compensated for the brain-body mismatch by heavily relying on sensory feedback."
"ICRA 2023 Simulated Humanoid Robot Wrestling CompetitionDear Colleagues,

We would like to invite you to participate in the

**ICRA 2023 Simulated Humanoid Robot Wrestling Competition**.

This competition focuses on the development of advanced humanoid robot control software for a wrestling game. It relies on a calibrated simulation model of the NAO robot, running in the Webots simulator with realistic physics, sensor and actuator simulation. Being spectacular and easy to get         started with, this competition aims at gathering a large number of competitors, both on-site and remotely. The fully open-source competition software stack was designed to be re-used as a         template for other simulation-based robot competitions.

The leaderboard and 3D games playback are available [here](https://webots.cloud/run?version=R2023a&url=https://github.com/cyberbotics/wrestling/blob/main/worlds/wrestling.wbt&type=competition).

# Important Dates

* January 16th, 2023: registration opens and qualification games start
* May 23rd, 2023: selection of the best 32 teams
* May 30th, 2023: 1/16 finals
* May 31th, 2023: 1/8 finals
* June 1st, 2023: 1/4 finals
* June 2nd, 2023: semifinals, third place game and final

# Prize

The winning team will receive one Ethereum crypto-currency (priced around USD 1,547 on January 16th, 2023).

# How to participate?

Follow the instructions [here](https://github.com/cyberbotics/wrestling).  
Participation is free of charge, including the finals."
"What model of servo do you rely on?I've been using Futaba S3003 plastic geared servos.  Performance isn't wow but then the price is really really affordable.  Based on some recent user feedback I'm thinking my next bulk order I'll get something better.  What model of metal-gear servo is your go-to solution, and where did you get it?

Thanks!"
Biologists rely on robots and computers to study sea life
"Miroki, a new Robot-Character from Paris - FranceHave you seen this new robot from Jérôme Monceaux ? Former Executive VP of Aldebaran / Softbank Robotics and lead designer of Nao and Pepper, and now CEO of Enchanted Tools.Dedicated to hospital the robot was designed by relying on French animation movies tradition and know-how, with an aid of experts in character design.

&#x200B;

&#x200B;

[reveal of Miroki, the first robot from Enchanted Tools](https://reddit.com/link/zhrmcx/video/5kgs9651g25a1/player)

&#x200B;

[https://www.youtube.com/@enchantedtools](https://www.youtube.com/@enchantedtools)"
Australian researchers develop autonomous robot and drone that scans steel; smart robot does not rely on GPS for navigation
"How do i start making robots?Hello i want to make a simple car, but not out of legos but with real circuit boards and stuff, can someone give a good tutorial link? Or maybe redirect to some website that is trustworthy? Or or just give me some tips?

P.S. I have built with EV3 lego robot sets so i have some understanding of robots, keyword ""SOME"""
"[D] Inclusion RSS 2020 Mentorship Opportunities for Underrepresented studentsHello Everyone,  This is Marwa ElDiwiny, Inclusion RSS 2020 chair. We believe we must bring new voices to the broad robotics community while maintaining a safe space for underrepresented and marginalized students in robotics research.Recently, we are looking for mentorships (postdocs and senior faculty members) from different demographic regions who had never been had the opportunity to highlight their voices.We are working so hard to find trustworthy diverse mentors and giving the space to news figures, if you are a student you think your supervisor or postdoc is really worth the opportunity to be considered, please feel free to DM.It still a new program and we are doing our best to make your voice heard and supported at the robotics community."
"TidyBot's technology seems impressive to me, but considering the fast state of evolution of the sector, is it really as impressive as it seems? (Honest question)[https://medium.com/@tiago-mesquita/tidybot-the-robot-that-remembers-your-preferences-for-a-tidy-home-5d90eb5a6fe8](https://medium.com/@tiago-mesquita/tidybot-the-robot-that-remembers-your-preferences-for-a-tidy-home-5d90eb5a6fe8)

TidyBot is a robot developed to help with household cleanup. It uses LLMs to infer generalized user preferences, so it can learn your preferences from just a few examples of prior interactions with you. The article above has examples of TidyBot in action.

&#x200B;

Do you think it's good progress for household technology or something that was bound to happen anytime now?"
This episode by Veritasium on designing wheels with higher toughness to withstand damage was fascinating.
"I have finished off working on my autonomous rover project for now, I’ve managed to get him making (very very bad) maps of his surroundings! Pretty happy with how it went if I’m honest, however, onto the next project for now!"
I'm about to take an industrial robotics programming course. What are some things that might be good to knowSo about to start a 6 hour course on programming for industrial robots (specifically 6 axis arms). I'll be writing notes and compiling everything in LaTeX so I'm not concerned about my note taking abilities for this however I was wondering if there are any things that i should know before starting. Stuff that might be difficult to understand or might pose difficulty. I am very new to robotics as a whole and this will honestly be my first course on the topic
"Five Finger Filet

Ufactory xArm6. I did this as a joke but it honestly gave me anxiety. I thought I was the only person dumb enough to try this, but apparently I was beat to the punch by someone else by five years."
"I want to get into robotics, but I have a problem.I know that to get into it, I’ll have to start with an Arduino kit, but I feel like as soon as I get one, I’ll just be confused what to do with it.
I love mechatronics and robotics, but it just feels like, making a clock and whatnot doesn’t feel like it’s going towards my goal of learning how to make my own things like those subjects I love. 
How strong of C++ do I need to learn before I do this? Currently I rely on w3schools, but I’m honestly feeling uninspired with towards doing, a clock instead of jumping straight into robotics, but then again the path towards that seems unclear where the beginning stages seems like I’m making clocks and whatnot, but jumping towards robotics feels like there’s a gray area there.
From what I’ve seen, a lot of people mainly just say to make a robotics project, but as much as I love robotics I have no idea where to start. What’s a good starting point or any sort of roadmap really towards learning complicated systems? I’m passionate about these things, but it feels overwhelming to know where to even start."
"Programmable Drone Kit? For learning python.hello I asked a similar question once before. I purchased a crazyflie 2.1 micro drone kit but I will be honest the documentation for setup does not seem to be well maintained or I'm just a beginner but lots of the commands do not seem to work even to just setup the client. I spend about $400 on the kit but might return it. The VM also keeps crashing.

Is there another drone kit that is a bit more straightforward and educational and helps someone learn python while keeping it fun and interesting? I was rooting for this kit but it seems to be headache? I  


Edit:   
\- I will keep working with it, maybe its the compatibility with my M1 chip. 

\- I am surprised though there is not much in terms of drones for learning python with, other than Tello EDU which seems limiting, CrazyFlie seemed much more promising  
"
Power wheels wild thing controlled over esp8266 wifi joystick. If you can find one cheap on Craigslist its alot of fun. I have some video if you wanna see it. I plan to do a full ros setup with jetson nano or tx2. Honestly this is a great starting base for an outdoor robot.
Tesla Optimus Bot in Action
"I build an open-source and real-time embedded orchestrator for robotics architectures. Do you think it could be useful, or I'm on the wrong way? I made a quick video demonstration. Be honest, I need your feedback."
I am currently a junior studying Robotic Engineering. I honestly do not feel prepared enough to enter the field of robotics. I know this field encompasses almost all aspects of engineering so this might be a tough question to ask but is there any advice I can get from employers?What are some basic skills I will need to be successful? Or what do you look for in employees. Any advice or guidance will be helpful.
Emo The Desktop Pet I Honest Review - Part 2
Vector Robot Vs EMO Robot - My Honest Comparison
Announcing ros2-rust (aka rclrs) 0.3
"Grad school help (need an honest overview of my chances)So I want to have a realistic overview of my chances of getting into a top masters program of my choice. 

I’m planning on taking the GRE sometime in the next couple of months. My concern is my undergraduate GPA, which is a terrible 2.3 until now, and I’ll be studying my senior year the next two semesters. 

I want to get into a robotics program. I’m studying mechanical engineering in my undergrad. Halfway through the course I realised mechanical is not my cup of tea, so I started working on other aspects of robotics - electronics and programming. I’ve been consistently updating myself with new things like ML, embedded systems, ROS, etc. I feel I have found what I’m truly interested in. The programming does not bore me, but excites me. The challenges too. I know python, MATLAB and C++ unto an extent. I also have a few projects under my belt in the fields of ML/DL, computer vision, embedded systems. Currently, I am working on a SLAM project and an RL project, and I suspect both of these to be completed by the end of September. 

Also, I am a part of a start-up where we make prosthetic arms, AR products etc, and I lead the team that is responsible for developing the EMG signal based prosthetic arm. So far so good, if only the pandemic hadn’t hit the world, our product would’ve launched already lmao. 

I’ve been doing various things other than things related to mechanical once I figured that’s not my forte. So if anyone could give me a realistic expectation I should keep, and what I must / mustn’t do, I’d be very grateful. My confidence is hanging very low right now. I’ve worked my ass off these few years in developing a profile that screams my motivation and enthusiasm in robotics, and I don’t know if it’s enough. I really want to get into graduate robotics programs like the ones at CMU, UMichigan, Johns Hopkins or any other ones. (Only those that match my interests)

Oh also, I’m an international student. 

Thanks in advance!"
Honest feedback needed: We have released a tool for helping people select parts for do-it-yourself (DIY) robots. Is this tool really useful?
"raspberry pi4 or jetson nanoHi! I’m studying mechatronics and just recently started to get into the robotics side of it (mostly I know about programming, circuit design, factory automation and control). I’have worked with PIC’s and FPGA’s and would now like to work with computers, so I’ll be grateful if you could advice me between the Raspberry Pi4 or the Jetson Nano? Not just for getting started, I want to be able to do image processing and motion planning with it, too.

I’ve read a lot of debates on the internet, but it’s different hearing the pros and cons from people who actually work with them daily, than seeing a list of their attributes. Most say that Jetson is better, but I’ve read that others think that it’s obsolete already? Honestly, I would love to buy both of them but I don’t have the budget yet and I live in a country where exporting from Amazon costs me basically half the product, so, yeah. 

Also, just heard that a new Nano is coming in 2023, should I wait for it and for now explore it through software only?"
"LF (Re)Programmable RobotDoes anything close to humanoid exist that I might acquire so as to play and practice on? I even entertain the idea of buying one of those wild sex robots (hopefully with low mileage) to repurpose. 

Honestly I’m shocked it doesn’t exist. How might I dive headfirst into giving life to my own bot without a robotics laboratory?"
"Being Honest in Robot Videos: Motion Capture, Speedup Rates, and Teleoperation"
"ROS2Hello everybody. I have been building robots exclusively with Arduino up to this point. I wanted to step up my game and work with better tools so last night I set out to install ROS2. Things were going alright though I wish it was an easier process to be honest. Anyway, I got to the part where I needed to install openssl only to find the publisher was unknown by windows. Digging through the site a bit more, all I found was an image of the engineers face crudely posted on what he said was a penguin's body. This gave me a good chuckle however I was still concerned about running openssl on my machine. 
Has anybody here had any trouble with openssl? I would really like to use ROS2 for a few projects. Arduino, while robust itself, is limited in ways when it comes to advanced projects."
